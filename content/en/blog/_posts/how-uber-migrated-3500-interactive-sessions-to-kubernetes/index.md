
---
title: "How Uber Migrated 3,500+ Interactive Sessions to Kubernetes â€” with Zero Downtime: A Breakdown"
date: 2025-09-01
author: "Eric Nyuydze Wiryenkfea"
authorURL: "https://github.com/Wiryenkfea-Eric"
authorImageURL: "https://raw.githubusercontent.com/Wiryenkfea-Eric/k8s-blog-assets/master/Downloads/k8s-blog-assets/Profile%20pic.jpeg"

slug: "uber-interactive-sessions-kubernetes-migration"
blog: true
tags: [Kubernetes, DevOps, Uber, Scalability, Infrastructure]
---

Ever tried changing shoes, while running to catch a bus?  

Thatâ€™s what it felt like when Uber migrated thousands of live Jupyter and RStudio sessions â€” the beating heart of data science at scale â€” across container platforms. And incredibly, they pulled it off without disrupting a single user.

But thatâ€™s not all. What started as a backend infrastructure migration turned into a masterclass in platform engineering, DevOps resilience, and user-centric innovation.

If you work in DevOps, Data Science, or Platform Engineering, buckle up â€” because this story is packed with clever tricks, painful lessons, and surprising wins.

Letâ€™s rewind and walk through step by step.

---

## ğŸ”¹ Why Migrate to Kubernetes? And why did they do it now?

Uberâ€™s infrastructure follows three key principles:

* Cloud-agnostic flexibility (no vendor lock-in)  
* Automated, homogeneous scaling  
* Seamless on-prem to cloud migration  

Peloton, while reliable, had limitations:

* Static host groups with zero workload portability  
* Zone failures that could wipe out entire services  
* A high operational overhead that didnâ€™t scale well  

Kubernetes promised:

* Better resource management  
* Resilience  
* Cloud readiness  

But migrating live interactive sessions â€” where users install Python/R packages on the fly â€” was like swapping a carâ€™s engine while driving.

![Figure1: Transition of interactive session workloads](https://raw.githubusercontent.com/Wiryenkfea-Eric/k8s-blog-assets/master/Downloads/k8s-blog-assets/Transition%20of%20interactive%20session%20workloads%20from%20Peloton%20to%20Kubernetes.webp)
*Figure 1: Transition of interactive session workloads from Peloton to Kubernetes*

---

## ğŸ”¹ The Technical Hurdles

They werenâ€™t just migrating apps.  
They were moving **3,500+ interactive sessions**, many of them long-lived, GPU-hungry, and running live production workloads.

And this is where it gets interestingâ€¦

### 1. Interactive Sessions â‰  Kubernetes Jobs

Kubernetes Jobs were built for short, ephemeral workloads.  
Uber had to force them into behaving like durable sessions:

* `parallelism: 1` â€” isolate each session  
* `completions: 1024` â€” trick the Job into staying â€œaliveâ€  
* `restartPolicy: Never` â€” no unwanted restarts  
* `backoffLimit: high` â€” stop Kubernetes from killing jobs too soon  

```yaml
apiVersion: batch/v1                  # Defines the API version for a Kubernetes Job
kind: Job                             # Specifies this resource is a Job
spec:
  completions: 1024                   # Total number of successful completions required â€” keeps sessions running
  parallelism: 1                      # Only one pod runs at a time â€” 1 per user
  backoffLimit: 100                   # Number of retries before considering the Job as failed
  template:
    spec:
      restartPolicy: Never            # Pods will not be restarted on failure
      containers:
      - name: dsw-session             # Name of the container running the interactive session
        image: uber-dsw:latest        # Custom Docker image for Uberâ€™s DSW (Data Science Workbench)
        hostNetwork: true             # Grants the pod access to the host's network stack
```

That alone was impressive. But what came next was even smarterâ€¦

### 2. Persisting user-installed packages

Hereâ€™s the nightmare: Imagine running a notebook for hours, installing packages, and then suddenly it crashes. Everything gone.

To solve this, Uber built a real-time `inotify` daemon that:

* Watched for package installs (`pip`, `conda`, etc.)
* Checkpointed them to NFS in real-time
* Restored them automatically on reboot

The result? No more reinstallation rage.

### 3. The NFS Conundrum

Peloton hardcoded NFS to static nodes. Kubernetes doesnâ€™t like that.
And at the time, CSI drivers werenâ€™t an option.

So what did they do?
They mounted NFS at the fleet level â€” making it accessible to every pod without rewriting all their storage logic. A bold move that saved them from months of rewriting legacy logic â€” and kept the migration moving.

### 4. Observability had to be rebuilt from scratch

Kubernetesâ€™ UI couldnâ€™t give users what they were used to â€” job statuses, logs, sandbox views.

Uber replicated `etcd` data into Apache Cassandra and built a custom Compute UI.
Think of it as Kubernetes, reimagined for data scientists.

### 5. Federation: The Secret Sauce

To prevent single-zone failures and optimize utilization, Uber created **Federator**, an in-house abstraction that:

* Routes jobs across clusters based on health, SLA, and capacity
* Enables high availability and smart resource usage
* Allows seamless failovers and better resilience

This was no ordinary migration. It was a strategic re-architecture.

Without Federation, a single availability zone hiccup could have taken out hundreds of sessions.

![Figure 2: Kubernetes clusters usage without and with federation](https://raw.githubusercontent.com/Wiryenkfea-Eric/k8s-blog-assets/master/Downloads/k8s-blog-assets/Kubernetes%20clusters%20usage%20without%20and%20with%20federation.webp)
*Figure 2: Kubernetes clusters usage without and with federation*

---

## ğŸ”¹ Real-World Lessons

Initially, they considered storing full environments on NFS, but the high IOPS cost quickly ruled that out. Instead, they checkpointed only the package installation list.

Uber also built a **restart API** that:

* Notifies users about the transition
* Saves package metadata
* Seamlessly resumes sessions on Kubernetes

All this, without disrupting workflows or requiring user intervention.

---

## ğŸ”¹ What did they achieve?

* Migrated thousands of long-lived, GPU-powered sessions
* Zero user disruption across 2,000+ data scientists
* Improved availability, observability, and fault tolerance
* Set the stage for cloud portability and future automation

---

## ğŸ”¹ Lessons for Every DevOps and Platform Team

* âœ“ Abstract Kubernetes with care â€” not everything needs a CRD
* âœ“ Automate package persistence â€” users will thank you
* âœ“ UI and Observability matter â€” donâ€™t make users dig for logs
* âœ“ Federation = resilience â€” let your workloads flow intelligently
* âœ“ Migrations = modernization opportunities â€” donâ€™t just lift and shift

---

## ğŸ”¹ What this means going forward

Migrating interactive workloads is hard.
Doing it at Uber-scale â€” with zero downtime â€” is next-level.

But with the right architecture, user-first design, and a few brilliant hacks, Uber proved that even the most delicate workloads can be moved without a hiccup.

If youâ€™re running:

* JupyterHub
* ML pipelines
* Kubernetes data platform

â€¦then take a page out of Uberâ€™s playbook.

---

## Further Reading

* [Uber Engineering Blog â€” Migrating Large-Scale Compute Workloads to Kubernetes](https://eng.uber.com/migrating-large-scale-compute-workloads-to-kubernetes/)

---

## About the Author

**Eric Nyuydze Wiryenkfea** â€“ Cloud & DevOps Engineer (5 years plus of experience), Cloud Solutions Trainer, Community builder, Public speaker and technical writer.
Follow on [GitHub](https://github.com/Wiryenkfea-Eric) for more real-world infrastructure breakdowns.

```