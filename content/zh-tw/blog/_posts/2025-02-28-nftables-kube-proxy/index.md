---
layout: blog
title: "kube-proxy 的 NFTables 模式"
date: 2025-02-28
slug: nftables-kube-proxy
author: >
  Dan Winship (Red Hat)
translator: Xin Li (Daocloud)
---
<!--
layout: blog
title: "NFTables mode for kube-proxy"
date: 2025-02-28
slug: nftables-kube-proxy
author: >
  Dan Winship (Red Hat)
-->

<!--
A new nftables mode for kube-proxy was introduced as an alpha feature
in Kubernetes 1.29. Currently in beta, it is expected to be GA as of
1.33. The new mode fixes long-standing performance problems with the
iptables mode and all users running on systems with reasonably-recent
kernels are encouraged to try it out. (For compatibility reasons, even
once nftables becomes GA, iptables will still be the _default_.)
-->
Kubernetes 1.29 引入了一種新的 Alpha 特性：kube-proxy 的 nftables 模式。
目前該模式處於 Beta 階段，並預計將在 1.33 版本中達到一般可用（GA）狀態。
新模式解決了 iptables 模式長期存在的性能問題，建議所有運行在較新內核版本系統上的使用者嘗試使用。
出於兼容性原因，即使 nftables 成爲 GA 功能，iptables 仍將是**預設**模式。

<!--
## Why nftables? Part 1: data plane latency

The iptables API was designed for implementing simple firewalls, and
has problems scaling up to support Service proxying in a large
Kubernetes cluster with tens of thousands of Services.

In general, the ruleset generated by kube-proxy in iptables mode has a
number of iptables rules proportional to the sum of the number of
Services and the total number of endpoints. In particular, at the top
level of the ruleset, there is one rule to test each possible Service
IP (and port) that a packet might be addressed to:
-->
## 爲什麼選擇 nftables？第一部分：資料平面延遲

iptables API 是被設計用於實現簡單的防火牆功能，在擴展到支持大型 Kubernetes 叢集中的 Service
代理時存在侷限性，尤其是在包含數萬個 Service 的叢集中。

通常，kube-proxy 在 iptables 模式下生成的規則集中的 iptables 規則數量與
Service 數量和總端點數量的總和成正比。
特別是，在規則集的頂層，針對資料包可能指向的每個可能的 Service IP（以及端口），
都有一條規則用於測試。

<!--
```
# If the packet is addressed to 172.30.0.41:80, then jump to the chain
# KUBE-SVC-XPGD46QRK7WJZT7O for further processing
-A KUBE-SERVICES -m comment --comment "namespace1/service1:p80 cluster IP" -m tcp -p tcp -d 172.30.0.41 --dport 80 -j KUBE-SVC-XPGD46QRK7WJZT7O

# If the packet is addressed to 172.30.0.42:443, then...
-A KUBE-SERVICES -m comment --comment "namespace2/service2:p443 cluster IP" -m tcp -p tcp -d 172.30.0.42 --dport 443 -j KUBE-SVC-GNZBNJ2PO5MGZ6GT

# etc...
-A KUBE-SERVICES -m comment --comment "namespace3/service3:p80 cluster IP" -m tcp -p tcp -d 172.30.0.43 --dport 80 -j KUBE-SVC-X27LE4BHSL4DOUIK
```
-->
```
# 如果數據包的目標地址是 172.30.0.41:80，則跳轉到 KUBE-SVC-XPGD46QRK7WJZT7O 鏈進行進一步處理
-A KUBE-SERVICES -m comment --comment "namespace1/service1:p80 cluster IP" -m tcp -p tcp -d 172.30.0.41 --dport 80 -j KUBE-SVC-XPGD46QRK7WJZT7O

# 如果數據包的目標地址是 172.30.0.42:443，則...
-A KUBE-SERVICES -m comment --comment "namespace2/service2:p443 cluster IP" -m tcp -p tcp -d 172.30.0.42 --dport 443 -j KUBE-SVC-GNZBNJ2PO5MGZ6GT

# 等等...
-A KUBE-SERVICES -m comment --comment "namespace3/service3:p80 cluster IP" -m tcp -p tcp -d 172.30.0.43 --dport 80 -j KUBE-SVC-X27LE4BHSL4DOUIK
```

<!--
This means that when a packet comes in, the time it takes the kernel
to check it against all of the Service rules is **O(n)** in the number
of Services. As the number of Services increases, both the average and
the worst-case latency for the first packet of a new connection
increases (with the difference between best-case, average, and
worst-case being mostly determined by whether a given Service IP
address appears earlier or later in the `KUBE-SERVICES` chain).

{{< figure src="iptables-only.svg" alt="kube-proxy iptables first packet latency, at various percentiles, in clusters of various sizes" >}}

By contrast, with nftables, the normal way to write a ruleset like
this is to have a _single_ rule, using a "verdict map" to do the
dispatch:
-->
這意味着當資料包到達時，內核檢查該資料包與所有 Service 規則所需的時間是 **O(n)**，
其中 n 爲 Service 的數量。隨着 Service 數量的增加，新連接的第一個資料包的平均延遲和最壞情況下的延遲都會增加
（最佳情況、平均情況和最壞情況之間的差異主要取決於某個 Service IP 地址在 `KUBE-SERVICES`
鏈中出現的順序是靠前還是靠後）。

{{< figure src="iptables-only.svg" alt="kube-proxy iptables 在不同規模叢集中各百分位數下的第一個資料包延遲" >}}

相比之下，使用 nftables，編寫此類規則集的常規方法是使用一個單一規則，
並通過"判決映射"（verdict map）來完成分發：

<!--
```
table ip kube-proxy {

        # The service-ips verdict map indicates the action to take for each matching packet.
	map service-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "ClusterIP, ExternalIP and LoadBalancer IP traffic"
		elements = { 172.30.0.41 . tcp . 80 : goto service-ULMVA6XW-namespace1/service1/tcp/p80,
                             172.30.0.42 . tcp . 443 : goto service-42NFTM6N-namespace2/service2/tcp/p443,
                             172.30.0.43 . tcp . 80 : goto service-4AT6LBPK-namespace3/service3/tcp/p80,
                             ... }
        }

        # Now we just need a single rule to process all packets matching an
        # element in the map. (This rule says, "construct a tuple from the
        # destination IP address, layer 4 protocol, and destination port; look
        # that tuple up in "service-ips"; and if there's a match, execute the
        # associated verdict.)
	chain services {
		ip daddr . meta l4proto . th dport vmap @service-ips
	}

        ...
}
```
-->
```none
table ip kube-proxy {

  # service-ips 判決映射指示了對每個匹配數據包應採取的操作。
  map service-ips {
    type ipv4_addr . inet_proto . inet_service : verdict
    comment "ClusterIP、ExternalIP 和 LoadBalancer IP 流量"
    elements = { 172.30.0.41 . tcp . 80 : goto service-ULMVA6XW-namespace1/service1/tcp/p80,
                 172.30.0.42 . tcp . 443 : goto service-42NFTM6N-namespace2/service2/tcp/p443,
                 172.30.0.43 . tcp . 80 : goto service-4AT6LBPK-namespace3/service3/tcp/p80,
                 ... }
    }

  # 現在我們只需要一條規則來處理所有與映射中元素匹配的數據包。
  # （此規則表示："根據目標 IP 地址、第 4 層協議和目標端口構建一個元組；
  # 在 'service-ips' 中查找該元組；如果找到匹配項，則執行與之關聯的判定。"）
  chain services {
    ip daddr . meta l4proto . th dport vmap @service-ips
  }

  ...
}
```

<!--
Since there's only a single rule, with a roughly **O(1)** map lookup,
packet processing time is more or less constant regardless of cluster
size, and the best/average/worst cases are very similar:

{{< figure src="nftables-only.svg" alt="kube-proxy nftables first packet latency, at various percentiles, in clusters of various sizes" >}}
-->
由於只有一條規則，並且映射查找的時間複雜度大約爲 **O(1)**，因此資料包處理時間幾乎與叢集規模無關，
並且最佳、平均和最壞情況下的表現非常接近：

{{< figure src="nftables-only.svg" alt="kube-proxy nftables 在不同規模叢集中各百分位數下的第一個資料包延遲" >}}

<!--
But note the huge difference in the vertical scale between the
iptables and nftables graphs! In the clusters with 5000 and 10,000
Services, the p50 (average) latency for nftables is about the same as
the p01 (approximately best-case) latency for iptables. In the 30,000
Service cluster, the p99 (approximately worst-case) latency for
nftables manages to beat out the p01 latency for iptables by a few
microseconds! Here's both sets of data together, but you may have to
squint to see the nftables results!:

{{< figure src="iptables-vs-nftables.svg" alt="kube-proxy iptables-vs-nftables first packet latency, at various percentiles, in clusters of various sizes" >}}
-->
但請注意圖表中 iptables 和 nftables 之間在縱軸上的巨大差異！
在包含 5000 和 10,000 個 Service 的叢集中，nftables 的 p50（平均）延遲與 iptables
的 p01（接近最佳情況）延遲大致相同。
在包含 30,000 個 Service 的叢集中，nftables 的 p99（接近最壞情況）延遲比 iptables 的 p01 延遲快了幾微秒！
以下是兩組資料的對比圖，但你可能需要仔細觀察才能看到 nftables 的結果！

{{< figure src="iptables-vs-nftables.svg" alt="kube-proxy iptables 與 nftables 在不同規模叢集中各百分位數下的第一個資料包延遲對比" >}}

<!--
## Why nftables? Part 2: control plane latency

While the improvements to data plane latency in large clusters are
great, there's another problem with iptables kube-proxy that often
keeps users from even being able to grow their clusters to that size:
the time it takes kube-proxy to program new iptables rules when
Services and their endpoints change.
-->
## 爲什麼選擇 nftables？第二部分：控制平面延遲

雖然在大型叢集中資料平面延遲的改進非常顯著，但 iptables 模式的 kube-proxy 還存在另一個問題，
這往往使得使用者無法將叢集擴展到較大規模：那就是當 Service 及其端點發生變化時，kube-proxy
更新 iptables 規則所需的時間。

<!--
With both iptables and nftables, the total size of the ruleset as a
whole (actual rules, plus associated data) is **O(n)** in the combined
number of Services and their endpoints. Originally, the iptables
backend would rewrite every rule on every update, and with tens of
thousands of Services, this could grow to be hundreds of thousands of
iptables rules. Starting in Kubernetes 1.26, we began improving
kube-proxy so that it could skip updating _most_ of the unchanged
rules in each update, but the limitations of `iptables-restore` as an
API meant that it was still always necessary to send an update that's
**O(n)** in the number of Services (though with a noticeably smaller
constant than it used to be). Even with those optimizations, it can
still be necessary to make use of kube-proxy's `minSyncPeriod` config
option to ensure that it doesn't spend every waking second trying to
push iptables updates.
-->
對於 iptables 和 nftables，規則集的整體大小（實際規則加上相關資料）與 Service
及其端點的總數呈 **O(n)** 關係。原來，iptables 後端在每次更新時都會重寫所有規則，
當叢集中存在數萬個 Service 時，這可能導致規則數量增長至數十萬條 iptables 規則。
從 Kubernetes 1.26 開始，我們開始優化 kube-proxy，使其能夠在每次更新時跳過對大多數未更改規則的更新，
但由於 `iptables-restore` API 的限制，仍然需要發送與 Service 數量呈 **O(n)**
比例的更新（儘管常數因子比以前明顯減小）。即使進行了這些優化，有時仍需使用 kube-proxy 的
`minSyncPeriod` 設定選項，以確保它不會每秒鐘都在嘗試推送 iptables 更新。

<!--
The nftables APIs allow for doing much more incremental updates, and
when kube-proxy in nftables mode does an update, the size of the
update is only **O(n)** in the number of Services and endpoints that
have changed since the last sync, regardless of the total number of
Services and endpoints. The fact that the nftables API allows each
nftables-using component to have its own private table also means that
there is no global lock contention between components like with
iptables. As a result, kube-proxy's nftables updates can be done much
more efficiently than with iptables.

(Unfortunately I don't have cool graphs for this part.)
-->
nftables API 支持更爲增量化的更新，當以 nftables 模式運行的 kube-proxy 執行更新時，
更新的規模僅與自上次同步以來發生變化的 Service 和端點數量呈 **O(n)** 關係，而與總的 Service 和端點數量無關。
此外，由於 nftables API 允許每個使用 nftables 的組件擁有自己的私有表，因此不會像 iptables
那樣在組件之間產生全局鎖競爭。結果是，kube-proxy 在 nftables 模式下的更新可以比 iptables 模式下高效得多。

（不幸的是，這部分我沒有酷炫的圖表。）

<!--
## Why _not_ nftables? {#why-not-nftables}

All that said, there are a few reasons why you might not want to jump
right into using the nftables backend for now.

First, the code is still fairly new. While it has plenty of unit
tests, performs correctly in our CI system, and has now been used in
the real world by multiple users, it has not seen anything close to as
much real-world usage as the iptables backend has, so we can't promise
that it is as stable and bug-free.
-->
## 不選擇 nftables 的理由有哪些？  {#why-not-nftables}

儘管如此，仍有幾個原因可能讓你目前不希望立即使用 nftables 後端。

首先，該代碼仍然相對較新。雖然它擁有大量的單元測試，在我們的 CI 系統中表現正確，
並且已經在現實世界中被多個使用者使用，但其實際使用量遠遠不及 iptables 後端，
因此我們無法保證它同樣穩定且無缺陷。

<!--
Second, the nftables mode will not work on older Linux distributions;
currently it requires a 5.13 or newer kernel. Additionally, because of
bugs in early versions of the `nft` command line tool, you should not
run kube-proxy in nftables mode on nodes that have an old (earlier
than 1.0.0) version of `nft` in the host filesystem (or else
kube-proxy's use of nftables may interfere with other uses of nftables
on the system).
-->
其次，nftables 模式無法在較舊的 Linux 發行版上工作；目前它需要 5.13 或更高版本的內核。
此外，由於早期版本的 `nft` 命令列工具存在缺陷，不應在運行舊版本（早於 1.0.0）
`nft` 的節點主機檔案系統中上以 nftables 模式運行 kube-proxy（否則 kube-proxy
對 nftables 的使用可能會影響系統上其他程式對 nftables 的使用）。

<!--
Third, you may have other networking components in your cluster, such
as the pod network or NetworkPolicy implementation, that do not yet
support kube-proxy in nftables mode. You should consult the
documentation (or forums, bug tracker, etc.) for any such components
to see if they have problems with nftables mode. (In many cases they
will not; as long as they don't try to directly interact with or
override kube-proxy's iptables rules, they shouldn't care whether
kube-proxy is using iptables or nftables.) Additionally, observability
and monitoring tools that have not been updated may report less data
for kube-proxy in nftables mode than they do for kube-proxy in
iptables mode.
-->
第三，你的叢集中可能還存在其他網路組件，例如 Pod 網路或 NetworkPolicy 實現，
這些組件可能尚不支持以 nftables 模式運行的 kube-proxy。你應查閱相關組件的文檔（或論壇、問題跟蹤系統等），
以確認它們是否與 nftables 模式存在兼容性問題。（在許多情況下，它們並不會受到影響；
只要它們不嘗試直接操作或覆蓋 kube-proxy 的 iptables 規則，就不在乎 kube-proxy
使用的是 iptables 還是 nftables。）
此外，相較於 iptables 模式下，尚未更新的可觀測性和監控工具在 nftables
模式下可能會爲 kube-proxy 提供更少的資料。

<!--
Finally, kube-proxy in nftables mode is intentionally not 100%
compatible with kube-proxy in iptables mode. There are a few old
kube-proxy features whose default behaviors are less secure, less
performant, or less intuitive than we'd like, but where we felt that
changing the default would be a compatibility break. Since the
nftables mode is opt-in, this gave us a chance to fix those bad
defaults without breaking users who weren't expecting changes. (In
particular, with nftables mode, NodePort Services are now only
reachable on their nodes' default IPs, as opposed to being reachable
on all IPs, including `127.0.0.1`, with iptables mode.) The
[kube-proxy documentation] has more information about this, including
information about metrics you can look at to determine if you are
relying on any of the changed functionality, and what configuration
options are available to get more backward-compatible behavior.

[kube-proxy documentation]: https://kubernetes.io/docs/reference/networking/virtual-ips/#migrating-from-iptables-mode-to-nftables
-->
最後，以 nftables 模式運行的 kube-proxy 有意不與以 iptables 模式運行的 kube-proxy 完全兼容。
有一些較舊的 kube-proxy 功能，預設行爲不如我們期望的那樣安全、高效或直觀，但我們認爲更改預設行爲會導致兼容性問題。
由於 nftables 模式是可選的，這爲我們提供了一個機會，在不影響期望穩定性的使用者的情況下修復這些不良預設設置。
（特別是，在 nftables 模式下，NodePort 類型的 Service 現在僅在其節點的預設 IP 上可訪問，而在 iptables 模式下，
它們在所有 IP 上均可訪問，包括 `127.0.0.1`。）[kube-proxy 文檔] 提供了更多關於此方面的資訊，
包括如何通過查看某些指標來判斷你是否依賴於任何已更改的特性，以及有哪些設定選項可用於實現更向後兼容的行爲。

[kube-proxy 文檔]: https://kubernetes.io/zh-cn/docs/reference/networking/virtual-ips/#migrating-from-iptables-mode-to-nftables

<!--
## Trying out nftables mode

Ready to try it out? In Kubernetes 1.31 and later, you just need to
pass `--proxy-mode nftables` to kube-proxy (or set `mode: nftables` in
your kube-proxy config file).

If you are using kubeadm to set up your cluster, the kubeadm
documentation explains [how to pass a `KubeProxyConfiguration` to
`kubeadm init`]. You can also [deploy nftables-based clusters with
`kind`].
-->
## 嘗試使用 nftables 模式

準備嘗試了嗎？在 Kubernetes 1.31 及更高版本中，你只需將 `--proxy-mode nftables`
參數傳遞給 kube-proxy（或在 kube-proxy 設定檔案中設置 `mode: nftables`）。

如果你使用 kubeadm 部署叢集，kubeadm 文檔解釋了[如何向 `kubeadm init` 傳遞 `KubeProxyConfiguration`]。
你還可以[通過 `kind` 部署基於 nftables 的叢集]。
  
[如何向 `kubeadm init` 傳遞 `KubeProxyConfiguration`]: https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file  
[通過 `kind` 部署基於 nftables 的叢集]: https://kind.sigs.k8s.io/docs/user/configuration/#kube-proxy-mode

<!--
You can also convert existing clusters from iptables (or ipvs) mode to
nftables by updating the kube-proxy configuration and restarting the
kube-proxy pods. (You do not need to reboot the nodes: when restarting
in nftables mode, kube-proxy will delete any existing iptables or ipvs
rules, and likewise, if you later revert back to iptables or ipvs
mode, it will delete any existing nftables rules.)

[how to pass a `KubeProxyConfiguration` to `kubeadm init`]: /docs/setup/production-environment/tools/kubeadm/control-plane-flags/#customizing-kube-proxy
[deploy nftables-based clusters with `kind`]: https://kind.sigs.k8s.io/docs/user/configuration/#kube-proxy-mode
-->
你還可以通過更新 kube-proxy 設定並重啓 kube-proxy Pod，將現有叢集從
iptables（或 ipvs）模式轉換爲 nftables 模式。（無需重啓節點：
在以 nftables 模式重新啓動時，kube-proxy 會刪除現有的所有 iptables 或 ipvs 規則；
同樣，如果你之後切換回 iptables 或 ipvs 模式，它將刪除現有的所有 nftables 規則。）

[如何向 `kubeadm init` 傳遞 `KubeProxyConfiguration`]: /zh-cn/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#customizing-kube-proxy
[通過 `kind` 部署基於 nftables 的叢集]: https://kind.sigs.k8s.io/docs/user/configuration/#kube-proxy-mode

<!--
## Future plans

As mentioned above, while nftables is now the _best_ kube-proxy mode,
it is not the _default_, and we do not yet have a plan for changing
that. We will continue to support the iptables mode for a long time.

The future of the IPVS mode of kube-proxy is less certain: its main
advantage over iptables was that it was faster, but certain aspects of
the IPVS architecture and APIs were awkward for kube-proxy's purposes
(for example, the fact that the `kube-ipvs0` device needs to have
_every_ Service IP address assigned to it), and some parts of
Kubernetes Service proxying semantics were difficult to implement
using IPVS (particularly the fact that some Services had to have
different endpoints depending on whether you connected to them from a
local or remote client). And now, the nftables mode has the same
performance as IPVS mode (actually, slightly better), without any of
the downsides:
-->
## 未來計劃

如上所述，雖然 nftables 現在是的 kube-proxy 的最佳模式，但它還不是預設模式，
我們目前還沒有更改這一設置的計劃。我們將繼續長期支持 iptables 模式。

kube-proxy 的 IPVS 模式的未來則不太確定：它相對於 iptables 的主要優勢在於速度更快，
但 IPVS 的架構和 API 在某些方面對 kube-proxy 來說不夠理想（例如，`kube-ipvs0`
設備需要被分配所有 Service IP 地址），
並且 Kubernetes Service 代理的部分語義使用 IPVS 難以實現（特別是某些
Service 根據連接的客戶端是本地還是遠程，需要有不同的端點）。
現在，nftables 模式的性能與 IPVS 模式相同（實際上略勝一籌），而且沒有任何缺點：

<!--
{{< figure src="ipvs-vs-nftables.svg" alt="kube-proxy ipvs-vs-nftables first packet latency, at various percentiles, in clusters of various sizes" >}}

(In theory the IPVS mode also has the advantage of being able to use
various other IPVS functionality, like alternative "schedulers" for
balancing endpoints. In practice, this ended up not being very useful,
because kube-proxy runs independently on every node, and the IPVS
schedulers on each node had no way of sharing their state with the
proxies on other nodes, thus thwarting the effort to balance traffic
more cleverly.)
-->
{{< figure src="ipvs-vs-nftables.svg" alt="kube-proxy IPVS 與 nftables 在不同規模叢集中各百分位數下的第一個資料包延遲對比" >}}

（理論上，IPVS 模式還具有可以使用其他 IPVS 功能的優勢，例如使用替代的"調度器"來平衡端點。
但實際上，這並不太有用，因爲 kube-proxy 在每個節點上獨立運行，每個節點上的 IPVS
調度器無法與其他節點上的代理共享狀態，從而無法實現更智能的流量均衡。）

<!--
While the Kubernetes project does not have an immediate plan to drop
the IPVS backend, it is probably doomed in the long run, and people
who are currently using IPVS mode should try out the nftables mode
instead (and file bugs if you think there is missing functionality in
nftables mode that you can't work around).
-->
雖然 Kubernetes 項目目前沒有立即放棄 IPVS 後端的計劃，但從長遠來看，IPVS 可能難逃被淘汰的命運。
目前使用 IPVS 模式的使用者應嘗試使用 nftables 模式（如果發現 nftables 模式中缺少某些無法繞過的功能，
請提交問題報告）。

<!--
## Learn more

- "[KEP-3866: Add an nftables-based kube-proxy backend]" has the
  history of the new feature.

- "[How the Tables Have Turned: Kubernetes Says Goodbye to IPTables]",
  from KubeCon/CloudNativeCon North America 2024, talks about porting
  kube-proxy and Calico from iptables to nftables.

- "[From Observability to Performance]", from KubeCon/CloudNativeCon
  North America 2024. (This is where the kube-proxy latency data came
  from; the [raw data for the charts] is also available.)
-->
## 進一步瞭解

- "[KEP-3866: Add an nftables-based kube-proxy backend]" 記錄了此新特性的歷史。

- "[How the Tables Have Turned: Kubernetes Says Goodbye to IPTables]"，來自 2024 年
  KubeCon/CloudNativeCon 北美大會，討論了將 kube-proxy 和 Calico 從 iptables 遷移到 nftables 的過程。

- "[From Observability to Performance]"，同樣來自 2024 年 KubeCon/CloudNativeCon 北美大會。
 （kube-proxy 延遲資料來源於此；[raw data for the charts] 也可用。）

[KEP-3866: Add an nftables-based kube-proxy backend]: https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/3866-nftables-proxy/README.md
[How the Tables Have Turned: Kubernetes Says Goodbye to IPTables]: https://youtu.be/yOGHb2HjslY?si=6O4PVJu7fGpReo1U
[From Observability to Performance]: https://youtu.be/uYo2O3jbJLk?si=py2AXzMJZ4PuhxNg
[raw data for the charts]: https://docs.google.com/spreadsheets/d/1-ryDNc6gZocnMHEXC7mNtqknKSOv5uhXFKDx8Hu3AYA/edit
