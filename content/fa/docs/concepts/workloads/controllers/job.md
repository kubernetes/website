---
reviewers:
- moh0ps
title: کارها
api_metadata:
- apiVersion: "batch/v1"
  kind: "Job"
content_type: concept
description: >-
  Jobs represent one-off tasks that run to completion and then stop.
feature:
  title: Batch execution
  description: >
    In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.
weight: 50
hide_summary: true # Listed separately in section index
---

<!-- overview -->

یک کار یک یا چند پاد ایجاد می‌کند و تا زمانی که تعداد مشخصی از آنها با موفقیت خاتمه نیابند، به تلاش مجدد برای اجرای پادها ادامه می‌دهد. با تکمیل موفقیت‌آمیز پادها، کار تکمیل‌های موفقیت‌آمیز را پیگیری می‌کند. هنگامی که تعداد مشخصی از تکمیل‌های موفقیت‌آمیز حاصل شود، کار  تکمیل شده است. حذف یک کار، پادهایی را که ایجاد کرده است پاک می‌کند. تعلیق یک کار، پادهای فعال آن را تا زمانی که کار دوباره از سر گرفته شود، حذف می‌کند.

یک مورد ساده، ایجاد یک شیء کار به منظور اجرای مطمئن یک پاد تا زمان تکمیل است. شیء کار در صورت خرابی یا حذف اولین پاد (مثلاً به دلیل خرابی سخت‌افزار گره یا راه‌اندازی مجدد گره)، یک پاد جدید را آغاز می‌کند.

همچنین می‌توانید از یک کار برای اجرای موازی چندین پاد استفاده کنید.

اگر می‌خواهید یک کار (چه یک کار واحد، چه چندین کار به صورت موازی) را در یک برنامه اجرا کنید، به [CronJob](/docs/concepts/workloads/controllers/cron-jobs/) مراجعه کنید.

<!-- body -->

## اجرای یک مثال کار

در اینجا یک نمونه پیکربندی کار آورده شده است. این کار عدد π را تا ۲۰۰۰ رقم اعشار محاسبه کرده و چاپ می‌کند. تکمیل این عملیات حدود ۱۰ ثانیه طول می‌کشد.

{{% code_sample file="controllers/job.yaml" %}}

می‌توانید مثال را با این دستور اجرا کنید:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
```

خروجی مشابه این است:

```
job.batch/pi created
```

وضعیت کار را با `kubectl` بررسی کنید:

{{< tabs name="Check status of Job" >}}
{{< tab name="kubectl describe job pi" codelang="bash" >}}
Name:           pi
Namespace:      default
Selector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                batch.kubernetes.io/job-name=pi
                ...
Annotations:    batch.kubernetes.io/job-tracking: ""
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           batch.kubernetes.io/job-name=pi
  Containers:
   pi:
    Image:      perl:5.34.0
    Port:       <none>
    Host Port:  <none>
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4
  Normal  Completed         18s   job-controller  Job completed
{{< /tab >}}
{{< tab name="kubectl get job pi -o yaml" codelang="bash" >}}
apiVersion: batch/v1
kind: Job
metadata:
  annotations: batch.kubernetes.io/job-tracking: ""
             ...  
  creationTimestamp: "2022-11-10T17:53:53Z"
  generation: 1
  labels:
    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
    batch.kubernetes.io/job-name: pi
  name: pi
  namespace: default
  resourceVersion: "4751"
  uid: 204fb678-040b-497f-9266-35ffa8716d14
spec:
  backoffLimit: 4
  completionMode: NonIndexed
  completions: 1
  parallelism: 1
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
  suspend: false
  template:
    metadata:
      creationTimestamp: null
      labels:
        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
        batch.kubernetes.io/job-name: pi
    spec:
      containers:
      - command:
        - perl
        - -Mbignum=bpi
        - -wle
        - print bpi(2000)
        image: perl:5.34.0
        imagePullPolicy: IfNotPresent
        name: pi
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  active: 1
  ready: 0
  startTime: "2022-11-10T17:53:57Z"
  uncountedTerminatedPods: {}
{{< /tab >}}
{{< /tabs >}}

برای مشاهده پادهای تکمیل‌شده‌ی یک کار، از دستور `kubectl get pods` استفاده کنید.

برای فهرست کردن تمام پادهای متعلق به یک کار به شکلی که برای ماشین قابل خواندن باشد، می‌توانید از دستوری مانند این استفاده کنید:

```shell
pods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')
echo $pods
```

خروجی مشابه این است:

```
pi-5rwd7
```

در اینجا، انتخابگر همان انتخابگر برای کار است. گزینه `--output=jsonpath` عبارتی را با نام هر پاد در لیست برگشتی مشخص می‌کند.

خروجی استاندارد یکی از پادها را مشاهده کنید:

```shell
kubectl logs $pods
```

راه دیگری برای مشاهده گزارش‌های یک کار:

```shell
kubectl logs jobs/pi
```

خروجی مشابه این است:

```
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
```

## نوشتن مشخصات کار

مانند سایر پیکربندی‌های کوبرنتیز، یک کار به بخش های `apiVersion`، `kind` و `metadata` نیاز دارد.

وقتی صفحه کنترل، پادهای جدیدی برای یک کار ایجاد می‌کند، `.metadata.name` کار، بخشی از مبنای نامگذاری آن پادها است. نام یک کار باید یک مقدار معتبر باشد، اما این می‌تواند نتایج غیرمنتظره‌ای برای نام‌های میزبان پاد ایجاد کند. برای بهترین سازگاری، نام باید از قوانین محدودکننده‌تر برای یک `[DNS label](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)` پیروی کند. حتی وقتی نام یک زیر دامنه DNS است، نام نباید بیش از ۶۳ کاراکتر باشد.

یک کار همچنین به یک بخش [`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) نیاز دارد.

### برچسب‌های کار

برچسب‌های کار، پیشوند `batch.kubernetes.io/` را برای `job-name` و `controller-uid` خواهند داشت.

### قالب پاد

تنها بخش الزامی در پرونده `.spec`، ``.spec.template`` است.

`.spec.template` یک [قالب پاد](/docs/concepts/workloads/pods/#pod-templates) است. این قالب دقیقاً همان طرحواره {{< glossary_tooltip text="Pod" term_id="pod" >}} را دارد، با این تفاوت که تودرتو است و `apiVersion` یا `kind` ندارد.

علاوه بر بخش های الزامی برای یک پاد، یک قالب پاد در یک کار باید برچسب‌های مناسب (به [انتخابگر پاد](#pod-selector) مراجعه کنید) و یک سیاست راه‌اندازی مجدد مناسب را مشخص کند.

فقط یک [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) برابر با `Never` یا `OnFailure` مجاز است.

### انتخابگر پاد

بخش `.spec.selector` اختیاری است. تقریباً در همه موارد نباید آن را مشخص کنید. به بخش [تعیین انتخابگر پاد خودتان](#specifying-your-own-pod-selector) مراجعه کنید.

### اجرای موازی برای کارها {#parallel-jobs}

سه نوع اصلی از وظایف مناسب برای اجرا به عنوان کار وجود دارد:

1. کارهای غیرموازی
   - معمولاً فقط یک پاد شروع به کار می‌کند، مگر اینکه پاد از کار بیفتد.
   - به محض اینکه پاد آن با موفقیت خاتمه یابد، کار کامل می‌شود.
1. کارهای موازی با *تعداد تکمیل ثابت*:
   - یک مقدار مثبت غیر صفر برای `.spec.completions` تعیین کنید.
   - این کار نشان دهنده‌ی وظیفه‌ی کلی است و زمانی کامل می‌شود که پادهای `.spec.completions` با موفقیت انجام شوند.
   - هنگام استفاده از `.spec.completionMode="Indexed"`، هر پاد یک شاخص متفاوت در محدوده 0 تا `.spec.completions-1` دریافت می‌کند.
1. کارهای موازی با *صف کار*:
   - مقدار `.spec.completions` را مشخص نکنید، مقدار پیش‌فرض `.spec.parallelism` است.
   - پادها باید با یکدیگر یا با یک سرویس خارجی هماهنگ شوند تا مشخص شود که هر کدام روی چه چیزی کار کنند. برای مثال، یک پاد ممکن است دسته‌ای تا N مورد را از صف کار دریافت کند.
   - هر پاد به طور مستقل قادر است تعیین کند که آیا همه همتایانش انجام شده‌اند یا خیر، و بنابراین کل کار انجام شده است.
   - وقتی هر پاد از کار با موفقیت خاتمه یابد، هیچ پاد جدیدی ایجاد نمی‌شود.
   - زمانی که حداقل یک پاد با موفقیت خاتمه یابد و همه پادها خاتمه یابند، آنگاه کار با موفقیت به پایان رسیده است.
   - وقتی هر پاد با موفقیت خارج شد، هیچ پاد دیگری نباید هنوز در حال انجام کاری برای این وظیفه یا نوشتن خروجی باشد. همه آنها باید در حال خروج باشند.

برای یک کار _غیر موازی_، می‌توانید هر دو `.spec.completions` و `.spec.parallelism` را تنظیم نشده رها کنید. وقتی هر دو تنظیم نشده باشند، هر دو به طور پیش‌فرض روی ۱ تنظیم می‌شوند.

برای یک کار با تعداد تکمیل ثابت، باید `.spec.completions` را روی تعداد تکمیل‌های مورد نیاز تنظیم کنید. می‌توانید `.spec.parallelism` را تنظیم کنید، یا آن را بدون تنظیم رها کنید تا به طور پیش‌فرض روی ۱ باشد.

برای یک کار در صف کار، باید `.spec.completions` را تنظیم نشده بگذارید و `.spec.parallelism` را روی یک عدد صحیح غیر منفی تنظیم کنید.

برای اطلاعات بیشتر در مورد نحوه استفاده از انواع مختلف کار، به بخش [الگوهای کاری] (#job-patterns) مراجعه کنید.

#### کنترل موازی‌سازی

موازی‌سازی درخواستی (`.spec.parallelism`) می‌تواند روی هر مقدار غیرمنفی تنظیم شود.
اگر مقدار آن مشخص نشده باشد، به طور پیش‌فرض ۱ در نظر گرفته می‌شود.
اگر به عنوان ۰ مشخص شود، آنگاه کار تا زمانی که افزایش یابد، عملاً متوقف می‌شود.

موازی‌سازی واقعی (تعداد پادهای در حال اجرا در هر لحظه) ممکن است به دلایل مختلف بیشتر یا کمتر از موازی‌سازی درخواستی باشد:

- برای تعداد تکمیل ثابت کارها، تعداد واقعی پادهایی که به صورت موازی اجرا می‌شوند از تعداد تکمیل‌های باقی‌مانده تجاوز نخواهد کرد. مقادیر بالاتر از `.spec.parallelism` عملاً نادیده گرفته می‌شوند.
- برای کارهای صف کار، پس از موفقیت هر پاد، هیچ پاد جدیدی شروع نمی‌شود -- با این حال، پادهای باقی مانده مجاز به تکمیل هستند.
- اگر کار {{< glossary_tooltip term_id="controller" >}} زمان کافی برای واکنش نشان دادن نداشته باشد.
- اگر کنترل‌کننده‌ی کار به هر دلیلی (کمبود `ResourceQuota`، کمبود مجوز و غیره) در ایجاد پادها شکست بخورد، ممکن است پادهای کمتری نسبت به تعداد درخواستی وجود داشته باشد.
- ممکن است کنترل‌کننده‌ی کار، به دلیل خرابی‌های بیش از حد پادهای قبلی در همان کار، ایجاد پاد جدید را متوقف کند.
- وقتی یک پاد به طرز ماهرانه‌ای خاموش می‌شود، متوقف کردن آن زمان می‌برد.

### حالت تکمیل

{{< feature-state for_k8s_version="v1.24" state="stable" >}}

کارهایی با تعداد تکمیل ثابت - یعنی کارهایی که مقدار null ندارند.
`.spec.completions` - می‌توانند حالت تکمیل داشته باشند که در `.spec.completionMode` مشخص شده است:

- `NonIndexed` (پیش‌فرض): کار زمانی کامل در نظر گرفته می‌شود که پادهای `.spec.completions` با موفقیت تکمیل شده باشند. به عبارت دیگر، تکمیل هر پاد با یکدیگر مشابه است. توجه داشته باشید که کارهایی که `.spec.completions` تهی دارند، به طور ضمنی `NonIndexed` هستند.
- `Indexed`: پادهای یک کار، یک اندیس تکمیل مرتبط از ۰ تا «.spec.completions-1» دریافت می‌کنند. این اندیس از طریق چهار سازوکار در دسترس است:
  - حاشیه‌نویسی پاد با عنوان `batch.kubernetes.io/job-completion-index`.
  - برچسب پاد با عنوان `batch.kubernetes.io/job-completion-index` (برای نسخه ۱.۲۸ و بالاتر). توجه داشته باشید که برای استفاده از این برچسب، باید ویژگی گیت `PodIndexLabel` فعال باشد و به طور پیش‌فرض نیز فعال است.
  - به عنوان بخشی از نام میزبان پاد، از الگوی `$(job-name)-$(index)` پیروی می‌کند. هنگامی که از یک کار فهرست‌بندی شده در ترکیب با یک {{< glossary_tooltip term_id="Service" >}} استفاده می‌کنید، پادهای درون کار می‌توانند از نام‌های میزبان قطعی برای نشانی دادن به یکدیگر از طریق DNS استفاده کنند. برای اطلاعات بیشتر در مورد نحوه پیکربندی این، به [کار با ارتباط پاد به پاد](/docs/tasks/job/job-with-pod-to-pod-communication/) مراجعه کنید.
  - از وظیفه‌ی کانتینر شده، در متغیر محیطی `JOB_COMPLETION_INDEX`.
  
  یک کار زمانی کامل در نظر گرفته می‌شود که برای هر شاخص، یک پاد با موفقیت تکمیل شده باشد. برای اطلاعات بیشتر در مورد نحوه استفاده از این حالت، به [کار شاخص‌گذاری شده برای پردازش موازی با تخصیص کار استاتیک](/docs/tasks/job/indexed-parallel-processing-static/) مراجعه کنید.

{{< note >}}
اگرچه نادر است، اما ممکن است بیش از یک پاد برای یک شاخص مشابه شروع شود (به دلایل مختلفی مانند خرابی گره، راه‌اندازی مجدد kubelet یا حذف پاد). در این حالت، فقط اولین پاد که با موفقیت تکمیل شود، در شمارش تکمیل حساب می‌شود و وضعیت کار را به‌روزرسانی می‌کند. پاد های دیگری که برای همان شاخص در حال اجرا هستند یا تکمیل شده‌اند، پس از شناسایی توسط کنترل‌کننده کار حذف می‌شوند.
{{< /note >}}

## مدیریت خرابی‌های پاد و کانتینر

یک کانتینر در یک پاد ممکن است به دلایل مختلفی از کار بیفتد، مانند اینکه فرآیند موجود در آن با یک کد خروج غیر صفر خارج شده باشد، یا کانتینر به دلیل تجاوز از حد حافظه از بین رفته باشد و غیره. اگر این اتفاق بیفتد، و `.spec.template.spec.restartPolicy = "OnFailure"` باشد، پاد روی گره باقی می‌ماند، اما کانتینر دوباره اجرا می‌شود. بنابراین، برنامه شما باید هنگام راه‌اندازی مجدد محلی، این مورد را مدیریت کند، یا در غیر این صورت `.spec.template.spec.restartPolicy = "Never"` را مشخص کند.
برای اطلاعات بیشتر در مورد `restartPolicy` به [چرخه عمر پاد](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) مراجعه کنید.

کل یک پاد همچنین می‌تواند به دلایل مختلفی از کار بیفتد، مانند زمانی که پاد از گره جدا می‌شود (گره ارتقا می‌یابد، راه‌اندازی مجدد می‌شود، حذف می‌شود و غیره)، یا اگر یک کانتینر از پاد از کار بیفتد و `.spec.template.spec.restartPolicy = "Never"`. وقتی یک پاد از کار می‌افتد، کنترل‌کننده‌ی کار یک پاد جدید را شروع می‌کند. این بدان معناست که برنامه‌ی شما باید هنگام راه‌اندازی مجدد در یک پاد جدید، این مورد را مدیریت کند. به طور خاص، باید فایل‌های موقت، قفل‌ها، خروجی ناقص و موارد مشابه ناشی از اجراهای قبلی را مدیریت کند.

به طور پیش‌فرض، هر خرابی پاد در محدوده `.spec.backoffLimit` محاسبه می‌شود، به [سیاست خرابی قطع اتصال پاد](#pod-backoff-failure-policy) مراجعه کنید. با این حال، می‌توانید با تنظیم [سیاست خرابی پاد](#pod-failure-policy) مربوط به کار، نحوه مدیریت خرابی‌های پاد را سفارشی کنید.

علاوه بر این، می‌توانید با تنظیم بخش `.spec.backoffLimitPerIndex`، تعداد خرابی‌های پاد را برای هر شاخص از یک کار [Indexed](#completion-mode) به طور مستقل بشمارید (برای اطلاعات بیشتر، به [محدودیت عقب‌نشینی برای هر شاخص](#backoff-limit-per-index) مراجعه کنید).

توجه داشته باشید که حتی اگر `.spec.parallelism = 1` و `.spec.completions = 1` و `.spec.template.spec.restartPolicy = "Never"` را مشخص کنید، ممکن است گاهی اوقات همان برنامه دو بار شروع شود.

اگر هر دو مقدار `.spec.parallelism` و `.spec.completions` را بزرگتر از ۱ تعیین کنید، ممکن است چندین پاد به طور همزمان اجرا شوند. بنابراین، پادهای شما باید تحمل همزمانی را نیز داشته باشند.

اگر بخش `.spec.podFailurePolicy` را مشخص کنید، کنترل‌کننده‌ی کار، یک پاد در حال خاتمه (پاد ای که بخش `.metadata.deletionTimestamp` آن تنظیم شده باشد) را تا زمانی که پاد به حالت پایانی (`.status.phase` آن `Failed` یا `Succeeded` باشد) نرسد، به عنوان یک خرابی در نظر نمی‌گیرد. با این حال، کنترل‌کننده‌ی کار به محض آشکار شدن خاتمه، یک پاد جایگزین ایجاد می‌کند. پس از خاتمه‌ی پاد، کنترل‌کننده‌ی کار، `.backoffLimit` و `.podFailurePolicy` را برای کار مربوطه ارزیابی می‌کند و این پاد که اکنون خاتمه یافته است را در نظر می‌گیرد.

اگر هر یک از این الزامات برآورده نشود، کنترل‌کننده‌ی کار، یک پادِ در حالِ خاتمه را به عنوان یک شکستِ فوری در نظر می‌گیرد، حتی اگر آن پاد بعداً با `مرحله: "موفق"` خاتمه یابد.

### سیاست شکست در قطع ارتباط پاد

موقعیت‌هایی وجود دارد که می‌خواهید یک کار پس از تعدادی تلاش مجدد - به دلیل خطای منطقی در پیکربندی و غیره - با شکست مواجه شود. برای انجام این کار، `.spec.backoffLimit` را برای مشخص کردن تعداد تلاش‌های مجدد قبل از اینکه یک کار به عنوان شکست خورده در نظر گرفته شود، تنظیم کنید.

مقدار `.spec.backoffLimit` به طور پیش‌فرض روی ۶ تنظیم شده است، مگر اینکه `[محدودیت عقب‌نشینی برای هر شاخص](#backoff-limit-per-index) (فقط کار شاخص شده) مشخص شده باشد. وقتی `.spec.backoffLimitPerIndex` مشخص شده باشد، مقدار `.spec.backoffLimit` به طور پیش‌فرض روی ۲۱۴۷۴۸۳۶۴۷ (MaxInt32) تنظیم می‌شود.

پادهای از کار افتاده مرتبط با کار، توسط کنترل‌کننده کار با یک تأخیر بازگشت نمایی (10، 20، 40 ثانیه ...) که حداکثر شش دقیقه است، دوباره ایجاد می‌شوند.

تعداد دفعات تکرار به دو روش محاسبه می‌شود:

- تعداد پادهایی که `.status.phase = "Failed"` دارند.
- هنگام استفاده از `restartPolicy = "OnFailure"`، تعداد تلاش‌های مجدد در تمام کانتینرهای پادها با `.status.phase` برابر با `Pending` یا `Running` است.

اگر هر یک از محاسبات به `.spec.backoffLimit` برسد، کار ناموفق تلقی می‌شود.

{{< note >}}
اگر کار شما دارای `restartPolicy = "OnFailure"` باشد، به خاطر داشته باشید که پاد شما که کار را اجرا می‌کند، پس از رسیدن به حد مجاز backoff job خاتمه می‌یابد. این می‌تواند اشکال‌زدایی از فایل اجرایی کار را دشوارتر کند. پیشنهاد می‌کنیم هنگام اشکال‌زدایی کار یا استفاده از سیستم ثبت وقایع، `restartPolicy = "Never"` را تنظیم کنید تا مطمئن شوید خروجی کارهای ناموفق سهواً از بین نمی‌روند.
{{< /note >}}

### محدودیت عقب نشینی به ازای هر شاخص {#backoff-limit-per-index}

{{< feature-state feature_gate_name="JobBackoffLimitPerIndex" >}}

وقتی یک کار [indexed](#completion-mode) را اجرا می‌کنید، می‌توانید انتخاب کنید که تلاش‌های مجدد برای خرابی‌های پاد برای هر شاخص به طور مستقل مدیریت شوند. برای انجام این کار، مقدار `.spec.backoffLimitPerIndex` را برای مشخص کردن حداکثر تعداد خرابی‌های پاد در هر شاخص تنظیم کنید.

وقتی محدودیت عقب نشینی به ازای هر شاخص برای یک شاخص از حد مجاز فراتر رود، کوبرنتیز آن شاخص را ناموفق در نظر می‌گیرد و آن را به  `.status.failedIndexes` اضافه می‌کند. شاخص‌های موفق، آن‌هایی که پادهایشان با موفقیت اجرا شده است، صرف نظر از اینکه فیلد `backoffLimitPerIndex` را تنظیم کرده‌اید یا خیر، در فیلد `.status.completedIndexes` ثبت می‌شوند.

توجه داشته باشید که یک شاخص ناموفق، اجرای سایر شاخص‌ها را مختل نمی‌کند. پس از اتمام تمام شاخص‌ها برای کاری که در آن محدودیت بازگشت به ازای هر شاخص تعیین کرده‌اید، اگر حداقل یکی از آن شاخص‌ها ناموفق باشد، کنترل‌کننده کار با تنظیم شرط ناموفق در وضعیت، کل کار را به عنوان ناموفق علامت‌گذاری می‌کند. حتی اگر برخی، یا تقریباً همه شاخص‌ها با موفقیت پردازش شده باشند، کار به عنوان ناموفق علامت‌گذاری می‌شود.

علاوه بر این، می‌توانید با تنظیم بخش `.spec.maxFailedIndexes` حداکثر تعداد شاخص‌های علامت‌گذاری شده با عنوان ناموفق را محدود کنید. هنگامی که تعداد شاخص‌های ناموفق از بخش `maxFailedIndexes` بیشتر شود، کنترل کننده کار باعث خاتمه تمام پادهای در حال اجرای باقی‌مانده برای آن کار می‌شود. پس از خاتمه همه پادها، کل کار توسط کنترل کننده کار با تنظیم شرط Failed در وضعیت کار، ناموفق علامت‌گذاری می‌شود.

در اینجا مثالی از پرونده تنظیمات برای یک کار که یک `backoffLimitPerIndex` را تعریف می‌کند، آورده شده است:

{{< code_sample file="/controllers/job-backoff-limit-per-index-example.yaml" >}}

در مثال بالا، کنترل‌کننده‌ی کار برای هر یک از شاخص‌ها امکان یک بار راه‌اندازی مجدد را فراهم می‌کند. وقتی تعداد کل شاخص‌های ناموفق از ۵ فراتر رود، کل کار خاتمه می‌یابد.

پس از اتمام کار، وضعیت کار به صورت زیر نمایش داده می‌شود:

```sh
kubectl get -o yaml job job-backoff-limit-per-index-example
```

```yaml
  status:
    completedIndexes: 1,3,5,7,9
    failedIndexes: 0,2,4,6,8
    succeeded: 5          # برای هر یک از ۵ شاخص موفق، ۱ پاد موفق وجود دارد
    failed: 10            # ۲ پاد ناموفق (۱ تلاش مجدد) برای هر یک از ۵ ایندکس ناموفق
    conditions:
    - message: Job has failed indexes
      reason: FailedIndexes
      status: "True"
      type: FailureTarget
    - message: Job has failed indexes
      reason: FailedIndexes
      status: "True"
      type: Failed
```

کنترل‌کننده‌ی کار، شرط کار `FailureTarget` را برای فعال کردن [خاتمه و پاکسازی کار](#job-termination-and-cleanup) اضافه می‌کند. هنگامی که تمام `Job Pods` خاتمه می‌یابند، کنترل‌کننده‌ی کار، شرط `Failed` را با همان مقادیر `reason` و `message` به عنوان شرط کار `FailureTarget` اضافه می‌کند. برای جزئیات بیشتر، به [خاتمه و پاکسازی کار Pods](#termination-of-job-pods) مراجعه کنید.

علاوه بر این، ممکن است بخواهید از عقب نشینی به ازای هر شاخص به همراه یک سیاست خرابی پاد (#pod-failure-policy) استفاده کنید. هنگام استفاده از عقب نشینی به ازای هر شاخص، یک اقدام جدید `FailIndex` در دسترس است که به شما امکان می‌دهد از تلاش‌های مجدد غیرضروری در یک شاخص جلوگیری کنید.

### سیاست خرابی پاد {#pod-failure-policy}

{{< feature-state feature_gate_name="JobPodFailurePolicy" >}}

یک سیاست خرابی پاد، که با بخش `.spec.podFailurePolicy` تعریف می‌شود، خوشه شما را قادر می‌سازد تا خرابی‌های پاد را بر اساس کدهای خروج کانتینر و شرایط پاد مدیریت کند.

در برخی شرایط، ممکن است بخواهید هنگام مدیریت خرابی‌های پاد، کنترل بهتری نسبت به کنترل ارائه شده توسط [سیاست شکست عقب نشینی پاد](#pod-backoff-failure-policy) داشته باشید، که بر اساس `.spec.backoffLimit` مربوط به کار است. اینها چند نمونه از موارد استفاده هستند:

* برای بهینه‌سازی هزینه‌های اجرای حجم کار با اجتناب از راه‌اندازی مجدد غیرضروری پاد، می‌توانید به محض اینکه یکی از پادهای یک کار از کار افتاد، آن را با یک کد خروج که نشان‌دهنده‌ی یک اشکال نرم‌افزاری است، خاتمه دهید.
* برای تضمین اینکه کار شما حتی در صورت وجود اختلال به پایان می‌رسد، می‌توانید از خرابی‌های پاد ناشی از اختلالات (مانند {{< glossary_tooltip text="preemption" term_id="preemption" >}}، {{< glossary_tooltip text="API-initiated eviction" term_id="api-eviction" >}} یا {{< glossary_tooltip text="taint" term_id="taint" >}}-based eviction) صرف نظر کنید تا در محدودیت `.spec.backoffLimit` تلاش‌های مجدد محاسبه نشوند.

شما می‌توانید یک سیاست خرابی پاد را در بخش `.spec.podFailurePolicy` پیکربندی کنید تا موارد استفاده فوق را برآورده کند. این سیاست می‌تواند خرابی‌های پاد را بر اساس کدهای خروج کانتینر و شرایط پاد مدیریت کند.

در اینجا تنظیمات برای یک کار وجود دارد که یک `podFailurePolicy` را تعریف می‌کند:

{{% code_sample file="/controllers/job-pod-failure-policy-example.yaml" %}}

در مثال بالا، اولین قانون سیاست شکست پاد مشخص می‌کند که اگر کانتینر «اصلی» با کد خروج ۴۲ شکست بخورد، کار باید به عنوان «شکست خورده» علامت‌گذاری شود. در ادامه قوانین مربوط به کانتینر «اصلی» به طور خاص آمده است:

- کد خروج ۰ به این معنی است که کانتینر با موفقیت اجرا شده است.
- کد خروج ۴۲ به این معنی است که **کل کار** با شکست مواجه شده است
- هر کد خروج دیگری نشان می‌دهد که کانتینر و در نتیجه کل پاد با شکست مواجه شده‌اند. اگر تعداد کل راه‌اندازی‌های مجدد کمتر از `backoffLimit` باشد، پاد دوباره ایجاد خواهد شد. اگر به `backoffLimit` برسد، **کل کار** با شکست مواجه می‌شود.

{{< note >}}
از آنجا که قالب پاد یک `restartPolicy: Never` را مشخص می‌کند، kubelet کانتینر `main` را در آن پاد خاص مجدداً راه‌اندازی نمی‌کند.
{{< /note >}}

دومین قانون سیاست خرابی پاد، که عمل `Ignore` را برای پادهای خراب با شرط «DisruptionTarget» مشخص می‌کند، اختلالات پاد را از شمارش در محدوده‌ی «.spec.backoffLimit» تلاش‌های مجدد مستثنی می‌کند.

{{< note >}}
اگر کار چه توسط سیاست خرابی پاد و چه توسط سیاست خرابی عقب نشینی پاد با شکست مواجه شود، و کار در حال اجرای چندین پاد باشد، کوبرنتیز تمام پاد های موجود در آن کار را که هنوز در انتظار یا در حال اجرا هستند، خاتمه می‌دهد.
{{< /note >}}

اینها برخی از الزامات و معانی API هستند:

- اگر می‌خواهید از بخش `.spec.podFailurePolicy` برای یک کار استفاده کنید، باید قالب پاد آن کار را نیز با `.spec.restartPolicy` که روی `Never` تنظیم شده است، تعریف کنید.
- قوانین مربوط به خطای پاد که در زیر `spec.podFailurePolicy.rules` مشخص می‌کنید، به ترتیب ارزیابی می‌شوند. هنگامی که یک قانون با خطای پاد مطابقت داشته باشد، قوانین باقی‌مانده نادیده گرفته می‌شوند. هنگامی که هیچ قانونی با خطای پاد مطابقت نداشته باشد، مدیریت پیش‌فرض اعمال می‌شود.
- ممکن است بخواهید با مشخص کردن نام یک کانتینر در `spec.podFailurePolicy.rules[*].onExitCodes.containerName`، یک قانون را به یک کانتینر خاص محدود کنید. در صورت عدم مشخص شدن، قانون برای همه کانتینرها اعمال می‌شود. در صورت مشخص شدن، باید با یکی از نام‌های کانتینر یا `initContainer` در الگوی پاد مطابقت داشته باشد.
- می‌توانید اقدامی را که هنگام مطابقت با سیاست خرابی پاد با `spec.podFailurePolicy.rules[*].action` انجام می‌شود، مشخص کنید. مقادیر ممکن عبارتند از:
  - `FailJob`: برای نشان دادن اینکه کار پاد باید به عنوان ناموفق علامت‌گذاری شود و تمام پادهای در حال اجرا باید خاتمه یابند، استفاده می‌شود.
  - `Ignore`: برای نشان دادن اینکه شمارنده نسبت به `.spec.backoffLimit` نباید افزایش یابد و یک پاد جایگزین باید ایجاد شود، استفاده می‌شود.
  - `Count`: برای نشان دادن اینکه پاد باید به روش پیش‌فرض مدیریت شود، استفاده می‌شود. شمارنده نسبت به `.spec.backoffLimit` باید افزایش یابد.
  - `FailIndex`: از این اقدام به همراه [backoff limit per index](#backoff-limit-per-index) برای جلوگیری از تلاش‌های مجدد غیرضروری در داخل اندیس یک پاد ناموفق استفاده کنید.

{{< note >}}
وقتی از `podFailurePolicy` استفاده می‌کنید، کنترل‌کننده‌ی کار فقط پادها را در فاز `Failed` مطابقت می‌دهد. پادهایی با مهر زمانی حذف که در فاز ترمینال (`Failed` یا `Succeeded`) نیستند، هنوز در حال خاتمه در نظر گرفته می‌شوند. این بدان معناست که پادهای در حال خاتمه، یک [نهایی کننده ردیابی](#job-tracking-with-finalizers) را تا رسیدن به فاز ترمینال حفظ می‌کنند. از کوبرنتیز 1.27، Kubelet پادهای حذف شده را به فاز ترمینال منتقل می‌کند. (به [فاز پاد](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) مراجعه کنید). این امر تضمین می‌کند که پادهای حذف شده، نهایی‌کننده‌های خود را توسط کنترل‌کننده‌ی کار حذف کنند.
{{< /note >}}

{{< note >}}
با شروع از کوبرنتیز نسخه ۱.۲۸، وقتی از سیاست خرابی پاد استفاده می‌شود، کنترل‌کننده‌ی کار، پادهای خاتمه‌دهنده را تنها زمانی که این پادها به فاز `Failed` ترمینال برسند، دوباره ایجاد می‌کند. این رفتار مشابه `podReplacementPolicy: Failed` است. برای اطلاعات بیشتر، به [سیاست جایگزینی پاد](#pod-replacement-policy) مراجعه کنید.
{{< /note >}}

وقتی از `podFailurePolicy` استفاده می‌کنید و کار به دلیل تطبیق قاعده با عمل `FailJob` توسط پاد با شکست مواجه می‌شود، کنترل کننده کار با اضافه کردن شرط `FailureTarget` فرآیند خاتمه کار را آغاز می‌کند. برای جزئیات بیشتر، به [اختتام و پاکسازی کار](#job-termination-and-cleanup) مراجعه کنید.

## سیاست موفقیت {#success-policy}

هنگام ایجاد یک کار شاخص‌گذاری شده، می‌توانید با استفاده از `.spec.successPolicy`، بر اساس پادهایی که موفق شده‌اند، تعیین کنید که چه زمانی یک کار می‌تواند به عنوان موفق اعلام شود.

به طور پیش‌فرض، یک کار زمانی موفق می‌شود که تعداد پادهای موفق برابر با `.spec.completions` باشد. در اینجا موقعیت‌هایی وجود دارد که ممکن است بخواهید کنترل بیشتری برای اعلام موفقیت‌آمیز بودن یک کار داشته باشید:

* هنگام اجرای شبیه‌سازی‌ها با پارامترهای مختلف، ممکن است برای موفقیت‌آمیز بودن کل کار، نیازی به موفقیت همه شبیه‌سازی‌ها نداشته باشید.
* هنگام پیروی از الگوی رهبر-کارگر، تنها موفقیت رهبر، موفقیت یا شکست یک کار را تعیین می‌کند. نمونه‌هایی از این چارچوب‌ها عبارتند از MPI و PyTorch و غیره.

شما می‌توانید یک سیاست موفقیت را در فیلد `.spec.successPolicy` پیکربندی کنید تا موارد استفاده فوق را برآورده کند. این سیاست می‌تواند موفقیت کار را بر اساس پادهای موفق مدیریت کند. پس از اینکه کار با سیاست موفقیت مواجه شد، کنترل‌کننده کار پادهای باقی‌مانده را خاتمه می‌دهد. یک سیاست موفقیت توسط قوانین تعریف می‌شود. هر قانون می‌تواند یکی از اشکال زیر را داشته باشد:

* وقتی فقط `succeededIndexes` را مشخص می‌کنید، به محض اینکه تمام شاخص‌های مشخص شده در `succeededIndexes` موفق شوند، کنترل‌کننده‌ی کار، کار را به عنوان موفق علامت‌گذاری می‌کند. `succeededIndexes` باید فهرستی از فواصل بین 0 و `.spec.completions-1` باشد.
* وقتی فقط مقدار «succeededCount» را مشخص می‌کنید، به محض اینکه تعداد اندیس‌های موفق به «succeededCount» برسد، کنترل‌کننده‌ی کار، کار را به عنوان موفق علامت‌گذاری می‌کند.
* وقتی هر دو مقدار `succeededIndexes` و `succeededCount` را مشخص می‌کنید، به محض اینکه تعداد شاخص‌های موفق از زیرمجموعه شاخص‌های مشخص شده در `succeededIndexes` به `succeededCount` برسد، کنترل‌کننده‌ی کار، کار را به عنوان موفق علامت‌گذاری می‌کند.

توجه داشته باشید که وقتی چندین قانون را در `.spec.successPolicy.rules` مشخص می‌کنید، کنترل‌کننده‌ی کار، قوانین را به ترتیب ارزیابی می‌کند. هنگامی که کار با یک قانون مطابقت داشته باشد، کنترل‌کننده‌ی کار، قوانین باقی‌مانده را نادیده می‌گیرد.

در اینجا تنظیمات برای یک کار با `successPolicy` آمده است:

{{% code_sample file="/controllers/job-success-policy.yaml" %}}

در مثال بالا، هر دو مقدار `succeededIndexes` و `succeededCount` مشخص شده‌اند. بنابراین، کنترل‌کننده‌ی کار، کار را به عنوان موفق علامت‌گذاری می‌کند و پادهای باقی‌مانده را هنگامی که هر یک از شاخص‌های مشخص‌شده، ۰، ۲ یا ۳، موفق شوند، خاتمه می‌دهد.
کاری که با سیاست موفقیت مطابقت داشته باشد، شرط «SuccessCriteriaMet» را به همراه دلیل «SuccessPolicy» دریافت می‌کند. پس از صدور دستور حذف پادهای باقی‌مانده، کار شرط `Complete` را دریافت می‌کند.

توجه داشته باشید که «succeededIndexes» به صورت فواصلی که با خط فاصله از هم جدا شده‌اند، نمایش داده می‌شوند. اعداد در فهرست با اولین و آخرین عنصر سری که با خط فاصله از هم جدا شده‌اند، نمایش داده می‌شوند.

{{< note >}}
وقتی شما هم یک سیاست موفقیت و هم برخی سیاست‌های خاتمه مانند `.spec.backoffLimit` و `.spec.podFailurePolicy` را مشخص می‌کنید، به محض اینکه کار با هر یک از این سیاست‌ها مطابقت داشته باشد، کنترل‌کننده کار به سیاست خاتمه احترام می‌گذارد و سیاست موفقیت را نادیده می‌گیرد.
{{< /note >}}

## خاتمه کار و پاکسازی

وقتی یک کار تکمیل می‌شود، هیچ پاد دیگری ایجاد نمی‌شود، اما پادها نیز [معمولاً](#pod-backoff-failure-policy) حذف نمی‌شوند. نگه داشتن آنها به شما این امکان را می‌دهد که همچنان گزارش‌های پادهای تکمیل‌شده را مشاهده کنید تا خطاها، هشدارها یا سایر خروجی‌های تشخیصی را بررسی کنید. شیء کار نیز پس از تکمیل باقی می‌ماند تا بتوانید وضعیت آن را مشاهده کنید. حذف کارهای قدیمی پس از مشاهده وضعیت آنها به کاربر بستگی دارد. کار را با `kubectl` حذف کنید (مثلاً `kubectl delete jobs/pi` یا `kubectl delete -f ./job.yaml`). وقتی کار را با `kubectl` حذف می‌کنید، تمام پادهایی که ایجاد کرده نیز حذف می‌شوند.

به طور پیش‌فرض، یک کار بدون وقفه اجرا می‌شود مگر اینکه یک پاد با شکست مواجه شود (`restartPolicy=Never`) یا یک Container به اشتباه خارج شود (`restartPolicy=OnFailure`)، که در آن مرحله، کار به `.spec.backoffLimit` که در بالا توضیح داده شد، موکول می‌شود. پس از رسیدن به `.spec.backoffLimit`، کار به عنوان ناموفق علامت‌گذاری می‌شود و هر پاد در حال اجرا خاتمه می‌یابد.

راه دیگر برای خاتمه دادن به یک کار، تعیین مهلت فعال است. این کار را با تنظیم بخش `.spec.activeDeadlineSeconds` مربوط به کار بر حسب ثانیه انجام دهید. `activeDeadlineSeconds` صرف نظر از تعداد پادهای ایجاد شده، برای مدت زمان کار اعمال می‌شود. به محض اینکه یک کار به `activeDeadlineSeconds` برسد، تمام پادهای در حال اجرای آن خاتمه می‌یابند و وضعیت کار به `type: Failed` با `reason: DeadlineExceeded` تبدیل می‌شود.

توجه داشته باشید که `.spec.activeDeadlineSeconds` یک کار بر `.spec.backoffLimit` آن اولویت دارد. بنابراین، کاری که یک یا چند پاد ناموفق را دوباره امتحان می‌کند، پس از رسیدن به محدودیت زمانی مشخص شده توسط `activeDeadlineSeconds`، پادهای اضافی را مستقر نمی‌کند، حتی اگر `backoffLimit` هنوز حاصل نشده باشد.

مثال:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

توجه داشته باشید که هم مشخصات کار و هم [مشخصات قالب پاد](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) در کار دارای بخش `activeDeadlineSeconds` هستند. مطمئن شوید که این بخش را در سطح مناسب تنظیم کرده‌اید.

به خاطر داشته باشید که `restartPolicy` برای پاد اعمال می‌شود و نه برای خود کار: پس از قرار گرفتن وضعیت کار در حالت `type: Failed`، امکان راه‌اندازی مجدد خودکار کار وجود ندارد. یعنی، سازوکارهای خاتمه کار که با `.spec.activeDeadlineSeconds` و `.spec.backoffLimit` فعال می‌شوند، منجر به خرابی دائمی کار می‌شوند که برای رفع آن نیاز به مداخله دستی است.

### شرایط کار خط فرمان

یک کار دو حالت پایانی ممکن دارد که هر کدام یک شرط کاری متناظر دارند:
* موفقیت آمیز: وضعیت کار `Complete`
* ناموفق: وضعیت کار `Failed`

کارها به دلایل زیر شکست می‌خورند:
- تعداد خرابی‌های پاد از مقدار مشخص شده در `.spec.backoffLimit` در مشخصات کار فراتر رفته است. برای جزئیات بیشتر، به [سیاست شکست عقب نشینی پاد](#pod-backoff-failure-policy) مراجعه کنید.
- زمان اجرای کار از مقدار مشخص شده برای `.spec.activeDeadlineSeconds` فراتر رفت.
- یک کار شاخص شده که از `.spec.backoffLimitPerIndex` استفاده می‌کرد، شاخص های ناموفقی دارد. برای جزئیات بیشتر، به [محدودیت عقب نشینی به ازای هر شاخص](#backoff-limit-per-index) مراجعه کنید.
- تعداد شاخص های ناموفق در کار از مقدار مشخص شده در `spec.maxFailedIndexes` فراتر رفته است. برای جزئیات بیشتر، به [محدودیت عقب نشینی به ازای هر شاخص](#backoff-limit-per-index) مراجعه کنید.
- یک پاد ناموفق با قاعده‌ای در `.spec.podFailurePolicy` که دارای عملکرد `FailJob` است، مطابقت دارد. برای جزئیات بیشتر در مورد چگونگی تأثیر قوانین سیاست خرابی پاد بر ارزیابی خرابی، به [سیاست خرابی پاد](#pod-failure-policy) مراجعه کنید.

کارها به دلایل زیر موفق می‌شوند:
- تعداد پادهای موفق به مقدار مشخص شده در `.spec.completions` رسید.
- معیارهای مشخص شده در `.spec.successPolicy` رعایت شده‌اند. برای جزئیات، به [سیاست موفقیت](#success-policy) مراجعه کنید.

در کوبرنتیز نسخه ۱.۳۱ و بالاتر، کنترل‌کننده‌ی کار، اضافه کردن شرایط پایانی، `Failed` یا `Complete`، را تا زمان خاتمه یافتن تمام کار پادها به تأخیر می‌اندازد.

در کوبرنتیز نسخه ۱.۳۰ و قبل از آن، کنترل‌کننده‌ی کار به محض اینکه فرآیند خاتمه‌ی کار آغاز می‌شد و تمام نهایی‌کننده‌های پاد حذف می‌شدند، شرایط ترمینال کار `Complete` یا `Failed` را اضافه می‌کرد. با این حال، برخی از پادها در لحظه‌ای که شرایط ترمینال اضافه می‌شد، همچنان در حال اجرا یا خاتمه بودند.

در کوبرنتیز نسخه ۱.۳۱ و بالاتر، کنترلر فقط شرایط ترمینال کار را پس از خاتمه یافتن همه پادها اضافه می‌کند. می‌توانید این رفتار را با استفاده از `JobManagedBy` و `JobPodReplacementPolicy` (که هر دو به طور پیش‌فرض فعال هستند) کنترل کنید. [دروازه های ویژگی](/docs/reference/command-line-tools-reference/feature-gates/).

### خاتمه پادهای کار

کنترل‌کننده‌ی کار، شرط `FailureTarget` یا شرط `SuccessCriteriaMet` را به کار اضافه می‌کند تا پس از اینکه یک کار معیارهای موفقیت یا شکست را برآورده کرد، خاتمه‌ی پاد را فعال کند.

عواملی مانند `terminationGracePeriodSeconds` ممکن است مدت زمان را از لحظه‌ای که کنترل‌کننده‌ی کار شرط `FailureTarget` یا شرط `SuccessCriteriaMet` را اضافه می‌کند تا لحظه‌ای که تمام پادهای کار خاتمه می‌یابند و کنترل‌کننده‌ی کار یک [شرایط ترمینال](#terminal-job-conditions) (`Failed` یا `Complete`) اضافه می‌کند، افزایش دهند.

شما می‌توانید از شرط‌های `FailureTarget` یا `SuccessCriteriaMet` برای ارزیابی اینکه آیا کار با شکست مواجه شده یا موفقیت‌آمیز بوده است، بدون نیاز به انتظار برای اضافه کردن شرط پایانی توسط کنترل کننده، استفاده کنید.

برای مثال، ممکن است بخواهید تصمیم بگیرید چه زمانی یک کار جایگزین ایجاد کنید که جایگزین یک کار ناموفق شود. اگر کار ناموفق را زمانی که وضعیت `FailureTarget` ظاهر می‌شود، جایگزین کنید، کار جایگزین شما زودتر اجرا می‌شود، اما می‌تواند منجر به اجرای همزمان پادها از کار ناموفق و کار جایگزین شود و از منابع محاسباتی اضافی استفاده کند.

از طرف دیگر، اگر خوشه شما ظرفیت منابع محدودی دارد، می‌توانید صبر کنید تا وضعیت `Failed` روی کار ظاهر شود، که این کار جایگزینی شما را به تأخیر می‌اندازد اما با انتظار تا حذف تمام پادهای ناموفق، از صرفه‌جویی در منابع اطمینان حاصل می‌کنید.

## پاکسازی کارهای تمام شده به صورت خودکار

کارهای تکمیل‌شده معمولاً دیگر در سیستم مورد نیاز نیستند. نگه‌داشتن آنها در سیستم، به سرور API فشار وارد می‌کند. اگر کارها مستقیماً توسط یک کنترل‌کننده سطح بالاتر، مانند [CronJobs](/docs/concepts/workloads/controllers/cron-jobs/) مدیریت شوند، کارها می‌توانند توسط CronJobs بر اساس سیاست پاکسازی مبتنی بر ظرفیت مشخص‌شده، پاک‌سازی شوند.

### سازوکار TTL برای کارهای تمام شده

{{< feature-state for_k8s_version="v1.23" state="stable" >}}

راه دیگر برای پاک کردن خودکار کارهای تمام‌شده (چه `Complete` و چه `Failed`) استفاده از سازوکار TTL ارائه شده توسط یک [کنترل کننده TTL](/docs/concepts/workloads/controllers/ttlafterfinished/) برای منابع تمام‌شده، با مشخص کردن فیلد `.spec.ttlSecondsAfterFinished` برای کار است.

وقتی کنترل‌کننده TTL کار را پاک می‌کند، آن کار را به صورت آبشاری حذف می‌کند، یعنی اشیاء وابسته به آن، مانند پادها، را به همراه کار حذف می‌کند. توجه داشته باشید که وقتی کار حذف می‌شود، ضمانت‌های چرخه عمر آن، مانند نهایی‌کننده‌ها، رعایت می‌شوند.

برای مثال:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

کار `pi-with-ttl` واجد شرایط حذف خودکار، `100` ثانیه پس از اتمام آن خواهد بود.

اگر بخش روی `0` تنظیم شود، کار بلافاصله پس از اتمام، واجد شرایط حذف خودکار خواهد بود. اگر بخش تنظیم نشده باشد، این کار پس از اتمام توسط کنترل‌کننده TTL پاک نخواهد شد.

{{< note >}}
توصیه می‌شود بخش `ttlSecondsAfterFinished` را تنظیم کنید زیرا کارهای مدیریت نشده (کارهایی که مستقیماً ایجاد کرده‌اید، و نه غیرمستقیم از طریق سایر APIهای بار کاری مانند CronJob) دارای سیاست حذف پیش‌فرض `orphanDependents` هستند که باعث می‌شود پادهای ایجاد شده توسط یک کار مدیریت نشده پس از حذف کامل آن کار، باقی بمانند. اگرچه {{< glossary_tooltip text="control plane" term_id="control-plane" >}} در نهایت پادها از یک کار حذف شده پس از شکست یا تکمیل، جمع‌آوری می‌شوند، اما گاهی اوقات این پادهای باقی مانده ممکن است باعث تخریب عملکرد خوشه شوند یا در بدترین حالت باعث شوند خوشه به دلیل این تخریب آفلاین شود.

شما می‌توانید از [محدوده محدود](/docs/concepts/policy/limit-range/) و [سهمیه منابع](/docs/concepts/policy/resource-quotas/) برای تعیین محدودیت برای میزان منابعی که یک فضای نام خاص می‌تواند مصرف کند، استفاده کنید.
{{< /note >}}

## الگوهای کاری

شیء کار می‌تواند برای پردازش مجموعه‌ای از *آیتم‌های کاری* مستقل اما مرتبط استفاده شود. این موارد می‌توانند ایمیل‌هایی باشند که باید ارسال شوند، فریم‌هایی که باید رندر شوند، پرونده هایی که باید تبدیل کد شوند، محدوده‌هایی از کلیدها در یک پایگاه داده NoSQL که باید اسکن شوند و غیره.

در یک سیستم پیچیده، ممکن است چندین مجموعه مختلف از اقلام کاری وجود داشته باشد. در اینجا ما فقط یک مجموعه از اقلام کاری را در نظر می‌گیریم که کاربر می‌خواهد آنها را با هم مدیریت کند - یک *کار دسته‌ای*.

الگوهای مختلفی برای محاسبات موازی وجود دارد که هر کدام نقاط قوت و ضعفی دارند. موارد مورد اختلاف عبارتند از:

- یک شیء کار برای هر آیتم کاری، در مقابل یک شیء کار واحد برای همه آیتم‌های کاری. یک کار به ازای هر آیتم کاری، سربار زیادی را برای کاربر و سیستم ایجاد می‌کند تا تعداد زیادی از اشیاء کار را مدیریت کند. یک کار واحد برای همه آیتم‌های کاری برای تعداد زیادی از آیتم‌ها بهتر است.
- تعداد پادهای ایجاد شده برابر با تعداد اقلام کاری است، در حالی که هر پاد می‌تواند چندین قلم کاری را پردازش کند. وقتی تعداد پادها برابر با تعداد اقلام کاری باشد، پادها معمولاً نیاز به اصلاح کمتری در کد و کانتینرهای موجود دارند. اینکه هر پاد چندین قلم کاری را پردازش کند، برای تعداد زیادی از اقلام بهتر است.
- چندین رویکرد از صف کار استفاده می‌کنند. این امر مستلزم اجرای یک سرویس صف و ایجاد تغییراتی در برنامه یا کانتینر موجود برای استفاده از صف کار است. رویکردهای دیگر، سازگاری آسان‌تری با یک برنامه کانتینری موجود دارند.
- وقتی که یک کار با یک [سرویس بدون سر](/docs/concepts/services-networking/service/#headless-services) مرتبط باشد، می‌توانید پادهای درون یک کار را قادر سازید تا برای همکاری در یک محاسبه با یکدیگر ارتباط برقرار کنند.

معاوضه ها در اینجا خلاصه می شوند و ستون های 2 تا 4 مربوط به معاوضه های بالا هستند. نام الگوها همچنین لینک‌هایی به مثال‌ها و توضیحات مفصل‌تر هستند.

|                  Pattern                        | Single Job object | Fewer pods than work items? | Use app unmodified? |
| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|
| [Queue with Pod Per Work Item]                  |         ✓         |                             |      sometimes      |
| [Queue with Variable Pod Count]                 |         ✓         |             ✓               |                     |
| [Indexed Job with Static Work Assignment]       |         ✓         |                             |          ✓          |
| [Job with Pod-to-Pod Communication]             |         ✓         |         sometimes           |      sometimes      |
| [Job Template Expansion]                        |                   |                             |          ✓          |

وقتی تکمیل‌ها را با `.spec.completions` مشخص می‌کنید، هر پاد ایجاد شده توسط کنترل‌کننده‌ی کار یک [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) یکسان دارد. این بدان معناست که همه پادهای مربوط به یک وظیفه، خط فرمان و image یکسان، حجم های یکسان و (تقریباً) متغیرهای محیطی یکسانی خواهند داشت. این الگوها روش‌های مختلفی برای ترتیب دادن پادها برای کار روی چیزهای مختلف هستند.

این جدول تنظیمات مورد نیاز برای `.spec.parallelism` و `.spec.completions` را برای هر یک از الگوها نشان می‌دهد. در اینجا، `W` تعداد اقلام کاری است.

|             Pattern                             | `.spec.completions` |  `.spec.parallelism` |
| ----------------------------------------------- |:-------------------:|:--------------------:|
| [Queue with Pod Per Work Item]                  |          W          |        any           |
| [Queue with Variable Pod Count]                 |         null        |        any           |
| [Indexed Job with Static Work Assignment]       |          W          |        any           |
| [Job with Pod-to-Pod Communication]             |          W          |         W            |
| [Job Template Expansion]                        |          1          |     should be 1      |

[صف با پاد به ازای هر آیتم کاری]: /docs/tasks/job/coarse-parallel-processing-work-queue/
[صف با تعداد پاد متغیر]: /docs/tasks/job/fine-parallel-processing-work-queue/
[کار اندیس شده با انتساب کار استاتیک]: /docs/tasks/job/indexed-parallel-processing-static/
[کار در حوزه ارتباطات پاد به پاد]: /docs/tasks/job/job-with-pod-to-pod-communication/
[گسترش قالب کاری]: /docs/tasks/job/parallel-processing-expansion/

## استفاده پیشرفته

### تعلیق یک کار

{{< feature-state for_k8s_version="v1.24" state="stable" >}}

وقتی یک کار ایجاد می‌شود، کنترل‌کننده کار بلافاصله شروع به ایجاد پادها می‌کند تا الزامات کار را برآورده کند و این کار را تا زمان تکمیل کار ادامه می‌دهد. با این حال، ممکن است بخواهید اجرای یک کار را به طور موقت به حالت تعلیق درآورید و بعداً آن را از سر بگیرید، یا کارها را در حالت تعلیق شروع کنید و یک کنترل‌کننده سفارشی داشته باشید که بعداً تصمیم بگیرد چه زمانی آنها را شروع کند.

برای تعلیق یک کار، می‌توانید بخش `.spec.suspend` مربوط به کار را به true به‌روزرسانی کنید؛ بعداً، وقتی می‌خواهید دوباره آن را از سر بگیرید، آن را به false به‌روزرسانی کنید. ایجاد یک کار با `.spec.suspend` که روی true تنظیم شده باشد، آن را در حالت تعلیق ایجاد می‌کند.

وقتی یک کار از حالت تعلیق از سر گرفته می‌شود، بخش `.status.startTime` آن به زمان فعلی بازنشانی می‌شود. این بدان معناست که زمان سنج `.spec.activeDeadlineSeconds` هنگام تعلیق و از سرگیری یک کار متوقف و بازنشانی می‌شود.

وقتی یک کار را به حالت تعلیق درمی‌آورید، هر پاد در حال اجرایی که وضعیت `Completed` نداشته باشد، با یک سیگنال SIGTERM [خاتمه‌یافته](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) خواهد شد. دوره خاتمه‌ی مناسب پاد رعایت می‌شود و پاد شما باید این سیگنال را در این دوره مدیریت کند. این ممکن است شامل ذخیره پیشرفت برای بعداً یا لغو تغییرات باشد. پادهایی که به این روش خاتمه می‌یابند، در تعداد `completions` کار حساب نمی‌شوند.

یک مثال از تعریف کار در حالت تعلیق می‌تواند به صورت زیر باشد:

```shell
kubectl get job myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  suspend: true
  parallelism: 1
  completions: 5
  template:
    spec:
      ...
```

همچنین می‌توانید با وصله کردن کار با استفاده از خط فرمان، تعلیق کار را تغییر دهید.

تعلیق یک کار فعال:

```shell
kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":true}}'
```

از سرگیری یک کار معلق:

```shell
kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":false}}'
```

وضعیت کار می‌تواند برای تعیین اینکه آیا یک کار به حالت تعلیق درآمده یا در گذشته به حالت تعلیق درآمده است، مورد استفاده قرار گیرد:

```shell
kubectl get jobs/myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
# .metadata و .spec حذف شده‌اند‎
status:
  conditions:
  - lastProbeTime: "2021-02-05T13:14:33Z"
    lastTransitionTime: "2021-02-05T13:14:33Z"
    status: "True"
    type: Suspended
  startTime: "2021-02-05T13:13:48Z"
```

شرط کار از نوع "Suspended" با وضعیت "True" به این معنی است که کار به حالت تعلیق درآمده است؛ بخش `lastTransitionTime` می‌تواند برای تعیین مدت زمان تعلیق کار استفاده شود. اگر وضعیت آن شرط "False" باشد، کار قبلاً به حالت تعلیق درآمده و اکنون در حال اجرا است. اگر چنین شرطی در وضعیت کار وجود نداشته باشد، کار هرگز متوقف نشده است.

رویدادها همچنین زمانی ایجاد می‌شوند که کار به حالت تعلیق درآمده و از سر گرفته شود:

```shell
kubectl describe jobs/myjob
```

```
Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
```

چهار رویداد آخر، به ویژه رویدادهای "Suspended" و "Resumed"، مستقیماً نتیجه‌ی تغییر وضعیت فیلد `.spec.suspend` هستند. در فاصله‌ی زمانی بین این دو رویداد، می‌بینیم که هیچ پاد ایجاد نشده است، اما به محض از سرگیری کار، ایجاد پاد مجدداً آغاز شده است.

### دستورالعمل‌های زمان‌بندی تغییرپذیر

{{< feature-state for_k8s_version="v1.27" state="stable" >}}

در بیشتر موارد، یک کار موازی می‌خواهد که پادها با محدودیت‌هایی اجرا شوند، مثلاً همه در یک منطقه باشند، یا همه روی مدل پردازنده گرافیکی x یا y باشند، اما نه ترکیبی از هر دو.

بخش [تعلیق](#suspending-a-job) اولین قدم برای دستیابی به این معانی است. تعلیق به یک کنترل کننده صف سفارشی اجازه می‌دهد تا تصمیم بگیرد که یک کار چه زمانی باید شروع شود؛ با این حال، هنگامی که یک کار از حالت تعلیق خارج می‌شود، یک کنترل کننده صف سفارشی هیچ تاثیری بر محل قرارگیری پادهای یک کار ندارد.

این ویژگی امکان به‌روزرسانی دستورالعمل‌های زمان‌بندی یک کار را قبل از شروع آن فراهم می‌کند، که به کنترل‌کننده‌های صف سفارشی این امکان را می‌دهد که در عین حال که تخصیص واقعی بین پادها و گره‌ها را به kube-scheduler واگذار می‌کنند، بر جایگذاری پادها تأثیر بگذارند. این ویژگی فقط برای کارهای معلقی که قبلاً هرگز از حالت تعلیق خارج نشده‌اند، مجاز است.

بخش هایی در قالب پاد یک کار که می‌توانند به‌روزرسانی شوند عبارتند از: گره‌های پیوند، انتخابگر گره، تحمل‌ها، برچسب‌ها، حاشیه‌نویسی‌ها و [دروازه‌های زمان‌بندی](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).

### تعیین انتخابگر پاد خودتان

معمولاً وقتی یک شیء کار ایجاد می‌کنید، `.spec.selector` را مشخص نمی‌کنید. منطق پیش‌فرض سیستم این بخش را هنگام ایجاد کار اضافه می‌کند. این مقدار، یک انتخابگر را انتخاب می‌کند که با هیچ شغل دیگری همپوشانی نداشته باشد.

با این حال، در برخی موارد، ممکن است لازم باشد این انتخابگر تنظیم‌شده خودکار را لغو کنید. برای انجام این کار، می‌توانید `.spec.selector` مربوط به کار را مشخص کنید.

هنگام انجام این کار بسیار مراقب باشید. اگر یک انتخابگر برچسب مشخص کنید که برای پادهای آن کار منحصر به فرد نباشد و با پادهای غیرمرتبط مطابقت داشته باشد، ممکن است پادهای کار غیرمرتبط حذف شوند، یا این کار ممکن است پادهای دیگر را به عنوان تکمیل کننده خود در نظر بگیرد، یا ممکن است یک یا هر دو کار از ایجاد پادها یا اجرای کامل خودداری کنند. اگر یک انتخابگر غیرمنحصر به فرد انتخاب شود، ممکن است سایر کنترل‌کننده‌ها (مثلاً ReplicationController) و پادهای آنها نیز به روش‌های غیرقابل پیش‌بینی رفتار کنند. کوبرنتیز مانع از اشتباه شما هنگام تعیین `.spec.selector` نمی‌شود.

در اینجا مثالی از موردی که ممکن است بخواهید از این ویژگی استفاده کنید، آورده شده است.

فرض کنید کار `old` در حال اجرا است. شما می‌خواهید پادهای موجود به اجرا ادامه دهند، اما می‌خواهید بقیه پادهایی که ایجاد می‌کند از یک الگوی پاد متفاوت استفاده کنند و کار نام جدیدی داشته باشد. شما نمی‌توانید کار را به‌روزرسانی کنید زیرا این بخش ها قابل به‌روزرسانی نیستند. بنابراین، کار `old` را حذف می‌کنید اما پادهای آن را با استفاده از `kubectl delete jobs/old --cascade=orphan` به حال خود رها می‌کنید. قبل از حذف آن، به انتخابگری که استفاده می‌کند توجه کنید:

```shell
kubectl get job old -o yaml
```

خروجی مشابه این است:

```yaml
kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

سپس یک کار جدید با نام `new` ایجاد می‌کنید و به صراحت همان انتخابگر را مشخص می‌کنید. از آنجایی که پادهای موجود دارای برچسب `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002` هستند، آنها نیز توسط کار `new` کنترل می‌شوند.

شما باید در کار جدید مقدار `manualSelector: true` را مشخص کنید، زیرا از انتخابگری که سیستم معمولاً به طور خودکار برای شما تولید می‌کند، استفاده نمی‌کنید.

```yaml
kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

خودِ کار جدید، شناسه کاربری متفاوتی از `a8f3d00d-c6d2-11e5-9f87-42010af00002` خواهد داشت. تنظیم `manualSelector: true` به سیستم می‌گوید که شما می‌دانید چه کاری انجام می‌دهید و اجازه این عدم تطابق را می‌دهد.

### ردیابی کار با نهایی‌کننده‌ها

{{< feature-state for_k8s_version="v1.26" state="stable" >}}

صفحه کنترل، پادهای متعلق به هر کار را ردیابی می‌کند و در صورت حذف چنین پادی از سرور API، به آن اطلاع می‌دهد. برای انجام این کار، کنترل‌کننده کار، پادهایی را با نهایی کننده `batch.kubernetes.io/job-tracking` ایجاد می‌کند. کنترل‌کننده، نهایی کننده را تنها پس از اینکه پاد در وضعیت کار به حساب آمد، حذف می‌کند و به سایر کنترل‌کننده‌ها یا کاربران اجازه می‌دهد تا پاد را حذف کنند.

{{< note >}}
اگر مشاهده کردید که پادهای یک کار در ردیابی نهایی کننده گیر کرده‌اند، به [پاد من در حال پایان است](/docs/tasks/debug/debug-application/debug-pods/) مراجعه کنید.
{{< /note >}}

### کارهای اندیس شده Elastic

{{< feature-state feature_gate_name="ElasticIndexedJob" >}}

شما می‌توانید با تغییر دادن هر دو `.spec.parallelism` و `.spec.completions` به یکدیگر به طوری که `.spec.parallelism == .spec.completions` باشد، مقیاس کارهای شاخص‌گذاری شده را افزایش یا کاهش دهید. هنگام کاهش مقیاس، کوبرنتیز پادهایی را که شاخص‌های بالاتری دارند حذف می‌کند.

موارد استفاده برای کارهای شاخص‌گذاری شده Elastic شامل بارهای کاری دسته‌ای است که نیاز به مقیاس‌بندی یک کار شاخص‌گذاری شده دارند، مانند کارهای آموزشی MPI، Horovod، Ray و PyTorch.

### ایجاد با تأخیر پادهای جایگزین {#pod-replacement-policy}

{{< feature-state for_k8s_version="v1.29" state="beta" >}}

{{< note >}}
شما فقط در صورتی می‌توانید `podReplacementPolicy` را روی کارها تنظیم کنید که `JobPodReplacementPolicy` [دروازه ویژگی](/docs/reference/command-line-tools-reference/feature-gates/) را فعال کرده باشید (که به طور پیش‌فرض فعال است).
{{< /note >}}

به طور پیش‌فرض، کنترل‌کننده‌ی کار به محض اینکه پادها از کار بیفتند یا در حال خاتمه باشند (دارای مهر زمانی حذف باشند)، آنها را دوباره ایجاد می‌کند. این بدان معناست که در یک زمان معین، وقتی برخی از پادها در حال خاتمه هستند، تعداد پادهای در حال اجرا برای یک کار می‌تواند بیشتر از `Parallelism` یا بیشتر از یک پاد به ازای هر شاخص باشد (اگر از یک کار اندیس شده استفاده می‌کنید).

شما می‌توانید فقط زمانی که پادِ خاتمه‌دهنده کاملاً خاتمه داده شده باشد (دارای `status.phase: Failed` باشد)، پادهای جایگزین ایجاد کنید.
برای انجام این کار، مقدار `.spec.podReplacementPolicy` را روی Failed تنظیم کنید.
سیاست جایگزینی پیش‌فرض بستگی به این دارد که آیا کار مورد نظر دارای `podFailurePolicy` است یا خیر.
بدون تعریف سیاست خرابی پاد برای یک کار، حذف بخش `podReplacementPolicy` سیاست جایگزینی `TermminatingOrFailed` را انتخاب می‌کند:
صفحه کنترل بلافاصله پس از حذف پاد، پادهای جایگزین ایجاد می‌کند (به محض اینکه صفحه کنترل ببیند که پاد برای این کار دارای `deletionTimestamp` تنظیم شده است). برای کارهایی که سیاست شکست پاد روی آنها تنظیم شده است، `podReplacementPolicy` پیش‌فرض `Failed` است و هیچ مقدار دیگری مجاز نیست.
برای کسب اطلاعات بیشتر در مورد سیاست‌های خرابی پاد برای مشاغل، به [سیاست خرابی پاد](#pod-failure-policy) مراجعه کنید.

```yaml
kind: Job
metadata:
  name: new
  ...
spec:
  podReplacementPolicy: Failed
  ...
```

در صورتی که خوشه شما قابلیت feature gate را فعال کرده باشد، می‌توانید بخش `.status.terminating` مربوط به یک کار را بررسی کنید. مقدار این بخش، تعداد پاد های متعلق به کار است که در حال حاضر در حال خاتمه یافتن هستند.

```shell
kubectl get jobs/myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
# .metadata و .spec حذف شده‌اند‎
status:
  terminating: 3 # سه پاد در حال اتمام هستند و هنوز به مرحله شکست نرسیده‌اند
```

### واگذاری مدیریت یک شیء کار به کنترل‌کننده خارجی

{{< feature-state feature_gate_name="JobManagedBy" >}}

{{< note >}}
شما فقط در صورتی می‌توانید بخش `managedBy` را روی کارها تنظیم کنید که `JobManagedBy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) (که به طور پیش‌فرض فعال است) را فعال کرده باشید.
{{< /note >}}

این ویژگی به شما امکان می‌دهد کنترلر داخلی کار را برای یک کار خاص غیرفعال کنید و تطبیق کار را به یک کنترل کننده خارجی واگذار کنید.

شما با تنظیم یک مقدار سفارشی برای بخش `spec.managedBy` - هر مقداری غیر از `kubernetes.io/job-controller` - کنترل‌کننده‌ای را که کار را تطبیق می‌دهد، مشخص می‌کنید. مقدار این بخش تغییرناپذیر است.

{{< note >}}
هنگام استفاده از این ویژگی، مطمئن شوید که کنترل‌کننده‌ی نشان داده شده توسط بخش نصب شده باشد، در غیر این صورت ممکن است کار به هیچ وجه تطبیق داده نشود.
{{< /note >}}

{{< note >}}
هنگام توسعه یک کنترل‌کننده‌ی خارجیِ کار توجه داشته باشید که کنترل‌کننده‌ی شما باید به شیوه‌ای مطابق با تعاریف مشخصات API و بخش های وضعیت شیء کار عمل کند.

لطفاً این موارد را با جزئیات در [کار API](/docs/reference/kubernetes-api/workload-resources/job-v1/) بررسی کنید. همچنین توصیه می‌کنیم برای تأیید پیاده‌سازی خود، تست‌های انطباق e2e را برای شیء کار اجرا کنید.

در نهایت، هنگام توسعه یک کنترل کننده کار خارجی، مطمئن شوید که از نهایی کننده `batch.kubernetes.io/job-tracking` که برای کنترل کننده داخلی رزرو شده است، استفاده نمی‌کند.
{{< /note >}}

{{< warning >}}
اگر در نظر دارید که دروازه ویژگی `JobManagedBy` را غیرفعال کنید، یا خوشه را به نسخه‌ای بدون دروازه ویژگی فعال، تنزل دهید، بررسی کنید که آیا `jobs` با مقدار سفارشی بخش `spec.managedBy` وجود دارد یا خیر. اگر `چنین کارهایی وجود داشته باشد، این خطر وجود دارد که پس از عملیات، توسط دو کنترل‌کننده با هم تطبیق داده شوند: کنترل‌کننده کار داخلی و کنترل‌کننده خارجی که با مقدار بخش مشخص شده است.
{{< /warning >}}

## جایگزین‌ها

### پادهای مستقل

وقتی گره‌ای که یک پاد روی آن اجرا می‌شود، راه‌اندازی مجدد شود یا از کار بیفتد، پاد خاتمه می‌یابد و دیگر راه‌اندازی مجدد نخواهد شد. با این حال، یک کار، پادهای جدیدی را برای جایگزینی پادهای خاتمه یافته ایجاد می‌کند. به همین دلیل، توصیه می‌کنیم حتی اگر برنامه شما فقط به یک پاد نیاز دارد، از یک کار به جای یک پاد ساده استفاده کنید.

### کنترل کننده تکرار

کارها مکمل [کنترل‌کننده‌های تکرار](/docs/concepts/workloads/controllers/replicationcontroller/) هستند. یک کنترل‌کننده تکرار، پادهایی را مدیریت می‌کند که انتظار نمی‌رود خاتمه یابند (مثلاً سرورهای وب) و یک کار، پادهایی را مدیریت می‌کند که انتظار می‌رود خاتمه یابند (مثلاً وظایف دسته‌ای).

همانطور که در [چرخه عمر پاد](/docs/concepts/workloads/pods/pod-lifecycle/) بحث شده است، `Job` *فقط* برای پادهایی مناسب است که `RestartPolicy` آنها برابر با `OnFailure` یا `Never` باشد. (توجه: اگر `RestartPolicy` تنظیم نشده باشد، مقدار پیش‌فرض `Always` است.)

### یک کار، پاد کنترل‌کننده را راه‌اندازی می‌کند

الگوی دیگر این است که یک کار واحد یک پاد ایجاد کند که سپس پادهای دیگری ایجاد می‌کند و به عنوان نوعی کنترل‌کننده سفارشی برای آن پادها عمل می‌کند. این روش بیشترین انعطاف‌پذیری را فراهم می‌کند، اما ممکن است شروع کار با آن تا حدودی پیچیده باشد و ادغام کمتری با کوبرنتیز ارائه می‌دهد.

یک نمونه از این الگو می‌تواند یک کار باشد که یک پاد را شروع می‌کند که اسکریپتی را اجرا می‌کند که به نوبه خود یک کنترل کننده اصلی Spark را شروع می‌کند (به [مثال spark](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md) مراجعه کنید)، یک درایور Spark را اجرا می‌کند و سپس آن را پاک‌سازی می‌کند.

یکی از مزایای این رویکرد این است که فرآیند کلی، تضمین تکمیل یک شیء کار را دریافت می‌کند، اما کنترل کاملی بر اینکه چه پادهایی ایجاد می‌شوند و چگونه کار به آنها اختصاص داده می‌شود، حفظ می‌کند.

## {{% heading "whatsnext" %}}

* درباره [پادها](/docs/concepts/workloads/pods) اطلاعات کسب کنید.
* درباره روش‌های مختلف اجرای کارها بخوانید:
  * [پردازش موازی کلی با استفاده از صف کاری](/docs/tasks/job/coarse-parallel-processing-work-queue/)
  * [پردازش موازی دقیق با استفاده از صف کاری](/docs/tasks/job/fine-parallel-processing-work-queue/)
  * استفاده از یک [کار اندیس شده برای پردازش موازی با انتساب کار استاتیک](/docs/tasks/job/indexed-parallel-processing-static/)
  * ایجاد چندین کار بر اساس یک قالب: [پردازش موازی با استفاده از بسط‌ها](/docs/tasks/job/parallel-processing-expansion/)
* برای کسب اطلاعات بیشتر در مورد نحوه پاکسازی وظایف تکمیل‌شده و/یا شکست‌خورده در خوشه خود، پیوندهای موجود در [پاکسازی خودکار وظایف تکمیل‌شده](#clean-up-finished-jobs-automatically) را دنبال کنید.
* `Job` بخشی از API REST کوبرنتیز است.
  برای درک API مربوط به کارها، تعریف شیء {{< api-reference page="workload-resources/job-v1" >}} را مطالعه کنید.
* درباره [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/) بخوانید، که می‌توانید از آن برای تعریف مجموعه‌ای از کارها که بر اساس یک برنامه اجرا می‌شوند، مشابه ابزار یونیکس `cron`، استفاده کنید.
* نحوه پیکربندی مدیریت خرابی‌های پاد قابل بازیابی و غیرقابل بازیابی را با استفاده از `podFailurePolicy`، بر اساس [مثال‌های] گام به گام (/docs/tasks/job/pod-failure-policy/) تمرین کنید.
