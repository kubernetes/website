---
title: مدیریت منابع برای پادها و کانتینرها
content_type: concept
weight: 40
feature:
  title: Automatic bin packing
  description: >
    Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability.
    Mix critical and best-effort workloads in order to drive up utilization and save even more resources.
---

<!-- overview -->

وقتی یک {{< glossary_tooltip term_id="pod" >}} مشخص می‌کنید، می‌توانید به صورت اختیاری مشخص کنید که یک {{< glossary_tooltip text="container" term_id="container" >}} به چه مقدار از هر منبع نیاز دارد. رایج‌ترین منابعی که باید مشخص شوند CPU و حافظه (RAM) هستند؛ منابع دیگری نیز وجود دارند.

وقتی درخواست منبع را برای کانتینرها در یک پاد مشخص می‌کنید، {{< glossary_tooltip text="kube-scheduler" term_id="kube-scheduler" >}} از این اطلاعات برای تصمیم‌گیری در مورد اینکه پاد را روی کدام گره قرار دهید، استفاده می‌کند. وقتی محدودیت منبع را برای یک کانتینر مشخص می‌کنید، {{< glossary_tooltip text="kubelet" term_id="kubelet" >}} آن محدودیت‌ها را اعمال می‌کند تا کانتینر در حال اجرا اجازه استفاده بیشتر از آن منبع را نسبت به محدودیتی که تعیین کرده‌اید، نداشته باشد. kubelet همچنین حداقل مقدار درخواست از آن منبع سیستم را به طور خاص برای استفاده آن کانتینر رزرو می‌کند.

<!-- body -->

## درخواست‌ها و محدودیت‌ها

اگر گره‌ای که یک پاد در آن اجرا می‌شود، منبع کافی در دسترس داشته باشد، یک کانتینر می‌تواند (و مجاز است) از منبعی بیشتر از آنچه `request` آن برای آن منبع تعیین کرده است، استفاده کند.

برای مثال، اگر درخواست `memory` برای یک کانتینر ۲۵۶ مگابایت تنظیم کنید، و آن کانتینر در یک پاد برنامه‌ریزی شده برای یک گره با ۸ گیگابایت حافظه و بدون پاد دیگر باشد، آنگاه کانتینر می‌تواند سعی کند از رم بیشتری استفاده کند.

محدودیت‌ها داستان متفاوتی دارند. محدودیت‌های `cpu` و `memory` هر دو توسط kubelet (و {{< glossary_tooltip text="container runtime" term_id="container-runtime" >}}) اعمال می‌شوند و در نهایت توسط هسته اجرا می‌شوند. در گره‌های لینوکس، هسته لینوکس محدودیت‌ها را با {{< glossary_tooltip text="cgroups" term_id="cgroup" >}} اعمال می‌کند. رفتار اعمال محدودیت `cpu` و `memory` کمی متفاوت است.

محدودیت‌های `cpu` توسط کنترل CPU اعمال می‌شوند. وقتی یک کانتینر به محدودیت `cpu` خود نزدیک می‌شود، هسته دسترسی به CPU را متناسب با محدودیت کانتینر محدود می‌کند. بنابراین، محدودیت `cpu` یک محدودیت قطعی است که هسته اعمال می‌کند. کانتینرها نمی‌توانند از CPU بیشتری نسبت به آنچه در محدودیت `cpu` آنها مشخص شده است، استفاده کنند.

محدودیت‌های `memory` توسط هسته با حذف‌های «خارج از حافظه» (OOM) اعمال می‌شوند. وقتی یک کانتینر بیش از حد `memory` خود استفاده می‌کند، هسته ممکن است آن را خاتمه دهد. با این حال، خاتمه‌ها فقط زمانی اتفاق می‌افتند که هسته فشار حافظه را تشخیص دهد. بنابراین، کانتینری که بیش از حد حافظه اختصاص می‌دهد، ممکن است بلافاصله از بین نرود. این بدان معناست که محدودیت‌های `memory` به صورت واکنشی اعمال می‌شوند. یک کانتینر ممکن است از حافظه بیشتری نسبت به حد `memory` خود استفاده کند، اما اگر این اتفاق بیفتد، ممکن است از بین برود.

{{< note >}}
یک ویژگی آلفا به نام «MemoryQoS» وجود دارد که تلاش می‌کند اعمال محدودیت پیشگیرانه بیشتری برای حافظه اضافه کند (برخلاف اعمال واکنشی توسط OOM killer). با این حال، این تلاش [ متوقف شد] (https://github.com/kubernetes/enhancements/tree/a47155b340/keps/sig-node/2570-memory-qos#latest-update-stalled)
به دلیل یک وضعیت بالقوه زنده نگه داشتن حافظه می تواند باعث شود.
{{< /note >}}

{{< note >}}
اگر برای یک منبع محدودیتی تعیین کنید، اما هیچ درخواستی را مشخص نکنید و هیچ سازوکار زمان پذیرشی درخواست پیش‌فرضی را برای آن منبع اعمال نکرده باشد، کوبرنتیز محدودیتی را که شما تعیین کرده‌اید رونوشت می‌کند و از آن به عنوان مقدار درخواستی برای منبع استفاده می‌کند.
{{< /note >}}

## انواع منابع

*پردازنده* و *حافظه* هر کدام یک *نوع منبع* هستند. یک نوع منبع یک واحد پایه دارد. پردازنده نشان دهنده پردازش محاسباتی است و با واحدهای [پردازنده‌های کوبرنتیز](#meaning-of-cpu) مشخص می‌شود. حافظه با واحد بایت مشخص می‌شود. برای بارهای کاری لینوکس، می‌توانید منابع _صفحه_بزرگ را مشخص کنید. صفحات بزرگ یک ویژگی خاص لینوکس هستند که در آن هسته گره بلوک‌هایی از حافظه را اختصاص می‌دهد که بسیار بزرگتر از اندازه صفحه پیش‌فرض هستند.

برای مثال، در سیستمی که اندازه پیش‌فرض صفحه ۴ کیلوبایت است، می‌توانید محدودیتی مانند «hugepages-2Mi: 80Mi» تعیین کنید. اگر کانتینر سعی کند بیش از ۴۰ صفحه بزرگ ۲ میبایتی (در مجموع ۸۰ میبایت) را اختصاص دهد، این تخصیص با شکست مواجه می‌شود.

{{< note >}}
شما نمی‌توانید منابع `hugepages-*` را بیش از حد مجاز کنید. این با منابع `memory` و `cpu` متفاوت است.
{{< /note >}}

CPU و حافظه در مجموع به عنوان *منابع محاسباتی* یا *منابع* شناخته می‌شوند. منابع محاسباتی مقادیر قابل اندازه‌گیری هستند که می‌توانند درخواست، تخصیص و مصرف شوند. آنها با [منابع API](/docs/concepts/overview/kubernetes-api/) متفاوت هستند. منابع API، مانند پادها و [سرویس ها](/docs/concepts/services-networking/service/) اشیاء هستند که می‌توانند از طریق سرور API کوبرنتیز خوانده و اصلاح شوند.

## درخواست‌های منابع و محدودیت‌های پاد و کانتینر

برای هر کانتینر، می‌توانید محدودیت‌ها و درخواست‌های منابع را مشخص کنید، از جمله موارد زیر:

* `spec.containers[].resources.limits.cpu`
* `spec.containers[].resources.limits.memory`
* `spec.containers[].resources.limits.hugepages-<size>`
* `spec.containers[].resources.requests.cpu`
* `spec.containers[].resources.requests.memory`
* `spec.containers[].resources.requests.hugepages-<size>`

اگرچه شما فقط می‌توانید درخواست‌ها و محدودیت‌های مربوط به کانتینرهای منفرد را مشخص کنید، اما فکر کردن به درخواست‌ها و محدودیت‌های کلی منابع برای یک پاد نیز مفید است.
برای یک منبع خاص، *درخواست/محدودیت منبع پاد* مجموع درخواست‌ها/محدودیت‌های منبع از آن نوع برای هر کانتینر در پاد است.

## مشخصات منبع در سطح پاد

{{< feature-state feature_gate_name="PodLevelResources" >}}

با شروع از کوبرنتیز 1.32، می‌توانید درخواست‌ها و محدودیت‌های منابع را در سطح پاد نیز مشخص کنید. در سطح پاد، کوبرنتیز {{< skew currentVersion >}}
فقط از درخواست‌ها یا محدودیت‌های منابع برای انواع خاص منابع پشتیبانی می‌کند: `cpu` و/یا `memory`. این ویژگی در حال حاضر در نسخه آلفا است و با فعال شدن این ویژگی، کوبرنتیز به شما امکان می‌دهد بودجه کلی منابع را برای پاد اعلام کنید، که به ویژه هنگام برخورد با تعداد زیادی کانتینر که در آن سنجش دقیق نیازهای منابع فردی دشوار است، مفید است. علاوه بر این، به کانتینرهای درون یک پاد این امکان را می‌دهد که منابع بیکار را با یکدیگر به اشتراک بگذارند و استفاده از منابع را بهبود بخشند.

برای یک پاد، می‌توانید محدودیت‌های منابع و درخواست‌ها برای CPU و حافظه را با وارد کردن موارد زیر مشخص کنید:
* `spec.resources.limits.cpu`
* `spec.resources.limits.memory`
* `spec.resources.requests.cpu`
* `spec.resources.requests.memory`

## واحدهای منابع در کوبرنتیز

### واحدهای منابع CPU {#meaning-of-cpu}

محدودیت‌ها و درخواست‌ها برای منابع CPU با واحدهای *cpu* اندازه‌گیری می‌شوند. در کوبرنتیز، 1 واحد CPU معادل **1 هسته CPU فیزیکی** یا **1 هسته مجازی** است، بسته به اینکه گره یک میزبان فیزیکی باشد یا یک ماشین مجازی که درون یک ماشین فیزیکی اجرا می‌شود.

درخواست‌های کسری مجاز هستند. وقتی یک کانتینر با مقدار `spec.containers[].resources.requests.cpu` روی `0.5` تنظیم می‌کنید، در مقایسه با حالتی که `1.0` CPU درخواست می‌کنید، نصف زمان CPU را درخواست می‌کنید. برای واحدهای منابع CPU، عبارت `0.1` معادل عبارت `100m` است که می‌تواند به صورت "صد میلی‌پوینت" خوانده شود. برخی افراد می‌گویند "صد میلی‌کور" و این به معنای یکسانی است.

منبع CPU همیشه به عنوان یک مقدار مطلق از منبع مشخص می‌شود، نه به عنوان یک مقدار نسبی. برای مثال، `500m` CPU تقریباً همان مقدار قدرت محاسباتی را نشان می‌دهد، چه آن کانتینر روی یک دستگاه تک هسته‌ای، دو هسته‌ای یا 48 هسته‌ای اجرا شود.

{{< note >}}
کوبرنتیز به شما اجازه نمی‌دهد منابع CPU را با دقتی کمتر از `1m` یا `0.001` CPU مشخص کنید. برای جلوگیری از استفاده تصادفی از مقدار CPU نامعتبر، هنگام استفاده از کمتر از 1 واحد CPU، بهتر است واحدهای CPU را با استفاده از فرم milliCPU به جای فرم اعشاری مشخص کنید. 

برای مثال، شما یک پاد دارید که از پردازنده‌ی «5m» یا «0.005» استفاده می‌کند و می‌خواهید منابع پردازنده‌ی خود را کاهش دهید. با استفاده از شکل اعشاری، تشخیص اینکه «0.0005» پردازنده یک مقدار نامعتبر است دشوارتر است، در حالی که با استفاده از شکل milliCPU، تشخیص اینکه «0.5m» یک مقدار نامعتبر است آسان‌تر است.
{{< /note >}}

### واحدهای منابع حافظه {#meaning-of-memory}

محدودیت‌ها و درخواست‌ها برای «حافظه» با واحد بایت اندازه‌گیری می‌شوند. می‌توانید حافظه را به صورت یک عدد صحیح ساده یا به صورت یک عدد با ممیز ثابت با استفاده از یکی از این پسوندهای [quantity](/docs/reference/kubernetes-api/common-definitions/quantity/) بیان کنید:
E، P، T، G، M، k. همچنین می‌توانید از معادل‌های توان دو استفاده کنید: Ei، Pi، Ti، Gi، Mi، Ki. به عنوان مثال، موارد زیر تقریباً همان مقدار را نشان می‌دهند:

```shell
128974848, 129e6, 129M,  128974848000m, 123Mi
```

به کوچکی و بزرگی پسوندها توجه کنید. اگر درخواست «۴۰۰ میلیون» حافظه دارید، این درخواست ۰.۴ بایت است. کسی که این را تایپ می‌کند احتمالاً منظورش ۴۰۰ میبی‌بایت (۴۰۰ میلی‌آمپر) یا ۴۰۰ مگابایت (۴۰۰ مگابایت) بوده است.

## مثال منابع کانتینر {#example-1}

پاد زیر دو کانتینر دارد. هر دو کانتینر با درخواستی برای ۰.۲۵ واحد پردازش مرکزی و ۶۴ مگابایت (۲۲۶ بایت) حافظه تعریف شده‌اند. هر کانتینر محدودیت ۰.۵ واحد پردازش مرکزی و ۱۲۸ مگابایت حافظه دارد. می‌توان گفت پاد درخواستی برای ۰.۵ واحد پردازش مرکزی و ۱۲۸ مگابایت حافظه و محدودیت ۱ واحد پردازش مرکزی و ۲۵۶ مگابایت حافظه دارد.

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

## مثال منابع پاد {#example-2}

{{< feature-state feature_gate_name="PodLevelResources" >}}

این ویژگی را می‌توان با تنظیم `PodLevelResources` فعال کرد.
[feature gate](/docs/reference/command-line-tools-reference/feature-gates). پاد زیر درخواست صریح ۱ CPU و ۱۰۰ مگابایت حافظه و محدودیت صریح ۱ CPU و ۲۰۰ مگابایت حافظه دارد. کانتینر `pod-resources-demo-ctr-1` درخواست‌ها و محدودیت‌های صریحی دارد. با این حال، کانتینر `pod-resources-demo-ctr-2` به سادگی منابع موجود در محدوده منابع پاد را به اشتراک می‌گذارد، زیرا درخواست‌ها و محدودیت‌های صریحی ندارد.

{{% code_sample file="pods/resource/pod-level-resources.yaml" %}}

## نحوه زمان‌بندی پادها با درخواست‌های منابع

وقتی یک پاد ایجاد می‌کنید، زمان‌بند کوبرنتیز یک گره را برای پاد انتخاب می‌کند تا روی آن اجرا شود. هر گره برای هر یک از انواع منابع، حداکثر ظرفیتی دارد: مقدار CPU و حافظه‌ای که می‌تواند برای پادها فراهم کند. زمان‌بند تضمین می‌کند که برای هر نوع منبع، مجموع درخواست‌های منبع کانتینرهای زمان‌بندی‌شده کمتر از ظرفیت گره باشد. توجه داشته باشید که اگرچه میزان استفاده واقعی از منابع حافظه یا CPU در گره‌ها بسیار کم است، اما اگر بررسی ظرفیت با شکست مواجه شود، زمان‌بند همچنان از قرار دادن پاد روی یک گره خودداری می‌کند. این امر از کمبود منابع در یک گره در زمانی که استفاده از منابع بعداً افزایش می‌یابد، به عنوان مثال، در طول اوج روزانه نرخ درخواست، جلوگیری می‌کند.

## نحوه اعمال درخواست‌ها و محدودیت‌های منابع توسط کوبرنتیز {#how-pods-with-resource-limits-are-run}

وقتی kubelet یک کانتینر را به عنوان بخشی از یک پاد شروع می‌کند، kubelet درخواست‌ها و محدودیت‌های آن کانتینر برای حافظه و CPU را به زمان اجرای کانتینر ارسال می‌کند.

در لینوکس، زمان اجرای کانتینر معمولاً هسته را پیکربندی می‌کند که محدودیت‌هایی را که شما تعریف کرده‌اید اعمال و اجرا می‌کند.

- محدودیت CPU یک سقف مشخص برای میزان زمان CPU که کانتینر می‌تواند استفاده کند، تعریف می‌کند. در طول هر بازه زمانی (برش زمانی)، هسته لینوکس بررسی می‌کند که آیا از این محدودیت تجاوز شده است یا خیر؛ اگر چنین است، هسته قبل از اجازه دادن به cgroup برای از سرگیری اجرا منتظر می ماند.
- درخواست CPU معمولاً یک وزن‌دهی را تعریف می‌کند. اگر چندین کانتینر مختلف (cgroups) بخواهند روی یک سیستم رقابتی اجرا شوند، به بارهای کاری با درخواست‌های CPU بزرگتر، زمان CPU بیشتری نسبت به بارهای کاری با درخواست‌های کوچک اختصاص داده می‌شود.
- درخواست حافظه عمدتاً در طول زمان‌بندی پاد (کوبرنتیز) استفاده می‌شود. در گره‌ای که از cgroups نسخه ۲ استفاده می‌کند، زمان اجرای کانتینر ممکن است از درخواست حافظه به عنوان راهنمایی برای تنظیم `memory.min` و `memory.low` استفاده کند.
- محدودیت حافظه یک محدودیت حافظه برای آن cgroup تعریف می کند. اگر کانتینر سعی کند حافظه بیشتری از این محدودیت اختصاص دهد، زیرسیستم خارج از حافظه هسته لینوکس فعال می‌شود و معمولاً با متوقف کردن یکی از فرآیندهای کانتینر که سعی در تخصیص حافظه داشته است، مداخله می‌کند. اگر آن فرآیند PID کانتینر ۱ باشد و کانتینر به عنوان قابل راه‌اندازی مجدد علامت‌گذاری شده باشد، کوبرنتیز کانتینر را مجدداً راه‌اندازی می‌کند.
- محدودیت حافظه برای پاد یا کانتینر همچنین می‌تواند برای صفحات موجود در حجم های دارای پشتیبانی حافظه، مانند `emptyDir`، اعمال شود. kubelet حجم های emptyDir `tmpfs` را به عنوان استفاده از حافظه کانتینر، به جای یک حافظه موقت محلی، ردیابی می‌کند. هنگام استفاده از `emptyDir` دارای پشتیبانی حافظه، حتماً نکات [زیر](#memory-backed-emptydir) را بررسی کنید.

اگر یک کانتینر از درخواست حافظه خود فراتر رود و گره‌ای که روی آن اجرا می‌شود به طور کلی با کمبود حافظه مواجه شود، احتمالاً پاد که کانتینر به آن تعلق دارد، {{< glossary_tooltip text="evicted" term_id="eviction" >}} خواهد بود.

ممکن است به یک کانتینر اجازه داده شود که برای مدت زمان طولانی از حد مجاز CPU خود فراتر رود یا اجازه نداشته باشد. با این حال، زمان‌های اجرای کانتینر، پادها یا کانتینرها را به دلیل استفاده بیش از حد از CPU خاتمه نمی‌دهند.

برای تشخیص اینکه آیا یک کانتینر نمی‌تواند زمان‌بندی شود یا به دلیل محدودیت‌های منابع در حال از بین رفتن است، به بخش [عیب‌یابی](#troubleshooting) مراجعه کنید.

### نظارت بر میزان استفاده از منابع محاسباتی و حافظه

kubelet میزان استفاده از منابع یک پاد را به عنوان بخشی از پاد گزارش می‌دهد. [`status`](/docs/concepts/overview/working-with-objects/#object-spec-and-status).

اگر [ابزارهای اختیاری برای نظارت](/docs/tasks/debug/debug-cluster/resource-usage-monitoring/) در خوشه شما موجود باشد، میزان استفاده از منابع پاد را می‌توان یا مستقیماً از [Metrics API](/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api) یا از ابزارهای نظارتی خود بازیابی کرد.

### ملاحظات مربوط به درایوهای `emptyDir` با پشتیبانی حافظه {#memory-backed-emptydir}

{{< caution >}}
اگر برای یک حجم از نوع `emptyDir` `sizeLimit` تعیین نکنید، آن حجم ممکن است تا سقف حافظه آن پاد (`Pod.spec.containers[].resources.limits.memory`) مصرف کند. اگر محدودیت حافظه تعیین نکنید، پاد هیچ حد بالایی برای مصرف حافظه ندارد و می‌تواند تمام حافظه موجود روی گره را مصرف کند. پادهای کوبرنتیز را بر اساس درخواست‌های منابع (`Pod.spec.containers[].resources.requests`) زمان‌بندی می‌کند و هنگام تصمیم‌گیری در مورد اینکه آیا پاد دیگری می‌تواند روی یک گره مشخص قرار گیرد یا خیر، استفاده از حافظه بالاتر از درخواست را در نظر نمی‌گیرد. این می‌تواند منجر به انکار سرویس شود و باعث شود سیستم عامل مدیریت "out-of-memory" (OOM) را انجام دهد. می‌توان هر تعداد `emptyDir` ایجاد کرد که به طور بالقوه می‌توانند تمام حافظه موجود روی گره را مصرف کنند و احتمال OOM را افزایش دهند.
{{< /caution >}}

از منظر مدیریت حافظه، شباهت‌هایی بین زمانی که یک فرآیند از حافظه به عنوان ناحیه کاری استفاده می‌کند و زمانی که از ‎`emptyDir`‎ با پشتیبانی حافظه استفاده می‌کند، وجود دارد. اما هنگام استفاده از حافظه به عنوان یک حجم، مانند `emptyDir` با پشتیبانی حافظه، نکات دیگری نیز وجود دارد که باید به آنها توجه کنید:

* فایل‌های ذخیره شده در یک درایو حافظه‌دار تقریباً به طور کامل توسط برنامه کاربر مدیریت می‌شوند. برخلاف زمانی که به عنوان یک ناحیه کاری برای یک فرآیند استفاده می‌شود، نمی‌توانید به چیزهایی مانند جمع‌آوری زباله در سطح زبان تکیه کنید.
* هدف از نوشتن پرونده ها در یک حجم، ذخیره داده‌ها یا انتقال آنها بین برنامه‌ها است. نه کوبرنتیز و نه سیستم عامل ممکن است به طور خودکار پرونده ها را از یک حجم حذف کنند، بنابراین حافظه استفاده شده توسط آن پرونده ها را نمی‌توان زمانی که سیستم یا پاد تحت فشار حافظه هستند، بازیابی کرد.
* یک `emptyDir` با پشتیبانی حافظه به دلیل عملکردش مفید است، اما حافظه معمولاً از نظر اندازه بسیار کوچکتر و از نظر هزینه بسیار بالاتر از سایر رسانه‌های ذخیره‌سازی مانند دیسک‌ها یا SSDها است. استفاده از مقادیر زیاد حافظه برای حجم‌های `emptyDir` ممکن است بر عملکرد عادی پاد یا کل گره شما تأثیر بگذارد، بنابراین باید با دقت استفاده شود.

اگر در حال مدیریت یک خوشه یا فضای نام هستید، می‌توانید [ResourceQuota](/docs/concepts/policy/resource-quotas/) را نیز تنظیم کنید که استفاده از حافظه را محدود می‌کند؛ همچنین می‌توانید برای اجرای بیشتر، یک [LimitRange](/docs/concepts/policy/limit-range/) تعریف کنید. اگر برای هر پاد یک `spec.containers[].resources.limits.memory` تعیین کنید، حداکثر اندازه یک حجم با `emptyDir`، محدودیت حافظه آن پاد خواهد بود.

به عنوان یک جایگزین، یک مدیر خوشه می‌تواند با استفاده از یک سازوکار سیاست‌گذاری مانند [سیاست پذیرش اعتبارسنجی](/docs/reference/access-authn-authz/validating-admission-policy)، محدودیت‌های اندازه را برای حجم های `emptyDir` در پادهای جدید اعمال کند.

## ذخیره‌سازی موقت محلی

<!-- feature gate LocalStorageCapacityIsolation -->
{{< feature-state for_k8s_version="v1.25" state="stable" >}}

گره‌ها دارای حافظه موقت محلی هستند که توسط دستگاه‌های قابل نوشتن متصل به صورت محلی یا گاهی اوقات توسط RAM پشتیبانی می‌شوند. "موقت" به این معنی است که هیچ تضمینی در مورد دوام طولانی مدت وجود ندارد.

پادها از فضای ذخیره‌سازی محلی موقت برای فضای خالی، ذخیره‌سازی موقت و گزارش‌ها استفاده می‌کنند. kubelet می‌تواند با استفاده از فضای ذخیره‌سازی محلی موقت، فضای خالی را برای پادها فراهم کند تا [`emptyDir`](/docs/concepts/storage/volumes/#emptydir) را در کانتینرها متصل کند.

kubelet همچنین از این نوع ذخیره‌سازی برای نگهداری [گزارش‌های کانتینر سطح گره](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)، تصاویر کانتینر و لایه‌های قابل نوشتن کانتینرهای در حال اجرا استفاده می‌کند.

{{< caution >}}
اگر یک گره از کار بیفتد، داده‌های موجود در حافظه موقت آن ممکن است از بین بروند. برنامه‌های شما نمی‌توانند هیچ SLA عملکردی (مثلاً IOPS دیسک) از حافظه موقت محلی انتظار داشته باشند.
{{< /caution >}}


{{< note >}}
برای اینکه سهمیه‌بندی منابع روی ذخیره‌سازی موقت کار کند، دو کار باید انجام شود:

* یک مدیر، سهمیه منابع را برای ذخیره‌سازی موقت در یک فضای نام تعیین می‌کند.
* کاربر باید محدودیت‌هایی را برای منبع ذخیره‌سازی موقت در مشخصات پاد مشخص کند.

اگر کاربر محدودیت منابع ذخیره‌سازی موقت را در مشخصات پاد مشخص نکند، سهمیه منابع روی ذخیره‌سازی موقت اعمال نمی‌شود.

{{< /note >}}

کوبرنتیز به شما امکان می‌دهد میزان فضای ذخیره‌سازی محلی موقت که یک پاد می‌تواند مصرف کند را ردیابی، ذخیره و محدود کنید.

### پیکربندی‌های مربوط به ذخیره‌سازی موقت محلی

کوبرنتیز از دو روش برای پیکربندی ذخیره‌سازی موقت محلی روی یک گره پشتیبانی می‌کند:
{{< tabs name="local_storage_configurations" >}}
{{% tab name="Single filesystem" %}}
در این پیکربندی، شما انواع مختلف داده‌های محلی موقت (حجم‌های `emptyDir`، لایه‌های قابل نوشتن، image کانتینر، گزارش‌ها) را در یک پرونده سیستم قرار می‌دهید. موثرترین راه برای پیکربندی kubelet به معنای اختصاص این سیستم فایل به داده‌های کوبرنتیز (kubelet) است.

kubelet همچنین گزارش‌های کانتینر سطح گره را می‌نویسد (/docs/concepts/cluster-administration/logging/#logging-at-the-node-level) و با آنها مانند حافظه‌های محلی موقت رفتار می‌کند.

Kubelet گزارش‌ها را در پرونده های داخل پوشه log پیکربندی شده خود می‌نویسد (به طور پیش‌فرض`/var/log`). و یک پوشه پایه برای سایر داده‌های ذخیره‌شده محلی دارد (`/var/lib/kubelet` به‌طور پیش‌فرض).

معمولاً هر دو فایل `/var/lib/kubelet` و `/var/log` در پرونده سیستم ریشه سیستم قرار دارند و kubelet با در نظر گرفتن این طرح‌بندی طراحی شده است.

گره شما می‌تواند به هر تعداد که مایل باشید، پرونده های سیستم دیگری که برای کوبرنتیز استفاده نمی‌شوند، داشته باشد.
{{% /tab %}}
{{% tab name="Two filesystems" %}}
شما یک سیستم فایل روی گره دارید که برای داده‌های موقت ناشی از اجرای Pods: logs و حجم های `emptyDir` استفاده می‌کنید. می‌توانید از این پرونده سیستم برای داده‌های دیگر (مثلاً: logهای سیستم که به کوبرنتیز مربوط نیستند) استفاده کنید؛ حتی می‌تواند پرونده سیستم ریشه باشد.

kubelet همچنین [گزارش‌های کانتینر سطح گره](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level) را در اولین پرونده سیستم می‌نویسد و با آنها مانند حافظه محلی موقت رفتار می‌کند.

شما همچنین از یک پرونده سیستم جداگانه استفاده می‌کنید که توسط یک دستگاه ذخیره‌سازی منطقی متفاوت پشتیبانی می‌شود. در این پیکربندی، پوشه که به kubelet می‌گویید لایه‌های image کانتینر و لایه‌های قابل نوشتن را در آن قرار دهد، در این پرونده سیستم دوم قرار دارد.

پرونده سیستم اول هیچ لایه image یا لایه قابل نوشتنی را در خود جای نمی‌دهد.

گره شما می‌تواند به هر تعداد که مایل باشید، پرونده های سیستم دیگری که برای کوبرنتیز استفاده نمی‌شوند، داشته باشد.
{{% /tab %}}
{{< /tabs >}}

Kubelet می تواند میزان استفاده از فضای ذخیره سازی محلی را اندازه گیری کند. این کار را در صورتی انجام می‌دهد که شما گره را با استفاده از یکی از پیکربندی‌های پشتیبانی‌شده برای فضای ذخیره‌سازی موقت محلی تنظیم کرده باشید.

اگر پیکربندی متفاوتی دارید، kubelet محدودیت منابع را برای ذخیره‌سازی محلی موقت اعمال نمی‌کند.

{{< note >}}
kubelet، حجم‌های emptyDir مربوط به `tmpfs` را به عنوان استفاده از حافظه کانتینر، به جای یک حافظه موقت محلی، ردیابی می‌کند.
{{< /note >}}

{{< note >}}
kubelet فقط پرونده سیستم ریشه را برای ذخیره‌سازی موقت ردیابی می‌کند. طرح‌بندی‌های سیستم عاملی که یک دیسک جداگانه را در `/var/lib/kubelet` یا `/var/lib/containers` نصب می‌کنند، ذخیره‌سازی موقت را به درستی گزارش نمی‌کنند.
{{< /note >}}

### تنظیم درخواست‌ها و محدودیت‌ها برای ذخیره‌سازی موقت محلی

شما می‌توانید برای مدیریت ذخیره‌سازی موقت محلی، از `ephemeral-storage` استفاده کنید. هر کانتینر از یک پاد می‌تواند یک یا هر دو مورد زیر را مشخص کند:

* `spec.containers[].resources.limits.ephemeral-storage`
* `spec.containers[].resources.requests.ephemeral-storage`

محدودیت‌ها و درخواست‌ها برای «ذخیره‌سازی موقت» با کمیت‌های بایتی اندازه‌گیری می‌شوند. می‌توانید ذخیره‌سازی را به صورت یک عدد صحیح ساده یا به صورت یک عدد با ممیز ثابت با استفاده از یکی از این پسوندها بیان کنید:
E، P، T، G، M، k. همچنین می‌توانید از معادل‌های توان دو استفاده کنید: Ei، Pi، Ti، Gi، Mi، Ki. به عنوان مثال، کمیت‌های زیر تقریباً نشان‌دهنده یک مقدار هستند:

- `128974848`
- `129e6`
- `129M`
- `123Mi`

به کوچکی و بزرگی پسوندها توجه کنید. اگر درخواست `400m` فضای ذخیره‌سازی موقت را دارید، این درخواست ۰.۴ بایت است. کسی که این را تایپ می‌کند احتمالاً منظورش ۴۰۰ مگابایت (۴۰۰ میلی‌) یا ۴۰۰ مگابایت (۴۰۰ مگابایت) بوده است.

در مثال زیر، پاد دو کانتینر دارد. هر کانتینر درخواستی به میزان ۲ گیگابایت فضای ذخیره‌سازی موقت محلی دارد. هر کانتینر محدودیتی به میزان ۴ گیگابایت فضای ذخیره‌سازی موقت محلی دارد. بنابراین، پاد درخواستی به میزان ۴ گیگابایت فضای ذخیره‌سازی موقت محلی و محدودیتی به میزان ۸ گیگابایت فضای ذخیره‌سازی موقت محلی دارد. ۵۰۰ میلیون از این محدودیت می‌تواند توسط درایو `emptyDir` مصرف شود.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
    volumeMounts:
    - name: ephemeral
      mountPath: "/tmp"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
    volumeMounts:
    - name: ephemeral
      mountPath: "/tmp"
  volumes:
    - name: ephemeral
      emptyDir:
        sizeLimit: 500Mi
```

### نحوه زمان‌بندی پادها با درخواست‌های ذخیره‌سازی موقت

وقتی یک پاد ایجاد می‌کنید، زمان‌بند کوبرنتیز یک گره را برای اجرا روی پاد انتخاب می‌کند. هر گره حداکثر مقدار فضای ذخیره‌سازی موقت محلی را دارد که می‌تواند برای پادها فراهم کند. برای اطلاعات بیشتر، به [قابلیت تخصیص گره](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable) مراجعه کنید.

زمانبند تضمین می‌کند که مجموع درخواست‌های منابع کانتینرهای زمان‌بندی‌شده کمتر از ظرفیت گره باشد.

### مدیریت مصرف ذخیره‌سازی زودگذر {#resource-emphemeralstorage-consumption}

اگر kubelet فضای ذخیره‌سازی موقت محلی را به عنوان یک منبع مدیریت کند، kubelet میزان استفاده از فضای ذخیره‌سازی را بر حسب موارد زیر اندازه‌گیری می‌کند:

- درایوهای `emptyDir`، به جز درایوهای `emptyDir` با پسوند _tmpfs_
- پوشه‌هایی که لاگ‌های سطح گره را نگه می‌دارند
- لایه های قابل نوشتن کانتینر

اگر یک پاد بیش از آنچه شما اجازه می‌دهید از فضای ذخیره‌سازی موقت استفاده کند، kubelet یک سیگنال تخلیه تنظیم می‌کند که باعث تخلیه پاد می‌شود.

برای جداسازی در سطح کانتینر، اگر لایه قابل نوشتن و میزان استفاده از لاگ یک کانتینر از حد مجاز ذخیره‌سازی آن فراتر رود، kubelet، پاد را برای حذف علامت‌گذاری می‌کند.

برای جداسازی در سطح پاد، kubelet با جمع کردن محدودیت‌های مربوط به کانتینرهای موجود در آن پاد، محدودیت کلی ذخیره‌سازی پاد را محاسبه می‌کند. در این حالت، اگر مجموع استفاده از فضای ذخیره‌سازی موقت محلی از همه کانتینرها و همچنین حجم‌های `emptyDir` پاد از محدودیت کلی ذخیره‌سازی پاد بیشتر شود، kubelet نیز پاد را برای تخلیه علامت‌گذاری می‌کند.

{{< caution >}}
اگر kubelet فضای ذخیره‌سازی موقت محلی را اندازه‌گیری نکند، پادی که از محدودیت فضای ذخیره‌سازی محلی خود فراتر رود، به دلیل نقض محدودیت‌های منابع ذخیره‌سازی محلی حذف نخواهد شد.

با این حال، اگر فضای پرونده سیستم برای لایه‌های کانتینر قابل نوشتن، لاگ‌های سطح گره یا حجم‌های `emptyDir` کم شود، گره خود به خود دچار کمبود فضای ذخیره‌سازی محلی می‌شود و این نقص باعث حذف هر پاد می‌شود که به طور خاص این نقص را تحمل نمی‌کند.

برای ذخیره‌سازی محلی موقت، به [پیکربندی‌ها](#configurations-for-local-ephemeral-storage) پشتیبانی‌شده مراجعه کنید.
{{< /caution >}}

kubelet از روش‌های مختلفی برای اندازه‌گیری میزان استفاده از فضای ذخیره‌سازی پاد پشتیبانی می‌کند:

{{< tabs name="resource-emphemeralstorage-measurement" >}}
{{% tab name="Periodic scanning" %}}
kubelet بررسی‌های منظم و زمان‌بندی‌شده‌ای را انجام می‌دهد که هر درایو `emptyDir`، پوشه لاگ کانتینر و لایه قابل نوشتن کانتینر را اسکن می‌کند.

اسکن میزان فضای استفاده شده را اندازه‌گیری می‌کند.

{{< note >}}
در این حالت، kubelet توصیف‌گرهای پرونده باز را برای پرونده های حذف‌شده ردیابی نمی‌کند.

اگر شما (یا یک کانتینر) پرونده ای را درون یک درایو `emptyDir` ایجاد کنید، چیزی آن پرونده را باز کند و شما پرونده را در حالی که هنوز باز است حذف کنید، inode مربوط به پرونده حذف شده تا زمانی که آن پرونده را ببندید باقی می‌ماند، اما kubelet فضا را به عنوان در حال استفاده طبقه‌بندی نمی‌کند.
{{< /note >}}
{{% /tab %}}
{{% tab name="Filesystem project quota" %}}

{{< feature-state feature_gate_name="LocalStorageCapacityIsolationFSQuotaMonitoring" >}}

سهمیه‌بندی پروژه یک ویژگی در سطح سیستم عامل برای مدیریت استفاده از فضای ذخیره‌سازی در پرونده های سیستمی است. با کوبرنتیز، می‌توانید سهمیه‌بندی پروژه را برای نظارت بر استفاده از فضای ذخیره‌سازی فعال کنید. مطمئن شوید که پرونده سیستمی که از حجم‌های `emptyDir` در گره پشتیبانی می‌کند، از سهمیه‌بندی پروژه پشتیبانی می‌کند. به عنوان مثال، XFS و ext4fs سهمیه‌بندی پروژه را ارائه می‌دهند.

{{< note >}}
سهمیه‌های پروژه به شما امکان می‌دهند میزان استفاده از فضای ذخیره‌سازی را رصد کنید؛ آنها محدودیتی اعمال نمی‌کنند.
{{< /note >}}

کوبرنتیز از شناسه‌های پروژه با شروع از `1048576` استفاده می‌کند. شناسه‌های مورد استفاده در `/etc/projects` و `/etc/projid` ثبت شده‌اند. اگر شناسه‌های پروژه در این محدوده برای اهداف دیگری در سیستم استفاده شوند، آن شناسه‌های پروژه باید در `/etc/projects` و `/etc/projid` ثبت شوند تا کوبرنتیز از آنها استفاده نکند.

سهمیه‌بندی‌ها سریع‌تر و دقیق‌تر از اسکن پوشه هستند. وقتی یک پوشه به یک پروژه اختصاص داده می‌شود، تمام پرونده های ایجاد شده تحت آن پوشه در آن پروژه ایجاد می‌شوند و هسته صرفاً باید تعداد بلوک‌های مورد استفاده توسط پرونده های آن پروژه را پیگیری کند. اگر پرونده ای ایجاد و حذف شود، اما دارای یک توصیف‌گر پرونده باز باشد، همچنان به مصرف فضا ادامه می‌دهد. ردیابی سهمیه‌بندی آن فضا را به طور دقیق ثبت می‌کند، در حالی که اسکن پوشه، فضای استفاده شده توسط پرونده‌های حذف شده را نادیده می‌گیرد.

برای استفاده از سهمیه‌ها برای ردیابی میزان استفاده از منابع یک پاد، پاد باید در یک فضای نام کاربری باشد. در فضاهای نام کاربری، هسته تغییرات در شناسه‌های پروژه در سیستم پرونده را محدود می‌کند و قابلیت اطمینان معیارهای ذخیره‌سازی محاسبه‌شده توسط سهمیه‌ها را تضمین می‌کند.

اگر می‌خواهید از سهمیه‌های پروژه استفاده کنید، باید:

* با استفاده از بخش `featureGates` در پرونده `kubelet configuration`، گزینه `LocalStorageCapacityIsolationFSQuotaMonitoring=true` را فعال کنید.

* مطمئن شوید که `UserNamespacesSupport` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) فعال است و هسته، پیاده‌سازی CRI و موتور OCI از فضاهای نام کاربری پشتیبانی می‌کنند.

* مطمئن شوید که سهمیه‌بندی پروژه در پرونده سیستم ریشه (یا پرونده سیستم مجری اختیاری) فعال باشد. همه پرونده های سیستمی XFS از سهمیه‌بندی پروژه پشتیبانی می‌کنند. برای پرونده سیستمی ext4، باید ویژگی ردیابی سهمیه‌بندی پروژه را در حالی که پرونده سیستم نصب نشده است، فعال کنید.

  ```bash
  # For ext4, with /dev/block-device not mounted
  sudo tune2fs -O project -Q prjquota /dev/block-device
  ```

* مطمئن شوید که پرونده سیستم ریشه (یا پرونده سیستم مجری اختیاری) با سهمیه‌بندی پروژه فعال شده باشد. برای هر دو پرونده سیستم XFS و ext4fs، گزینه mount با نام `prjquota` نامگذاری شده است.


اگر نمی‌خواهید از سهمیه پروژه استفاده کنید، باید:

* غیرفعال کردن `LocalStorageCapacityIsolationFSQuotaMonitoring`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
با استفاده از بخش `featureGates` در `[kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/).`
{{% /tab %}}
{{< /tabs >}}

## منابع توسعه‌یافته

منابع توسعه‌یافته، نام‌های منبع کاملاً واجد شرایط خارج از دامنه‌ی `kubernetes.io` هستند. آن‌ها به اپراتورهای خوشه اجازه می‌دهند تا منابع غیر توکار کوبرنتیز را تبلیغ کنند و به کاربران اجازه می‌دهند تا از آن‌ها استفاده کنند.

برای استفاده از منابع توسعه‌یافته دو مرحله لازم است. اول، اپراتور خوشه باید یک منبع توسعه‌یافته را تبلیغ کند. دوم، کاربران باید منبع توسعه‌یافته را در پادها درخواست کنند.

### مدیریت منابع توسعه‌یافته

#### منابع توسعه‌یافته در سطح گره

منابع توسعه‌یافته در سطح گره به گره‌ها گره خورده‌اند.

##### منابع مدیریت شده افزونه دستگاه
برای نحوه‌ی تبلیغ منابع مدیریت‌شده افزونه‌ی دستگاه روی هر گره، به [افزونه‌ی دستگاه](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) مراجعه کنید.

##### منابع دیگر

برای تبلیغ یک منبع توسعه‌یافته جدید در سطح گره، اپراتور خوشه می‌تواند یک درخواست HTTP از نوع PATCH به سرور API ارسال کند تا مقدار موجود در `status.capacity` را برای یک گره در خوشه مشخص کند. پس از این عملیات، `status.capacity` گره شامل یک منبع جدید خواهد بود. بخش `status.allocatable` به طور خودکار و غیرهمزمان توسط kubelet با منبع جدید به‌روزرسانی می‌شود.

از آنجا که زمان‌بند هنگام ارزیابی اندازه پاد از مقدار `status.allocatable` گره استفاده می‌کند، زمان‌بند فقط مقدار جدید را پس از آن به‌روزرسانی ناهمزمان در نظر می‌گیرد. ممکن است بین وصله‌بندی ظرفیت گره با یک منبع جدید و زمانی که اولین پادی که منبع را درخواست می‌کند می‌تواند روی آن گره زمان‌بندی شود، تأخیر کوتاهی وجود داشته باشد.

**مثال:**

در اینجا مثالی آورده شده است که نحوه استفاده از `curl` برای تشکیل یک درخواست HTTP را نشان می‌دهد که پنج منبع "example.com/foo" را در گره `k8s-node-1` که سرور اصلی آن `k8s-master` است، نشان می دهد.

```shell
curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
```

{{< note >}}
در درخواست قبلی، `~1` کدگذاری کاراکتر `/` در مسیر وصله است. مقدار مسیر عملیات در JSON-Patch به عنوان یک `JSON-Pointer` تفسیر می‌شود. برای جزئیات بیشتر، به [IETF RFC 6901، بخش 3] (https://tools.ietf.org/html/rfc6901#section-3) مراجعه کنید.
{{< /note >}}

#### منابع توسعه‌یافته در سطح خوشه‌

منابع توسعه‌یافته در سطح خوشه به گره‌ها وابسته نیستند. آن‌ها معمولاً توسط توسعه‌دهندگان زمان‌بندی مدیریت می‌شوند که مصرف منابع و سهمیه منابع را مدیریت می‌کنند.

می‌توانید منابع توسعه‌یافته‌ای را که توسط توسعه‌دهندگان زمان‌بندی مدیریت می‌شوند، در [scheduler configuration](/docs/reference/config-api/kube-scheduler-config.v1/) مشخص کنید.

**مثال:**

پیکربندی زیر برای یک سیاست زمان‌بندی نشان می‌دهد که منبع توسعه‌یافته در سطح خوشه "example.com/foo" توسط توسعه‌دهنده زمان‌بندی مدیریت می‌شود.

- زمانبند فقط در صورتی که پاد درخواست "example.com/foo" را داشته باشد، یک پاد به توسعه‌دهنده زمانبند ارسال می‌کند.
- بخش «ignoredByScheduler» مشخص می‌کند که زمان‌بند، منبع «example.com/foo» را در گزاره «PodFitsResources» خود بررسی نمی‌کند.

```json
{
  "kind": "Policy",
  "apiVersion": "v1",
  "extenders": [
    {
      "urlPrefix":"<extender-endpoint>",
      "bindVerb": "bind",
      "managedResources": [
        {
          "name": "example.com/foo",
          "ignoredByScheduler": true
        }
      ]
    }
  ]
}
```

### مصرف منابع توسعه‌یافته

کاربران می‌توانند از منابع توسعه‌یافته در مشخصات پاد مانند پردازنده و حافظه استفاده کنند. زمان‌بند، حسابداری منابع را به گونه‌ای انجام می‌دهد که بیش از مقدار موجود به طور همزمان به پادها اختصاص داده نشود.

سرور API مقادیر منابع توسعه‌یافته را به اعداد صحیح محدود می‌کند. نمونه‌هایی از مقادیر معتبر عبارتند از `3`، `3000m` و `3Ki`. نمونه‌هایی از مقادیر نامعتبر عبارتند از `0.5` و `1500m` (زیرا `1500m` منجر به `1.5` می‌شود).

{{< note >}}
منابع توسعه‌یافته جایگزین منابع عددی مبهم شده‌اند. کاربران می‌توانند از هر پیشوند نام دامنه‌ای به‌جز `kubernetes.io` استفاده کنند، چرا که این پیشوند برای کوبرنتیز رزرو شده است.
{{< /note >}}

برای استفاده از یک منبع توسعه‌یافته در یک پاد، نام منبع را به عنوان کلید در نگاشت `spec.containers[].resources.limits` در مشخصات کانتینر قرار دهید.

{{< note >}}
منابع توسعه‌یافته نمی‌توانند بیش از حد تخصیص داده شوند، بنابراین اگر درخواست و محدودیت در مشخصات کانتینر وجود داشته باشند، باید برابر باشند.
{{< /note >}}

یک پاد تنها در صورتی زمان‌بندی می‌شود که تمام درخواست‌های منابع، از جمله پردازنده، حافظه و هرگونه منابع توسعه‌یافته، برآورده شوند. پاد تا زمانی که درخواست منبع برآورده نشود، در حالت `PENDING` باقی می‌ماند.

**مثال:**

پاد زیر درخواست ۲ پردازنده و ۱ "example.com/foo" (یک منبع توسعه‌یافته) را دارد.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: myimage
    resources:
      requests:
        cpu: 2
        example.com/foo: 1
      limits:
        example.com/foo: 1
```

## محدود کردن PID

محدودیت‌های شناسه فرآیند (PID) به پیکربندی یک kubelet اجازه می‌دهد تا تعداد PIDهایی را که یک Pod مشخص می‌تواند مصرف کند، محدود کند. برای اطلاعات بیشتر به [محدود کردن PID](/docs/concepts/policy/pid-limiting/) مراجعه کنید.

## عیب‌یابی

### پادهای من با پیام رویداد `FailedScheduling` در حالت انتظار هستند.

اگر زمانبند نتواند هیچ گره‌ای را پیدا کند که یک پاد بتواند در آن قرار گیرد، پاد تا زمانی که مکانی پیدا نشود، بدون زمان‌بندی باقی می‌ماند. هر بار که زمانبند نتواند مکانی برای پاد پیدا کند، یک رویداد [Event](/docs/reference/kubernetes-api/cluster-resources/event-v1/) تولید می‌شود. می‌توانید از `kubectl` برای مشاهده رویدادهای یک پاد استفاده کنید. به عنوان مثال:

```shell
kubectl describe pod frontend | grep -A 9999999999 Events
```
```
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
```

در مثال قبلی، پاد با نام "frontend" به دلیل کمبود منابع CPU در هر گره، در زمان‌بندی با شکست مواجه می‌شود. پیام‌های خطای مشابه همچنین می‌توانند نشان‌دهنده‌ی شکست به دلیل کمبود حافظه (پاد از حافظه آزاد فراتر می‌رود) باشند. به طور کلی، اگر یک پاد با پیامی از این نوع در حال انتظار باشد، چندین کار برای امتحان کردن وجود دارد:

- گره‌های بیشتری به خوشه اضافه کنید.
- پادهای غیرضروری را خاتمه دهید تا جا برای پادهای در حال انتظار باز شود.
- بررسی کنید که پاد از همه گره‌ها بزرگتر نباشد. برای مثال، اگر همه گره‌ها ظرفیت `cpu: 1` داشته باشند، پاد با درخواست `cpu: 1.1` هرگز زمان‌بندی نخواهد شد.
- بررسی کنید که آیا نودها Taint دارند یا نه. اگر بیشتر نودهای شما Taint شده باشند و پاد جدید این Taint را تحمل  نکند، زمانبند فقط آن دسته از نودهایی را برای جای‌گذاری در نظر می‌گیرد که آن Taint خاص را ندارند.

شما می‌توانید ظرفیت گره‌ها و مقادیر اختصاص داده شده را با دستور `kubectl describe nodes` بررسی کنید. برای مثال:

```shell
kubectl describe nodes e2e-test-node-pool-4lw4
```
```
Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
```

در خروجی قبلی، می‌توانید ببینید که اگر یک پاد بیش از ۱.۱۲۰ CPU یا بیش از ۶.۲۳Gi حافظه درخواست کند، آن پاد روی گره جا نمی‌شود.

با نگاه کردن به بخش “Pods”، می‌توانید ببینید کدام پادها فضای گره را اشغال کرده‌اند.

میزان منابع موجود برای پادها کمتر از ظرفیت گره است زیرا سرویس‌های سیستم بخشی از منابع موجود را استفاده می‌کنند. در API کوبرنتیز، هر گره یک بخش `.status.allocatable` دارد (برای جزئیات بیشتر به [وضعیت گره](/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus) مراجعه کنید).

بخش `.status.allocatable` میزان منابعی را که برای پادها در آن گره در دسترس است، توصیف می‌کند (برای مثال: ۱۵ پردازنده مجازی و ۷۵۳۸ مگابایت حافظه). برای اطلاعات بیشتر در مورد منابع قابل تخصیص گره در کوبرنتیز، به [رزرو منابع محاسباتی برای سرویس های سیستم](/docs/tasks/administer-cluster/reserve-compute-resources/) مراجعه کنید.

شما می‌توانید [سهمیه منابع](/docs/concepts/policy/resource-quotas/) را برای محدود کردن کل منابعی که یک فضای نام می‌تواند مصرف کند، پیکربندی کنید. کوبرنتیز سهمیه‌بندی را برای اشیاء در یک فضای نام خاص اعمال می‌کند، زمانی که سهمیه منابع در آن فضای نام وجود داشته باشد. به عنوان مثال، اگر فضاهای نام خاصی را به تیم‌های مختلف اختصاص دهید، می‌توانید سهمیه منابع را به آن فضاهای نام اضافه کنید. تنظیم سهمیه‌بندی منابع به جلوگیری از استفاده بیش از حد یک تیم از هر منبعی که این استفاده بیش از حد بر تیم‌های دیگر تأثیر می‌گذارد، کمک می‌کند.

همچنین باید در نظر بگیرید که چه دسترسی‌هایی را به آن فضای نام اعطا می‌کنید:
**دسترسی کامل** برای نوشتن به یک فضای نام به فردی که آن دسترسی را دارد، اجازه می‌دهد هر منبعی، از جمله سهمیه منابع پیکربندی شده را حذف کند.

### کانتینر من خاتمه یافته است

ممکن است کانتینر شما به دلیل کمبود منابع از کار بیفتد. برای بررسی اینکه آیا یک کانتینر به دلیل رسیدن به محدودیت منابع از کار افتاده است یا خیر، دستور `kubectl describe pod` را در پاد مورد نظر فراخوانی کنید:

```shell
kubectl describe pod simmemleak-hra99
```

خروجی مشابه زیر است:
```
Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image "saadali/simmemleak:latest" already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
```

در مثال قبلی، `تعداد راه‌اندازی مجدد: ۵` نشان می‌دهد که کانتینر `simmemleak` در پاد پنج بار (تاکنون) خاتمه یافته و مجدداً راه‌اندازی شده است. دلیل `OOMKilled` نشان می‌دهد که کانتینر سعی کرده از حافظه بیشتری نسبت به محدودیت خود استفاده کند.

قدم بعدی شما می‌تواند بررسی کد برنامه برای یافتن نشت حافظه باشد. اگر متوجه شدید که برنامه مطابق انتظار شما رفتار می‌کند، تنظیم محدودیت حافظه بالاتر (و احتمالاً درخواست) برای آن کانتینر را در نظر بگیرید.

## {{% heading "whatsnext" %}}

* تجربه عملی [اختصاص منابع حافظه به کانتینرها و پادها](/docs/tasks/configure-pod-container/assign-memory-resource/) را کسب کنید.
* تجربه عملی [اختصاص منابع CPU به کانتینرها و Podها](/docs/tasks/configure-pod-container/assign-cpu-resource/) را کسب کنید.
* نحوه تعریف یک [کانتینر](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) توسط مرجع API و [منابع مورد نیاز](/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources) آن را بخوانید.
* درباره [سهمیه‌بندی پروژه](https://www.linux.org/docs/man8/xfs_quota.html) در XFS بخوانید
* برای اطلاعات بیشتر در مورد [مرجع پیکربندی kube-scheduler (نسخه ۱)](/docs/reference/config-api/kube-scheduler-config.v1/) مطالعه کنید.
* درباره [کلاس‌های کیفیت سرویس برای پادها](/docs/concepts/workloads/pods/pod-qos/) بیشتر بخوانید
