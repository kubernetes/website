---
title: 대형 클러스터에 대한 고려 사항
weight: 20
---

클러스터는 {{< glossary_tooltip text="컨트롤 플레인" term_id="control-plane" >}}에서 관리하는
쿠버네티스 에이전트를 실행하는 {{< glossary_tooltip text="노드" term_id="node" >}}(물리
또는 가상 머신)의 집합이다.
쿠버네티스 {{<param "version">}}는 노드 5000개까지의 클러스터를 지원한다. 보다 정확하게는,
쿠버네티스는 다음 기준을 *모두* 만족하는 설정을 수용하도록 설계되었다.

* 노드 당 파드 100 개 이하
* 노드 5000개 이하
* 전체 파드 150000개 이하
* 전체 컨테이너 300000개 이하

노드를 추가하거나 제거하여 클러스터를 확장할 수 있다. 이를 수행하는 방법은
클러스터 배포 방법에 따라 다르다.

## 클라우드 프로바이더 리소스 쿼터 {#quota-issues}

여러 노드를 가지는 클러스터를 만들 때, 클라우드 프로바이더 쿼터 이슈를 피하기 위해
고려할 점은 다음과 같다.

* 다음과 같은 클라우드 리소스에 대한 쿼터 증가를 요청한다.
    * 컴퓨터 인스턴스
    * CPU
    * 스토리지 볼륨
    * 사용 중인 IP 주소
    * 패킷 필터링 규칙 세트
    * 로드밸런서 개수
    * 로그 스트림
* 일부 클라우드 프로바이더는 새로운 인스턴스 생성 속도에 상한이 있어, 클러스터 확장 작업 간 새로운 노드를 일괄적으로 배치하고, 배치 간에 일시 중지한다.

## 컨트롤 플레인 컴포넌트

대규모 클러스터의 경우, 충분한 컴퓨트 및 기타 리소스가 있는 컨트롤 플레인이
필요하다.

일반적으로 장애 영역 당 하나 또는 두 개의 컨트롤 플레인 인스턴스를
실행하고, 해당 인스턴스를 수직으로(vertically) 먼저 확장한 다음 (수직) 규모로 하락하는
지점에 도달한 후 수평으로(horizontally) 확장한다.

내결함성을 제공하려면 장애 영역 당 하나 이상의 인스턴스를 실행해야 한다. 쿠버네티스
노드는 동일한 장애 영역에 있는 컨트롤 플레인 엔드포인트로 트래픽을
자동으로 조정하지 않는다. 그러나, 클라우드 프로바이더는 이를 수행하기 위한 자체 메커니즘을 가지고 있을 수 있다.

예를 들어, 관리형 로드 밸런서를 사용하여 장애 영역 _A_ 의
kubelet 및 파드에서 시작되는 트래픽을 전송하도록 로드 밸런서를 구성하고, 해당 트래픽을
_A_ 영역에 있는 컨트롤 플레인 호스트로만 전달한다. 단일 컨트롤 플레인 호스트 또는
엔드포인트 장애 영역 _A_ 이 오프라인이 되면, 영역 _A_ 의 노드에 대한
모든 컨트롤 플레인 트래픽이 이제 영역간에 전송되고 있음을 의미한다. 각 영역에서 여러 컨트롤 플레인 호스트를
실행하면 가용성이 낮아진다.

### etcd 저장소

큰 클러스터의 성능 향상을 위해, 사용자는 이벤트 오브젝트를 각각의
전용 etcd 인스턴스에 저장한다.

클러스터 생성시의 부가 스트립트이다.
클러스터 생성 시에 (사용자 도구를 사용하여) 다음을 수행할 수 있다.

* 추가 etcd 인스턴스 시작 및 설정
* 이벤트를 저장하기 위한 {{< glossary_tooltip term_id="kube-apiserver" text="API server" >}} 설정

[쿠버네티스를 위한 etcd 클러스터 운영하기](/docs/tasks/administer-cluster/configure-upgrade-etcd/)와
[kubeadm을 이용하여 고가용성 etcd 생성하기](/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)에서 
큰 클러스터를 위한 etcd를 설정하고 관리하는 방법에 대한 상세 사항을 확인한다.

## 애드온 리소스

쿠버네티스 [리소스 제한](/ko/docs/concepts/configuration/manage-resources-containers/)은
파드와 컨테이너가 다른 컴포넌트에 영향을 줄 수 있는 메모리 누수 및 기타 방식의 영향을
최소화하는 데 도움이 된다. 이러한 리소스 제한은 애플리케이션 워크로드에 적용될 수 있는 것처럼 
{{< glossary_tooltip text="애드온" term_id="addons" >}} 리소스에도 적용될 수 있다.

예를 들어, 로깅 컴포넌트에 대한 CPU 및 메모리 제한을 설정할 수 있다.

```yaml
  ...
  containers:
  - name: fluentd-cloud-logging
    image: fluent/fluentd-kubernetes-daemonset:v1
    resources:
      limits:
        cpu: 100m
        memory: 200Mi
```

애드온의 기본 제한은 일반적으로 중소형 쿠버네티스 클러스터에서
각 애드온을 실행한 경험에서 수집된 데이터를 기반으로 한다. 대규모 클러스터에서
실행할 때, 애드온은 종종 기본 제한보다 많은 리소스를 소비한다.
이러한 값을 조정하지 않고 대규모 클러스터를 배포하면, 애드온이
메모리 제한에 계속 도달하기 때문에 지속적으로 종료될 수 있다.
또는, 애드온이 실행될 수 있지만 CPU 시간 슬라이스 제한으로 인해
성능이 저하된다.

클러스터 애드온 리소스 문제가 발생하지 않도록, 노드가 많은 클러스터를
만들 때, 다음 사항을 고려한다.

* 일부 애드온은 수직으로 확장된다. 클러스터 또는 전체 장애 영역을
  제공하는 애드온 레플리카가 하나 있다. 이러한 애드온의 경우, 클러스터를 확장할 때
  요청과 제한을 늘린다.
* 많은 애드온은 수평으로 확장된다. 더 많은 파드를 실행하여 용량을 추가하지만,
  매우 큰 클러스터에서는 CPU 또는 메모리 제한을 약간 높여야 할 수도 있다.
  VerticalPodAutoscaler는 _recommender_ 모드에서 실행되어 요청 및 제한에 대한
  제안 수치를 제공할 수 있다.
* 일부 애드온은 {{< glossary_tooltip text="데몬셋(DaemonSet)"
  term_id="daemonset" >}}에 의해 제어되는 노드 당 하나의 복사본으로 실행된다(예: 노드 수준 로그 수집기). 수평
  확장 애드온의 경우와 유사하게, CPU 또는 메모리 제한을 약간 높여야
  할 수도 있다.

## {{% heading "whatsnext" %}}

`VerticalPodAutoscaler` 는 리소스 요청 및 파드 제한을 관리하는 데 도움이 되도록
클러스터에 배포할 수 있는 사용자 정의 리소스이다.
클러스터에 중요한 애드온을 포함하여 클러스터 컴포넌트를 확장하는 방법에 대한
자세한 내용은 [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme)를
방문하여 배워본다.

[클러스터 오토스케일러](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme)는
여러 클라우드 프로바이더와 통합되어 클러스터의 리소스 요구 수준에 맞는
노드 수를 실행할 수 있도록 도와준다.
