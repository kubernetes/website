---
title: APIの優先度とフェアネス
content_type: concept
min-kubernetes-server-version: v1.18
weight: 110
---

<!-- overview -->

{{< feature-state state="stable"  for_k8s_version="v1.29" >}}

過負荷状態におけるKubernetes APIサーバーの動作を制御することは、クラスター管理者にとって重要なタスクです。
{{< glossary_tooltip term_id="kube-apiserver" text="kube-apiserver" >}}には、受け付ける処理量を制限するためにいくつかのコントロール(すなわち`--max-requests-inflight`および`--max-mutating-requests-inflight`コマンドラインフラグ)が用意されており、大量のリクエストがAPIサーバーに殺到してクラッシュするのを防ぎますが、これらのフラグだけではトラフィックが集中する期間に最も重要なリクエストを確実に通過させるには不十分です。

APIの優先度とフェアネス(APF)機能は、前述のmax-inflightの制限を改善する代替手段です。
APFはリクエストをより細かい粒度で分類し、分離します。
また、限られた量のキューイングを導入しており、非常に短いバーストの場合にリクエストが拒否されないようにしています。
リクエストはフェアキューイング技術を使用してキューからディスパッチされるため、例えば、振る舞いの悪い{{< glossary_tooltip text="コントローラー" term_id="controller" >}}が(同一の優先度レベルであっても)他のコントローラーを枯渇させることはありません。

この機能は、インフォーマーを使用し、APIリクエストの失敗に対して指数バックオフで対応する標準的なコントローラーや、同様に動作する他のクライアントとうまく連携するように設計されています。

{{< caution >}}
「長時間実行」として分類される一部のリクエスト&mdash;リモートコマンド実行やログテーリングなど&mdash;は、APIの優先度とフェアネスフィルターの対象外です。
これは、APIの優先度とフェアネス機能が有効になっていない場合の`--max-requests-inflight`フラグについても同様です。
APIの優先度とフェアネスは**watch**リクエストには適用_されます_。
APIの優先度とフェアネスが無効な場合、**watch**リクエストは`--max-requests-inflight`の制限を受けません。
{{< /caution >}}

<!-- body -->

## APIの優先度とフェアネスの有効化/無効化

APIの優先度とフェアネス機能はコマンドラインフラグで制御され、デフォルトで有効になっています。
利用可能なkube-apiserverのコマンドラインオプションとその有効化・無効化方法に関する一般的な説明については、[オプション](/docs/reference/command-line-tools-reference/kube-apiserver/#options)を参照してください。
APFのコマンドラインオプション名は「--enable-priority-and-fairness」です。
この機能には、以下を含む{{<glossary_tooltip term_id="api-group" text="APIグループ" >}}も関係しています。
(a) 1.29で導入されデフォルトで有効になっている安定版`v1`バージョン、および(b) デフォルトで有効でv1.29で非推奨となった`v1beta3`バージョン。
以下のコマンドラインフラグを`kube-apiserver`の起動時に追加することで、APIグループのベータバージョン`v1beta3`を無効にできます。

```shell
kube-apiserver \
--runtime-config=flowcontrol.apiserver.k8s.io/v1beta3=false \
 # …およびその他の通常のフラグ
```

コマンドラインフラグ`--enable-priority-and-fairness=false`を使用すると、APIの優先度とフェアネス機能が無効になります。

## 再帰サーバーシナリオ

APIの優先度とフェアネスは再帰サーバーシナリオで注意して使用する必要があります。
これは、サーバーAがリクエストを処理している最中に、サーバーBに対して補助リクエストを発行するシナリオです。
サーバーBはさらにサーバーAに対して補助呼び出しを行う可能性もあります。
元のリクエストといくつかの補助リクエストの両方に優先度とフェアネスの制御が適用される状況では、再帰の深さに関わらず、優先度の逆転やデッドロックの危険があります。

再帰の一例は、`kube-apiserver`がサーバーBに対してアドミッションWebhook呼び出しを発行し、その呼び出しの処理中にサーバーBが`kube-apiserver`に対してさらなる補助リクエストを行う場合です。
再帰のもう一つの例は、`APIService`オブジェクトが特定のAPIグループに関するリクエストをカスタム外部サーバーBに委任するよう`kube-apiserver`に指示する場合です(これは「アグリゲーション」と呼ばれるものの一つです)。

元のリクエストが特定の優先度レベルに属することがわかっていて、補助的な制御されたリクエストがより高い優先度レベルに分類される場合、これは一つの可能な解決策です。
元のリクエストが任意の優先度レベルに属する可能性がある場合、補助的な制御されたリクエストは優先度とフェアネスの制限から免除される必要があります。
その方法の一つは、以下で説明する分類と処理を構成するオブジェクトを使用することです。
別の方法は、上で説明した手法を使用してサーバーBで優先度とフェアネスを完全に無効にすることです。
サーバーBが`kube-apiserver`でない場合に最も簡単な3番目の方法は、コード内で優先度とフェアネスを無効にしてサーバーBをビルドすることです。

## コンセプト

APIの優先度とフェアネス機能にはいくつかの異なる機能が含まれています。
受信リクエストはリクエストの属性を使用して_FlowSchema_で分類され、優先度レベルに割り当てられます。
優先度レベルは個別の並行性制限を維持することで一定の分離を追加し、異なる優先度レベルに割り当てられたリクエストが互いを枯渇させないようにします。
優先度レベル内では、フェアキューイングアルゴリズムにより、異なる_フロー_のリクエストが互いを枯渇させることを防ぎ、平均負荷が許容範囲内の低さであるときにバースト的なトラフィックによってリクエストが失敗しないようにキューイングが可能です。

### 優先度レベル

APFが有効でない場合、APIサーバーの全体的な並行性は`kube-apiserver`のフラグ`--max-requests-inflight`と`--max-mutating-requests-inflight`で制限されます。
APFが有効な場合、これらのフラグで定義された並行性制限が合算され、その合計が構成可能な_優先度レベル_のセットに分配されます。
各受信リクエストは単一の優先度レベルに割り当てられ、各優先度レベルはその個別の制限が許す範囲でのみ並行リクエストをディスパッチします。

デフォルトの構成には、例えば、リーダー選出リクエスト、組み込みコントローラーからのリクエスト、Podからのリクエストのための個別の優先度レベルが含まれています。
これは、振る舞いの悪いPodがAPIサーバーにリクエストを大量送信しても、リーダー選出や組み込みコントローラーのアクションの成功を妨げることができないことを意味します。

優先度レベルの並行性制限は定期的に調整され、利用率の低い優先度レベルが一時的に並行性を利用率の高いレベルに貸し出すことができます。
これらの制限は、公称制限と、優先度レベルが貸し出せる並行性の量および借り入れできる量の上限に基づいており、すべて以下で述べる構成オブジェクトから導出されます。

### リクエストが占有するシート

上記の並行性管理の説明は基本的なストーリーです。
リクエストは異なる期間を持ちますが、優先度レベルの並行性制限と比較する際にはどの時点でも等しくカウントされます。
基本的なストーリーでは、各リクエストは1単位の並行性を占有します。
「シート」という言葉は1単位の並行性を意味するために使用され、列車や飛行機の各乗客が固定数のシートの1つを占めることに着想を得ています。

しかし、一部のリクエストは複数のシートを占有します。
これらの中には、サーバーが大量のオブジェクトを返すと推定する**list**リクエストがあります。
これらはサーバーに対して格別に重い負担をかけることがわかっています。
このため、サーバーは返されるオブジェクトの数を推定し、リクエストがその推定数に比例したシート数を占有するものとみなします。

### watchリクエストの実行時間の調整

APIの優先度とフェアネスは**watch**リクエストを管理しますが、これには基本的な動作からさらにいくつかの逸脱が含まれます。
最初の逸脱は、**watch**リクエストがどのくらいの期間シートを占有するとみなされるかに関するものです。
リクエストパラメータに応じて、**watch**リクエストへのレスポンスは、すべての関連する既存オブジェクトの**create**通知で始まる場合とそうでない場合があります。
APIの優先度とフェアネスは、その初期バーストの通知(もしあれば)が終了した時点で、**watch**リクエストがシートの使用を完了したとみなします。

通常の通知は、サーバーがオブジェクトのcreate/update/deleteを通知されるたびに、すべての関連する**watch**レスポンスストリームに対して同時バーストで送信されます。
この作業を考慮するために、APIの優先度とフェアネスはすべての書き込みリクエストが実際の書き込み完了後に追加の時間シートを占有するとみなします。
サーバーは送信される通知の数を推定し、この追加作業を含めるように書き込みリクエストのシート数とシート占有時間を調整します。

### キューイング

優先度レベル内であっても、トラフィックの発生源が多数存在する場合があります。
過負荷状態では、1つのリクエストストリームが他のストリームを枯渇させないようにすることが重要です(特に、単一のバグのあるクライアントがkube-apiserverにリクエストを大量送信するという比較的一般的なケースでは、そのバグのあるクライアントが他のクライアントに測定可能な影響を与えないことが理想的です)。
これは、同じ優先度レベルに割り当てられたリクエストを処理するためにフェアキューイングアルゴリズムを使用することで対処されます。
各リクエストは_フロー_に割り当てられ、一致するFlowSchemaの名前と_フロー識別子_（リクエスト元のユーザー、対象リソースのNamespace、またはなし）によって識別されます。
システムは同じ優先度レベルの異なるフロー内のリクエストにほぼ等しいウェイトを与えようとします。
異なるインスタンスを個別に処理できるようにするために、多数のインスタンスを持つコントローラーは異なるユーザー名で認証する必要があります。

リクエストをフローに分類した後、APIの優先度とフェアネス機能はリクエストをキューに割り当てることがあります。
この割り当てには、{{< glossary_tooltip term_id="shuffle-sharding" text="シャッフルシャーディング" >}}と呼ばれる手法が使用され、キューを比較的効率的に使用して低負荷フローを高負荷フローから分離します。

キューイングアルゴリズムの詳細は各優先度レベルごとに調整可能であり、管理者はメモリ使用量、フェアネス(合計トラフィックが容量を超えた場合に独立したフローがすべて進捗する特性)、バースト的なトラフィックへの耐性、およびキューイングによって生じる追加レイテンシーのトレードオフを行うことができます。

### 免除リクエスト

一部のリクエストは十分に重要であるとみなされ、この機能によって課されるいかなる制限の対象にもなりません。
これらの免除により、不適切に構成されたフロー制御構成がAPIサーバーを完全に無効化することを防ぎます。

## リソース

フロー制御APIには2種類のリソースが関係しています。
[PriorityLevelConfiguration](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#prioritylevelconfiguration-v1-flowcontrol-apiserver-k8s-io)は、利用可能な優先度レベル、各レベルが処理できる利用可能な並行性バジェットの割合を定義し、キューイング動作の微調整を可能にします。
[FlowSchema](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#flowschema-v1-flowcontrol-apiserver-k8s-io)は、各受信リクエストを分類し、単一のPriorityLevelConfigurationにマッチさせるために使用されます。

### PriorityLevelConfiguration

PriorityLevelConfigurationは単一の優先度レベルを表します。
各PriorityLevelConfigurationは、未処理リクエスト数の独立した制限と、キューに入れられたリクエスト数の制限を持ちます。

PriorityLevelConfigurationの公称並行性制限は、絶対的なシート数ではなく、「公称並行性シェア」で指定されます。
APIサーバーの合計並行性制限は、既存のPriorityLevelConfiguration間でこれらのシェアに比例して分配され、各レベルにシート数での公称制限が与えられます。
これにより、クラスター管理者は`--max-requests-inflight`(または`--max-mutating-requests-inflight`)に異なる値を指定して`kube-apiserver`を再起動することでサーバーへのトラフィック総量をスケールアップまたはスケールダウンでき、すべてのPriorityLevelConfigurationの許可される最大並行性が同じ割合で増加(または減少)します。

{{< caution >}}
`v1beta3`より前のバージョンでは、関連するPriorityLevelConfigurationのフィールドは「公称並行性シェア」ではなく「保証並行性シェア」と呼ばれていました。
また、Kubernetesリリース1.25以前では、定期的な調整はなく、公称/保証制限は常に調整なしで適用されていました。
{{< /caution >}}

優先度レベルが貸し出せる並行性の量と借り入れできる量の上限は、PriorityLevelConfigurationでレベルの公称制限に対するパーセンテージとして表現されます。
これらは公称制限を100.0で割って乗算し、丸めることで絶対的なシート数に解決されます。
優先度レベルの動的に調整された並行性制限は、(a)公称制限から貸出可能シート数を引いた下限と、(b)公称制限に借入可能シート数を加えた上限の間に制約されます。
各調整時に、動的制限は各優先度レベルが最近需要が出現した貸出シートを回収し、その後、上記の範囲内で優先度レベルの最近のシート需要に共同で公正に応答することで導出されます。

{{< caution >}}
優先度とフェアネス機能が有効な場合、サーバーの合計並行性制限は`--max-requests-inflight`と`--max-mutating-requests-inflight`の合計に設定されます。
変更リクエストと非変更リクエストの区別はなくなります。
特定のリソースに対してそれらを個別に扱いたい場合は、変更動詞と非変更動詞にそれぞれ一致する個別のFlowSchemaを作成してください。
{{< /caution >}}

単一のPriorityLevelConfigurationに割り当てられた受信リクエストの量が許可された並行性レベルを超えた場合、その仕様の`type`フィールドが追加のリクエストに対する処理を決定します。
`Reject`タイプは、超過トラフィックがHTTP 429(Too Many Requests)エラーで即座に拒否されることを意味します。
`Queue`タイプは、しきい値を超えるリクエストがキューに入れられ、リクエストフロー間の進捗をバランスさせるためにシャッフルシャーディングとフェアキューイング技術が使用されることを意味します。

キューイング構成により、優先度レベルごとにフェアキューイングアルゴリズムを調整できます。
アルゴリズムの詳細は[拡張提案](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness)で読むことができますが、要約すると:

* `queues`を増やすと、異なるフロー間の衝突率が減少しますが、メモリ使用量が増加します。
  値を1にすると、フェアキューイングロジックが実質的に無効になりますが、リクエストのキューイングは引き続き可能です。

* `queueLengthLimit`を増やすと、リクエストをドロップすることなく、より大きなトラフィックバーストを維持できますが、レイテンシーとメモリ使用量が増加します。

* `handSize`を変更すると、異なるフロー間の衝突確率と、過負荷状態で単一のフローが利用できる全体的な並行性を調整できます。

  {{< note >}}
  `handSize`を大きくすると、2つの個別フローが衝突する(そのため一方が他方を枯渇させる)可能性は低くなりますが、少数のフローがapiserverを支配する可能性が高くなります。
  `handSize`を大きくすると、単一の高トラフィックフローが引き起こすレイテンシーの量も潜在的に増加します。
  単一フローからのキューに入れられるリクエストの最大数は`handSize * queueLengthLimit`です。
  {{< /note >}}

以下の表は、シャッフルシャーディング構成の興味深い組み合わせを示しており、象(高負荷フロー)の例示的な数の組み合わせに対して、特定のマウス(低負荷フロー)が象に押しつぶされる確率を各構成について示しています。
この表を計算する https://play.golang.org/p/Gi0PLgVHiUg を参照してください。

{{< table caption = "シャッフルシャーディング構成の例" >}}
HandSize | Queues | 1 elephant | 4 elephants | 16 elephants
|----------|-----------|------------|----------------|--------------------| 
| 12 | 32 | 4.428838398950118e-09 | 0.11431348830099144 | 0.9935089607656024 |
| 10 | 32 | 1.550093439632541e-08 | 0.0626479840223545 | 0.9753101519027554 |
| 10 | 64 | 6.601827268370426e-12 | 0.00045571320990370776 | 0.49999929150089345 |
| 9 | 64 | 3.6310049976037345e-11 | 0.00045501212304112273 | 0.4282314876454858 |
| 8 | 64 | 2.25929199850899e-10 | 0.0004886697053040446 | 0.35935114681123076 |
| 8 | 128 | 6.994461389026097e-13 | 3.4055790161620863e-06 | 0.02746173137155063 |
| 7 | 128 | 1.0579122850901972e-11 | 6.960839379258192e-06 | 0.02406157386340147 |
| 7 | 256 | 7.597695465552631e-14 | 6.728547142019406e-08 | 0.0006709661542533682 |
| 6 | 256 | 2.7134626662687968e-12 | 2.9516464018476436e-07 | 0.0008895654642000348 |
| 6 | 512 | 4.116062922897309e-14 | 4.982983350480894e-09 | 2.26025764343413e-05 |
| 6 | 1024 | 6.337324016514285e-16 | 8.09060164312957e-11 | 4.517408062903668e-07 |
{{< /table >}}

### FlowSchema

FlowSchemaは一部の受信リクエストにマッチし、それらを優先度レベルに割り当てます。
すべての受信リクエストはFlowSchemaに対してテストされ、`matchingPrecedence`が数値的に最も低いものから順に上方に向かって処理されます。
最初にマッチしたものが優先されます。

{{< caution >}}
特定のリクエストに対して最初にマッチしたFlowSchemaのみが重要です。
複数のFlowSchemaが単一の受信リクエストにマッチした場合、`matchingPrecedence`が最も高いものに基づいて割り当てられます。
同じ`matchingPrecedence`を持つ複数のFlowSchemaが同じリクエストにマッチした場合、辞書順で`name`が小さいものが優先されますが、これに依存しないようにし、代わりに2つのFlowSchemaが同じ`matchingPrecedence`を持たないようにすることが推奨されます。
{{< /caution >}}

FlowSchemaは、その`rules`のうち少なくとも1つがマッチする場合に、特定のリクエストにマッチします。
ルールは、その`subjects`のうち少なくとも1つ*および*その`resourceRules`または`nonResourceRules`(受信リクエストがリソースURLか非リソースURLかに応じて)のうち少なくとも1つがリクエストにマッチする場合にマッチします。

subjectsの`name`フィールド、およびリソースルールと非リソースルールの`verbs`、`apiGroups`、`resources`、`namespaces`、`nonResourceURLs`フィールドでは、ワイルドカード`*`を指定して指定されたフィールドのすべての値にマッチさせることができ、そのフィールドを考慮から事実上除外します。

FlowSchemaの`distinguisherMethod.type`は、そのスキーマにマッチしたリクエストがどのようにフローに分離されるかを決定します。
`ByUser`の場合、1人のリクエストユーザーが他のユーザーの容量を枯渇させることはできません。
`ByNamespace`の場合、あるNamespaceのリソースへのリクエストが他のNamespaceのリソースへのリクエストの容量を枯渇させることはできません。
空の場合(または`distinguisherMethod`自体が省略された場合)、このFlowSchemaにマッチしたすべてのリクエストが単一のフローの一部とみなされます。
特定のFlowSchemaに対する正しい選択は、リソースとお使いの環境に依存します。

## デフォルト

各kube-apiserverは、必須と推奨の2種類のAPF構成オブジェクトを管理します。

### 必須構成オブジェクト

4つの必須構成オブジェクトは、固定された組み込みのガードレール動作を反映しています。
これはサーバーがこれらのオブジェクトが存在する前から持っている動作であり、これらのオブジェクトが存在する場合、そのspecはこの動作を反映します。
4つの必須オブジェクトは以下のとおりです。

* 必須の`exempt`優先度レベルは、フロー制御の対象外のリクエストに使用されます。
  これらは常に即座にディスパッチされます。
  必須の`exempt` FlowSchemaは、`system:masters`グループからのすべてのリクエストをこの優先度レベルに分類します。
  必要に応じて、他のリクエストをこの優先度レベルに向ける他のFlowSchemaを定義できます。

* 必須の`catch-all`優先度レベルは、必須の`catch-all` FlowSchemaと組み合わせて使用され、すべてのリクエストが何らかの分類を受けることを保証します。
  通常、このキャッチオール構成に依存すべきではなく、独自のキャッチオールFlowSchemaとPriorityLevelConfiguration(またはデフォルトでインストールされる推奨の`global-default`優先度レベル)を適切に作成すべきです。
  通常使用されることは想定されていないため、必須の`catch-all`優先度レベルは非常に小さな並行性シェアを持ち、リクエストをキューに入れません。

### 推奨構成オブジェクト

推奨されるFlowSchemaとPriorityLevelConfigurationは、合理的なデフォルト構成を構成します。
これらを変更したり、追加の構成オブジェクトを作成したりできます。
クラスターに高い負荷がかかる可能性がある場合は、どの構成が最適かを検討する必要があります。

推奨構成はリクエストを6つの優先度レベルにグループ化します。

* `node-high`優先度レベルは、ノードからのヘルスアップデート用です。

* `system`優先度レベルは、`system:nodes`グループからの非ヘルスリクエスト用です。
  つまり、Kubeletで、ワークロードがスケジュールされるためにAPIサーバーに接続できる必要があります。

* `leader-election`優先度レベルは、組み込みコントローラーからのリーダー選出リクエスト(特に、`system:kube-controller-manager`または`system:kube-scheduler`ユーザーおよび`kube-system` Namespaceのサービスアカウントからの`endpoints`、`configmaps`、`leases`へのリクエスト)用です。
  リーダー選出の失敗はコントローラーの障害と再起動を引き起こし、新しいコントローラーがインフォーマーを同期する際により高コストなトラフィックが発生するため、これらを他のトラフィックから分離することが重要です。

* `workload-high`優先度レベルは、組み込みコントローラーからのその他のリクエスト用です。

* `workload-low`優先度レベルは、その他のサービスアカウントからのリクエスト用で、通常、Pod内で実行されるコントローラーからのすべてのリクエストが含まれます。

* `global-default`優先度レベルは、その他のすべてのトラフィック(例: 非特権ユーザーが実行する対話的な`kubectl`コマンド)を処理します。

推奨されるFlowSchemaは、上記の優先度レベルにリクエストを振り分ける役割を果たし、ここでは列挙しません。

### 必須および推奨構成オブジェクトのメンテナンス

各`kube-apiserver`は、初期動作と定期的な動作を使用して、必須および推奨構成オブジェクトを独立して管理します。
そのため、異なるバージョンのサーバーが混在する状況では、異なるサーバーがこれらのオブジェクトの適切なコンテンツについて異なる意見を持つ限り、スラッシングが発生する可能性があります。

各`kube-apiserver`は、必須および推奨構成オブジェクトに対する初回メンテナンスパスを実行し、その後、これらのオブジェクトの定期的なメンテナンス(1分ごと)を行います。

必須構成オブジェクトのメンテナンスは、オブジェクトが存在し、存在する場合は適切なspecを持つことを確認することで構成されます。
サーバーは、サーバーのガードレール動作と一致しないspecでの作成または更新を拒否します。

推奨構成オブジェクトのメンテナンスは、そのspecが上書きされることを許可するように設計されています。
一方で、削除は尊重されません。メンテナンスによってオブジェクトが復元されます。
推奨構成オブジェクトが不要な場合は、オブジェクトを保持しつつ、最小限の結果となるようにspecを設定する必要があります。
推奨オブジェクトのメンテナンスは、`kube-apiserver`の新しいバージョンがロールアウトされたときの自動マイグレーションをサポートするようにも設計されていますが、サーバーの混在集団がある間はスラッシングが発生する可能性があります。

推奨構成オブジェクトのメンテナンスは、オブジェクトが存在しない場合にサーバーの推奨specで作成することで構成されます。
一方、オブジェクトがすでに存在する場合、メンテナンスの動作はkube-apiserverまたはユーザーのどちらがオブジェクトを制御しているかによって異なります。
前者の場合、サーバーはオブジェクトのspecがサーバーの推奨するものであることを確認します。
後者の場合、specはそのまま残されます。

オブジェクトを誰が制御しているかという問題は、まずキー`apf.kubernetes.io/autoupdate-spec`のアノテーションを確認することで回答されます。
そのようなアノテーションがありその値が`true`の場合、kube-apiserverがオブジェクトを制御します。
そのようなアノテーションがありその値が`false`の場合、ユーザーがオブジェクトを制御します。
どちらの条件も満たさない場合、オブジェクトの`metadata.generation`が参照されます。
それが1の場合、kube-apiserverがオブジェクトを制御します。
それ以外の場合、ユーザーがオブジェクトを制御します。
これらのルールはリリース1.22で導入され、`metadata.generation`の考慮は以前のよりシンプルな動作からの移行のためのものです。
推奨構成オブジェクトを制御したいユーザーは、`apf.kubernetes.io/autoupdate-spec`アノテーションを`false`に設定する必要があります。

必須または推奨構成オブジェクトのメンテナンスには、kube-apiserverがオブジェクトを制御しているかどうかを正確に反映する`apf.kubernetes.io/autoupdate-spec`アノテーションを確保することも含まれます。

メンテナンスには、必須でも推奨でもないが`apf.kubernetes.io/autoupdate-spec=true`のアノテーションが付けられたオブジェクトの削除も含まれます。

## ヘルスチェックの並行性免除

推奨構成は、ローカルのkubeletからkube-apiserverへのヘルスチェックリクエストに特別な扱いを与えません。
これらのリクエストはセキュアポートを使用する傾向がありますが、クレデンシャルを提供しません。
推奨構成では、これらのリクエストは`global-default` FlowSchemaと対応する`global-default`優先度レベルに割り当てられ、他のトラフィックによって押し出される可能性があります。

以下の追加FlowSchemaを追加すると、これらのリクエストがレート制限から免除されます。

{{< caution >}}
この変更を行うと、敵対的な第三者がこのFlowSchemaにマッチするヘルスチェックリクエストを好きなだけ送信できるようになります。
クラスターのAPIサーバーを一般的なインターネットトラフィックから保護するWebトラフィックフィルターまたは同様の外部セキュリティメカニズムがある場合は、クラスター外部から発信されるヘルスチェックリクエストをブロックするルールを構成できます。
{{< /caution >}}

{{% code_sample file="priority-and-fairness/health-for-strangers.yaml" %}}

## オブザーバビリティ

### メトリクス

{{< note >}}
v1.20より前のKubernetesバージョンでは、ラベル`flow_schema`と`priority_level`は、それぞれ`flowSchema`と`priorityLevel`と一貫性なく命名されていました。
Kubernetes v1.19以前を実行している場合は、お使いのバージョンのドキュメントを参照してください。
{{< /note >}}

APIの優先度とフェアネス機能を有効にすると、kube-apiserverは追加のメトリクスをエクスポートします。
これらを監視することで、構成が重要なトラフィックを不適切にスロットリングしていないかどうかを判断したり、システムの健全性を損なう可能性のある不正なワークロードを見つけたりするのに役立ちます。

#### 成熟度レベル BETA

* `apiserver_flowcontrol_rejected_requests_total`は、拒否されたリクエストのカウンターベクトル(サーバー起動からの累積)で、`flow_schema`(リクエストにマッチしたもの)、`priority_level`(リクエストが割り当てられたもの)、および`reason`のラベルで分類されます。
  `reason`ラベルは次のいずれかの値になります。

  * `queue-full`: すでにキューに入れられたリクエストが多すぎることを示します。
  * `concurrency-limit`: PriorityLevelConfigurationが超過リクエストをキューに入れるのではなく拒否するように構成されていることを示します。
  * `time-out`: リクエストのキューイング時間制限が期限切れになった時点で、リクエストがまだキュー内にあったことを示します。
  * `cancelled`: リクエストがパージロックされておらず、キューから排出されたことを示します。

* `apiserver_flowcontrol_dispatched_requests_total`は、実行を開始したリクエストのカウンターベクトル(サーバー起動からの累積)で、`flow_schema`と`priority_level`で分類されます。

* `apiserver_flowcontrol_current_inqueue_requests`は、キューに入れられている(実行されていない)リクエストの瞬時数を保持するゲージベクトルで、`priority_level`と`flow_schema`で分類されます。

* `apiserver_flowcontrol_current_executing_requests`は、実行中の(キューで待機していない)リクエストの瞬時数を保持するゲージベクトルで、`priority_level`と`flow_schema`で分類されます。

* `apiserver_flowcontrol_current_executing_seats`は、占有されているシートの瞬時数を保持するゲージベクトルで、`priority_level`と`flow_schema`で分類されます。

* `apiserver_flowcontrol_request_wait_duration_seconds`は、リクエストがキューで待機した時間のヒストグラムベクトルで、`flow_schema`、`priority_level`、および`execute`のラベルで分類されます。
  `execute`ラベルは、リクエストの実行が開始されたかどうかを示します。

  {{< note >}}
  各FlowSchemaは常にリクエストを単一のPriorityLevelConfigurationに割り当てるため、1つの優先度レベルのすべてのFlowSchemaのヒストグラムを加算することで、その優先度レベルに割り当てられたリクエストの有効なヒストグラムを取得できます。
  {{< /note >}}

* `apiserver_flowcontrol_nominal_limit_seats`は、各優先度レベルの公称並行性制限を保持するゲージベクトルで、APIサーバーの合計並行性制限と優先度レベルの構成された公称並行性シェアから計算されます。

#### 成熟度レベル ALPHA

* `apiserver_current_inqueue_requests`は、キューに入れられたリクエスト数の最近のハイウォーターマークのゲージベクトルで、値が`mutating`または`readOnly`のラベル`request_kind`でグループ化されます。
  これらのハイウォーターマークは、最近完了した1秒間のウィンドウで確認された最大数を示します。
  これらは、アクティブに処理されているリクエスト数の最後のウィンドウのハイウォーターマークを保持する古い`apiserver_current_inflight_requests`ゲージベクトルを補完します。

* `apiserver_current_inqueue_seats`は、キューに入れられたリクエストごとの最大占有シート数の合計のゲージベクトルで、`flow_schema`と`priority_level`のラベルでグループ化されます。

* `apiserver_flowcontrol_read_vs_write_current_requests`は、毎ナノ秒の終わりに行われる観測値のヒストグラムベクトルで、`phase`(`waiting`と`executing`の値を取る)と`request_kind`(`mutating`と`readOnly`の値を取る)のラベルで分類されたリクエスト数です。
  各観測値は0から1の間の比率で、リクエスト数をリクエスト数の対応する制限(待機の場合はキューボリューム制限、実行の場合は並行性制限)で割ったものです。

* `apiserver_flowcontrol_request_concurrency_in_use`は、占有されているシートの瞬時数を保持するゲージベクトルで、`priority_level`と`flow_schema`で分類されます。

* `apiserver_flowcontrol_priority_level_request_utilization`は、毎ナノ秒の終わりに行われる観測値のヒストグラムベクトルで、`phase`(`waiting`と`executing`の値を取る)と`priority_level`のラベルで分類されたリクエスト数です。
  各観測値は0から1の間の比率で、リクエスト数をリクエスト数の対応する制限(待機の場合はキューボリューム制限、実行の場合は並行性制限)で割ったものです。

* `apiserver_flowcontrol_priority_level_seat_utilization`は、毎ナノ秒の終わりに行われる観測値のヒストグラムベクトルで、`priority_level`で分類された優先度レベルの並行性制限の使用率です。
  この使用率は(占有シート数) / (並行性制限)の割合です。
  このメトリクスは、WATCHを除くすべてのリクエストの実行のすべてのステージ(通常の実行と対応する通知作業をカバーする書き込み後の追加遅延の両方)を考慮します。
  WATCHについては、既存オブジェクトの通知を配信する初期ステージのみを考慮します。
  ベクトル内の各ヒストグラムには`phase: executing`のラベルも付けられます(待機フェーズにはシート制限はありません)。

* `apiserver_flowcontrol_request_queue_length_after_enqueue`は、キューの長さのヒストグラムベクトルで、`priority_level`と`flow_schema`で分類され、キューに入れられたリクエストによってサンプリングされます。
  キューに入れられた各リクエストは、リクエストが追加された直後のキューの長さを報告する1つのサンプルをそのヒストグラムに寄与します。
  これは偏りのない調査とは異なる統計を生成することに注意してください。

  {{< note >}}
  ヒストグラムの外れ値は、単一のフロー(つまり、構成に応じて1人のユーザーまたは1つのNamespaceによるリクエスト)がAPIサーバーに大量送信してスロットリングされている可能性が高いことを意味します。
  対照的に、1つの優先度レベルのヒストグラムがその優先度レベルのすべてのキューが他の優先度レベルよりも長いことを示している場合、そのPriorityLevelConfigurationの並行性シェアを増やすことが適切かもしれません。
  {{< /note >}}

* `apiserver_flowcontrol_request_concurrency_limit`は`apiserver_flowcontrol_nominal_limit_seats`と同じです。
  優先度レベル間の並行性借入が導入される前は、これは常に`apiserver_flowcontrol_current_limit_seats`(別のメトリクスとして存在しなかった)と等しかったです。

* `apiserver_flowcontrol_lower_limit_seats`は、各優先度レベルの動的並行性制限の下限を保持するゲージベクトルです。

* `apiserver_flowcontrol_upper_limit_seats`は、各優先度レベルの動的並行性制限の上限を保持するゲージベクトルです。

* `apiserver_flowcontrol_demand_seats`は、毎ナノ秒の終わりに各優先度レベルの(シート需要) / (公称並行性制限)の比率の観測値をカウントするヒストグラムベクトルです。
  優先度レベルのシート需要は、キューに入れられたリクエストと初期実行フェーズのリクエストの両方について、リクエストの初期実行フェーズと最終実行フェーズでの占有シート数の最大値の合計です。

* `apiserver_flowcontrol_demand_seats_high_watermark`は、各優先度レベルについて、最後の並行性借入調整期間中に確認された最大シート需要を保持するゲージベクトルです。

* `apiserver_flowcontrol_demand_seats_average`は、各優先度レベルについて、最後の並行性借入調整期間中に確認された時間加重平均シート需要を保持するゲージベクトルです。

* `apiserver_flowcontrol_demand_seats_stdev`は、各優先度レベルについて、最後の並行性借入調整期間中に確認されたシート需要の時間加重母集団標準偏差を保持するゲージベクトルです。

* `apiserver_flowcontrol_demand_seats_smoothed`は、各優先度レベルについて、最後の並行性調整で決定された平滑化されたエンベロープシート需要を保持するゲージベクトルです。

* `apiserver_flowcontrol_target_seats`は、各優先度レベルについて、借入割り当て問題に入力される並行性ターゲットを保持するゲージベクトルです。

* `apiserver_flowcontrol_seat_fair_frac`は、最後の借入調整で決定されたフェア割り当て分数を保持するゲージです。

* `apiserver_flowcontrol_current_limit_seats`は、各優先度レベルについて、最後の調整で導出された動的並行性制限を保持するゲージベクトルです。

* `apiserver_flowcontrol_request_execution_seconds`は、リクエストの実際の実行時間のヒストグラムベクトルで、`flow_schema`と`priority_level`で分類されます。

* `apiserver_flowcontrol_watch_count_samples`は、特定の書き込みに関連するアクティブなWATCHリクエスト数のヒストグラムベクトルで、`flow_schema`と`priority_level`で分類されます。

* `apiserver_flowcontrol_work_estimated_seats`は、リクエストに関連する推定シート数(初期実行ステージと最終実行ステージの最大値)のヒストグラムベクトルで、`flow_schema`と`priority_level`で分類されます。

* `apiserver_flowcontrol_request_dispatch_no_accommodation_total`は、原則としてリクエストがディスパッチされる可能性があったが、利用可能な並行性の不足によりディスパッチされなかったイベントの数のカウンターベクトルで、`flow_schema`と`priority_level`で分類されます。

* `apiserver_flowcontrol_epoch_advance_total`は、数値オーバーフローを避けるために優先度レベルの進捗メーターを後方にジャンプさせる試行回数のカウンターベクトルで、`priority_level`と`success`でグループ化されます。

## APIの優先度とフェアネスの使用に関するグッドプラクティス

特定の優先度レベルが許可された並行性を超えると、リクエストのレイテンシーが増加したり、HTTP 429(Too Many Requests)エラーでドロップされたりする可能性があります。
APFのこれらの副作用を防ぐために、ワークロードを変更するか、APF設定を調整して、リクエストを処理するのに十分なシートが利用可能であることを確認できます。

APFによりリクエストが拒否されているかどうかを検出するには、以下のメトリクスを確認してください。

- apiserver_flowcontrol_rejected_requests_total: FlowSchemaとPriorityLevelConfigurationごとの拒否されたリクエストの合計数。
- apiserver_flowcontrol_current_inqueue_requests: FlowSchemaとPriorityLevelConfigurationごとのキューに入れられている現在のリクエスト数。
- apiserver_flowcontrol_request_wait_duration_seconds: キューで待機しているリクエストに追加されたレイテンシー。
- apiserver_flowcontrol_priority_level_seat_utilization: PriorityLevelConfigurationごとのシート使用率。

### ワークロードの変更 {#good-practice-workload-modifications}

APFによりリクエストがキューに入れられてレイテンシーが追加されたりドロップされたりするのを防ぐために、以下によりリクエストを最適化できます。

- リクエストの実行レートを下げる。
  一定期間でのリクエスト数が少なくなると、ある時点で必要なシート数が少なくなります。
- 大量のコストの高いリクエストを同時に発行しない。
  リクエストを最適化して使用シート数を減らしたり、レイテンシーを低くしたりすることで、これらのリクエストがシートを占有する期間を短くできます。
  リストリクエストは、リクエスト中にフェッチされるオブジェクト数に応じて1シート以上を占有する場合があります。
  リストリクエストで取得するオブジェクト数を制限すること(例えばページネーションの使用)により、より短い期間でより少ない合計シートを使用できます。
  さらに、リストリクエストをwatchリクエストに置き換えると、watchリクエストは初期バーストの通知中に1シートのみを占有するため、必要な並行性シェアの合計が少なくなります。
  バージョン1.27以降でストリーミングリストを使用する場合、watchリクエストはコレクションの全状態をストリーミングする必要があるため、初期バーストの通知についてはリストリクエストと同じシート数を占有します。
  どちらの場合も、この初期フェーズの後はwatchリクエストはシートを保持しないことに注意してください。

APFからのキューイングまたは拒否されたリクエストは、リクエスト数の増加または既存のリクエストのレイテンシーの増加のいずれかによって引き起こされる可能性があることに留意してください。
例えば、通常1秒で実行されるリクエストが60秒かかるようになると、このレイテンシーの増加によりリクエストが通常よりも長い時間シートを占有するため、APFがリクエストを拒否し始める可能性があります。
ワークロードの大幅な変更なしにAPFが複数の優先度レベルでリクエストを拒否し始めた場合、APF設定ではなくコントロールプレーンのパフォーマンスに根本的な問題がある可能性があります。

### 優先度とフェアネスの設定 {#good-practice-apf-settings}

デフォルトのFlowSchemaおよびPriorityLevelConfigurationオブジェクトを変更するか、これらのタイプの新しいオブジェクトを作成して、ワークロードに適した構成にすることもできます。

APF設定は以下のように変更できます。

- 高優先度リクエストにより多くのシートを割り当てる。
- 共有されている場合に並行性レベルを枯渇させる、重要でないまたはコストの高いリクエストを分離する。

#### 高優先度リクエストにより多くのシートを割り当てる

1. 可能であれば、特定の`kube-apiserver`のすべての優先度レベルで利用可能なシート数を、`max-requests-inflight`および`max-mutating-requests-inflight`フラグの値を増やすことで増加できます。
   あるいは、十分なリクエストの負荷分散がある場合、`kube-apiserver`インスタンスを水平スケーリングすることで、クラスター全体の優先度レベルごとの合計並行性を増やすことができます。
1. 新しいFlowSchemaを作成して、より大きな並行性レベルのPriorityLevelConfigurationを参照できます。
   この新しいPriorityLevelConfigurationは、独自の公称並行性シェアのセットを持つ既存のレベルまたは新しいレベルにすることができます。
   例えば、リクエストのPriorityLevelConfigurationをglobal-defaultからworkload-lowに変更する新しいFlowSchemaを導入して、ユーザーが利用可能なシート数を増やすことができます。
   新しいPriorityLevelConfigurationを作成すると、既存のレベルに指定されるシート数が減少します。
   デフォルトのFlowSchemaまたはPriorityLevelConfigurationを編集するには、`apf.kubernetes.io/autoupdate-spec`アノテーションを`false`に設定する必要があることを思い出してください。
1. PriorityLevelConfigurationのNominalConcurrencyShaesを増やすこともできます。
   あるいは、バージョン1.26以降では、競合する優先度レベルのLendablePercentを増やして、特定の優先度レベルが借り入れ可能なシートのプールを大きくすることもできます。

#### 重要でないリクエストを他のフローの枯渇から分離する

リクエストの分離のために、これらのリクエストを行うユーザーにマッチするsubjectを持つFlowSchemaを作成するか、リクエストの内容(resourceRulesに対応)にマッチするFlowSchemaを作成できます。
次に、このFlowSchemaを少ないシートシェアのPriorityLevelConfigurationにマッピングできます。

例えば、defaultのNamespaceで実行されているPodからのlist eventリクエストがそれぞれ10シートを使用し、1分間実行されるとします。
これらのコストの高いリクエストが、既存のservice-accounts FlowSchemaを使用する他のPodからのリクエストに影響を与えないようにするために、以下のFlowSchemaを適用してこれらのlist呼び出しを他のリクエストから分離できます。

list eventリクエストを分離するFlowSchemaオブジェクトの例:

{{% code_sample file="priority-and-fairness/list-events-default-service-account.yaml" %}}

- このFlowSchemaは、defaultのNamespaceのデフォルトサービスアカウントによるすべてのlist event呼び出しをキャプチャします。
  マッチング優先度8000は、既存のservice-accounts FlowSchemaで使用される値9000よりも低いため、これらのlist event呼び出しはservice-accountsではなくlist-events-default-service-accountにマッチします。
- catch-all PriorityLevelConfigurationは、これらのリクエストを分離するために使用されます。
  catch-all優先度レベルは非常に小さな並行性シェアを持ち、リクエストをキューに入れません。

## {{% heading "whatsnext" %}}

- トラブルシューティングの詳細については、フロー制御の[リファレンスドキュメント](/docs/reference/debug-cluster/flow-control/)を参照してください。
- APIの優先度とフェアネスの設計の詳細な背景情報については、[拡張提案](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness)を参照してください。
- [SIG API Machinery](https://github.com/kubernetes/community/tree/master/sig-api-machinery)または機能の[Slackチャンネル](https://kubernetes.slack.com/messages/api-priority-and-fairness)を通じて提案や機能リクエストを行うことができます。
