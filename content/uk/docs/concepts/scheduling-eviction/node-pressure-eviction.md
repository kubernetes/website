---
title: Виселення внаслідок тиску на вузол
content_type: concept
weight: 100
---

{{<glossary_definition term_id="node-pressure-eviction" length="short">}}</br>

The {{<glossary_tooltip term_id="kubelet" text="kubelet">}} моніторить ресурси, такі як памʼять, простір на диску та i-nodeʼи файлової системи на вузлах вашого кластера. Коли один або декілька з цих ресурсів досягають певних рівнів використання, kubelet може проактивно припинити роботу одного або кількох Podʼів на вузлі, щоб відновити ресурси та запобігти голодуванню.

Під час виселення внаслідок тиску на вузол, kubelet встановлює [фазу](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) для вибраних Podʼів в значення `Failed` та завершує роботу Podʼа.

Виселення внаслідок тиску на вузол, не є тим самим, що й [виселення, ініційоване API](/docs/concepts/scheduling-eviction/api-eviction/).

kubelet не дотримується вашого налаштованого {{<glossary_tooltip term_id="pod-disruption-budget" text="PodDisruptionBudget">}} або параметру `terminationGracePeriodSeconds` Podʼа. Якщо ви використовуєте [порогові значення мʼякого виселення](#soft-eviction-thresholds), kubelet дотримується вашого налаштованого `eviction-max-pod-grace-period`. Якщо ви використовуєте [порогові значення жорсткого виселення](#hard-eviction-thresholds), kubelet використовує граничний період `0s` (негайне завершення) для завершення роботи Podʼа.

## Самовідновлення {#self-healing-behavior}

kubelet намагається [відновлювати ресурси на рівні вузла](#reclaim-node-resources) перед тим, як він завершує роботу Podʼів кінцевих користувачів. Наприклад, він видаляє не використані образи контейнерів, коли дискових ресурсів недостатньо.

Якщо Podʼи управляються обʼєктом {{< glossary_tooltip term_id="workload" text="workload" >}} (таким як {{< glossary_tooltip term_id="statefulset" text="StatefulSet" >}} або {{< glossary_tooltip term_id="deployment" text="Deployment" >}}), який заміщує несправні Podʼи, панель управління (`kube-controller-manager`) створює нові Podʼи на місці виселених Podʼів.

### Самовідновлення статичних Podʼів {#self-healing-for-static-pods}

Якщо ви запускаєте [статичний Pod](/docs/concepts/workloads/pods/#static-pods) на вузлі, який перебуває під тиском нестачі ресурсів, kubelet може виселити цей статичний Pod. Потім kubelet намагається створити заміну, оскільки статичні Podʼи завжди представляють намір запустити Pod на цьому вузлі.

Kubelet враховує _пріоритет_ статичного Podʼа при створенні заміни. Якщо маніфест статичного Podʼа вказує низький пріоритет, і в межах панелі управління кластера визначені Podʼи з вищим пріоритетом, і вузол перебуває під тиском нестачі ресурсів, то kubelet може не мати можливості звільнити місце для цього статичного Podʼа. Kubelet продовжує намагатися запустити всі статичні Podʼи навіть тоді, коли на вузлі є тиск на ресурси.

## Сигнали та пороги виселення {#eviction-signals-and-thresholds}

Kubelet використовує різні параметри для прийняття рішень щодо виселення, такі як:

- Сигнали виселення
- Пороги виселення
- Інтервали моніторингу

### Сигнали виселення {#eviction-signals}

Сигнали виселення — це поточний стан певного ресурсу в конкретний момент часу. Kubelet використовує сигнали виселення для прийняття рішень про виселення, порівнюючи їх з порогами виселення, які є мінімальною кількістю ресурсу, яка повинна бути доступна на вузлі.

Лubelet використовує такі сигнали виселення:

| Сигнал виселення         | Опис                                                                                  | Linux Only |
|--------------------------|---------------------------------------------------------------------------------------|------------|
| `memory.available`       | `memory.available` := `node.status.capacity[memory]` - `node.stats.memory.workingSet` |            |
| `nodefs.available`       | `nodefs.available` := `node.stats.fs.available`                                       |            |
| `nodefs.inodesFree`      | `nodefs.inodesFree` := `node.stats.fs.inodesFree`                                     |      •     |
| `imagefs.available`      | `imagefs.available` := `node.stats.runtime.imagefs.available`                         |            |
| `imagefs.inodesFree`     | `imagefs.inodesFree` := `node.stats.runtime.imagefs.inodesFree`                       |      •     |
| `containerfs.available`  | `containerfs.available` := `node.stats.runtime.containerfs.available`                 |            |
| `containerfs.inodesFree` | `containerfs.inodesFree` := `node.stats.runtime.containerfs.inodesFree`               |      •     |
| `pid.available`          | `pid.available` := `node.stats.rlimit.maxpid` - `node.stats.rlimit.curproc`           |      •     |

У цій таблиці стовпець **Опис** показує, як kubelet отримує значення сигналу. Кожен сигнал підтримує відсоткове або літеральне значення. Kubelet обчислює відсоткове значення відносно загальної потужності, повʼязаної з сигналом.

#### Сигнали памʼяті {#memory-signals}

На вузлах Linux, значення для `memory.available` походить з cgroupfs замість таких інструментів, як `free -m`. Це важливо, оскільки `free -m` не працює у контейнері, і якщо користувачі використовують можливість [виділення ресурсів для вузла](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable), рішення про відсутність ресурсів
приймаються локально для Podʼа кінцевого користувача, який є частиною ієрархії cgroup, а також кореневого вузла. Цей [скрипт](/examples/admin/resource/memory-available.sh) або [скрипт cgroupv2](/examples/admin/resource/memory-available-cgroupv2.sh) відтворює той самий набір кроків, які kubelet виконує для обчислення `memory.available`. Kubelet виключає зі свого обчислення inactive_file (кількість байтів памʼяті, що резервується файлами, у списку неактивних LRU) на основі припущення, що памʼять можна вилучити під час натиску.

На Windows-вузлах значення `memory.available` отримується з глобального рівня комітів памʼяті вузла (запитується через системний виклик [`GetPerformanceInfo()`](https://learn.microsoft.com/windows/win32/api/psapi/nf-psapi-getperformanceinfo)) шляхом віднімання глобального значення [`CommitTotal`](https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information) від [`CommitLimit`](https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information) вузла. Зверніть увагу, що значення `CommitLimit` може змінюватися, якщо змінюється розмір файлу підкачки вузла!

#### Сигнали файлової системи {#filesystem-signals}

Kubelet розпізнає три конкретні ідентифікатори файлової системи, які можна використовувати зі сигналами виселення (`<identifier>.inodesFree` або `<identifier>.available`):

1. `nodefs`: Основна файлова система вузла, використовується для локальних дискових томів, томів `emptyDir`, які не підтримуються памʼяттю, зберігання логів, ефемерного зберігання та іншого. Наприклад, `nodefs` містить `/var/lib/kubelet`.

2. `imagefs`: Опціональна файлова система, яку середовище виконання контейнерів може використовувати для зберігання образів контейнерів (що є тільки для читання) і записуваних шарів контейнерів.

3. `containerfs`: Опціональна файлова система, яку середовище виконання контейнерів може використовувати для зберігання записуваних шарів. Подібно до основної файлової системи (див. `nodefs`), вона використовується для зберігання локальних дискових томів, томів `emptyDir`, які не підтримуються памʼяттю, зберігання логів і ефемерного зберігання, за винятком образів контейнерів. Коли використовується `containerfs`, файлова система `imagefs` може бути розділена так, щоб зберігати лише образи (шари тільки для читання) і більше нічого.

{{< note >}}
{{< feature-state feature_gate_name="KubeletSeparateDiskGC" >}}
Функція _split image filesystem_, яка забезпечує підтримку файлової системи `containerfs`, додає кілька нових сигналів виселення, порогів та метрик. Щоб використовувати `containerfs`, у випуску Kubernetes v{{< skew currentVersion >}} потрібно увімкнути [функціональну можливість](/docs/reference/command-line-tools-reference/feature-gates/) `KubeletSeparateDiskGC`. Наразі підтримка файлової системи `containerfs` доступна лише в CRI-O (версія 1.29 або вище).
{{</ note >}}

Таким чином, kubelet зазвичай підтримує три варіанти файлових систем для контейнерів:

- Все зберігається на єдиній файловій системі `nodefs`, також відомій як "rootfs" або просто "root", і немає окремої файлової системи для образів.

- Зберігання контейнерів (див. `nodefs`) на окремому диску, а `imagefs` (записувані та лише для читання шари) відокремлені від кореневої файлової системи. Це часто називають "split disk" (або "separate disk") файловою системою.

- Файлова система контейнерів `containerfs` (така ж, як `nodefs`, плюс записувані шари) знаходиться на root, а образи контейнерів (шари лише для читання) зберігаються на окремій `imagefs`. Це часто називають "split image" файловою системою.

Kubelet спробує автоматично виявити ці файлові системи з їхньою поточною конфігурацією безпосередньо з підсистеми контейнерів і ігноруватиме інші локальні файлові системи вузла.

Kubelet не підтримує інші файлові системи контейнерів або конфігурації зберігання і наразі не підтримує кілька файлових систем для образів і контейнерів.

### Застарілі функції збору сміття kubelet {#deprecated-kubelet-garbage-collection-features}

Деякі функції сміттєзбірника kubelet застаріли на користь процесу виселення:

| Наявний Прапорець | Обґрунтування |
| ------------- | --------- |
| `--maximum-dead-containers` | застаріло, коли старі журнали зберігаються поза контекстом контейнера |
| `--maximum-dead-containers-per-container` | застаріло, коли старі журнали зберігаються поза контекстом контейнера |
| `--minimum-container-ttl-duration` | застаріло, коли старі журнали зберігаються поза контекстом контейнера |

### Пороги виселення {#eviction-thresholds}

Ви можете вказати спеціальні пороги виселення для kubelet, які він буде використовувати під час прийняття рішень про виселення. Ви можете налаштувати [мʼякі](#soft-eviction-thresholds) та [жорсткі](#hard-eviction-thresholds) пороги виселення.

Пороги виселення мають формат `[eviction-signal][operator][quantity]`, де:

- `eviction-signal` — це [сигнал виселення](#eviction-signals), який використовується.
- `operator` — це [оператор відношення](https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators), який ви хочете використати, наприклад, `<` (менше ніж).
- `quantity` — це кількість порогу виселення, наприклад, `1Gi`. Значення `quantity` повинно відповідати представленню кількості, яке використовується в Kubernetes. Ви можете використовувати як літеральні значення, так і відсотки (`%`).

Наприклад, якщо вузол має загальний обсяг памʼяті 10ГіБ і ви хочете спровокувати виселення, якщо доступна памʼять знизиться до менше ніж 1ГіБ, ви можете визначити поріг виселення як `memory.available<10%` або `memory.available<1Gi` (використовувати обидва варіанти одночасно не можна).

#### Мʼякі пороги виселення {#soft-eviction-thresholds}

Мʼякий поріг виселення поєднує в собі поріг виселення з обовʼязковим пільговим періодом, вказаним адміністратором. Kubelet не виселяє Podʼи до закінчення пільгового періоду. Kubelet повертає помилку при запуску, якщо ви не вказали пільговий період.

Ви можете вказати як пільговий період для мʼякого порогу виселення, так і максимально дозволений пільговий період для завершення Podʼів, який kubelet використовуватиме під час виселення. Якщо ви вказали максимально дозволений пільговий період і мʼякий поріг виселення досягнутий, kubelet використовує менший із двох пільгових періодів. Якщо ви не вказали максимально дозволений пільговий період, kubelet негайно завершує роботу виселених Podʼів без очікування належного завершення їх роботи.

Ви можете використовувати наступні прапорці для налаштування мʼяких порогів виселення:

- `eviction-soft`: Набір порогів виселення, наприклад, `memory.available<1.5Gi`, які можуть спровокувати виселення Podʼів, якщо вони утримуються протягом вказаного пільгового періоду.
- `eviction-soft-grace-period`: Набір пільгових періодів для виселення, наприклад, `memory.available=1m30s`, які визначають, як довго мʼякий поріг виселення повинен утримуватися, перш ніж буде спровоковане виселення Podʼа.
- `eviction-max-pod-grace-period`: Максимально дозволений пільговий період (у секундах) для використання при завершенні Podʼів у відповідь на досягнення мʼякого порогу виселення.

#### Жорсткі пороги виселення {#hard-eviction-thresholds}

Жорсткий поріг виселення не має пільгового періоду. Коли досягнуто жорсткого порогу виселення, kubelet негайно завершує роботу Podʼів без очікування належного завершення їх роботи, щоб повернути ресурси, яких бракує.

Ви можете використовувати прапорець `eviction-hard` для налаштування набору жорстких порогів виселення, наприклад `memory.available<1Gi`.

Kubelet має наступні стандартні жорсткі пороги виселення:

- `memory.available<100Mi` (для вузлів Linux)
- `memory.available<50Mi` (для вузлів Windows)
- `nodefs.available<10%`
- `imagefs.available<15%`
- `nodefs.inodesFree<5%` (для вузлів Linux)
- `imagefs.inodesFree<5%` (для вузлів Linux)

Ці стандартні значення жорстких порогів виселення будуть встановлені лише у випадку, якщо жоден з параметрів не змінено. Якщо ви зміните значення будь-якого параметра, то значення інших параметрів не будуть успадковані як стандартні та будуть встановлені в нуль. Щоб надати власні значення, ви повинні вказати всі пороги відповідно. Ви також можете встановити значення параметру kubelet config MergeDefaultEvictionSettings рівним true у файлі конфігурації kubelet. Якщо встановити значення true і змінити будь-який параметр, то інші параметри успадкують їх стандартні значення, а не 0.

Порогові стандартні значення для `containerfs.available` і `containerfs.inodesFree` (для Linux-вузлів) будуть встановлені наступним чином:

- Якщо для всього використовується одна файлова система, то порогові значення для `containerfs` встановлюються такими ж, як і для `nodefs`.

- Якщо налаштовані окремі файлові системи для образів і контейнерів, то порогові значення для `containerfs` встановлюються такими ж, як і для `imagefs`.

Наразі налаштування власниї порогових значень, повʼязаних із `containerfs`, не підтримується, і буде видано попередження, якщо спробувати це зробити; будь-які надані власні значення будуть ігноровані.

## Інтервал моніторингу виселення {#eviction-monitoring-interval}

Kubelet оцінює пороги виселення, виходячи з налаштованого `інтервалу прибирання (housekeeping-interval)`, який типово становить `10с`.

## Стани вузла {#node-conditions}

Kubelet повідомляє про [стан вузла](/docs/concepts/architecture/nodes/#condition), показуючи, що вузол перебуває під тиском через досягнення порогу жорсткого або мʼякого виселення, незалежно від налаштованих пільгових періодів для належного завершення роботи.

Kubelet зіставляє сигнали виселення зі станами вузла наступним чином:

| Умова вузла       | Сигнал виселення                                                                      | Опис                                                                                                 |
|-------------------|---------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| `MemoryPressure`  | `memory.available`                                                                    | Доступна памʼять на вузлі досягла порогового значення для виселення                                  |
| `DiskPressure`    | `nodefs.available`, `nodefs.inodesFree`, `imagefs.available`, `imagefs.inodesFree`, `containerfs.available`, або `containerfs.inodesFree` | Доступний диск і вільні іноди на кореневій файловій системі вузла, файловій системі образів або файловій системі контейнерів досягли порогового значення для виселення |
| `PIDPressure`     | `pid.available`                                                                       | Доступні ідентифікатори процесів на (Linux) вузлі зменшилися нижче порогового значення для виселення |

Панель управління також [зіставляє](/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition) ці стани вузла у вигляді taint.

Kubelet оновлює стани вузла відповідно до налаштованої `частоти оновлення стану вузла (--node-status-update-frequency)`, яка за типово становить `10с`.

### Коливання стану вузла {#node-condition-oscillation}

У деяких випадках вузли коливаються вище та нижче мʼяких порогів виселення без затримки на визначених пільгових періодах. Це призводить до того, що звітний стан вузла постійно перемикається між `true` та `false`, що призводить до поганих рішень щодо виселення.

Для захисту від коливань можна використовувати прапорець `eviction-pressure-transition-period`, який контролює, як довго kubelet повинен чекати перед переходом стану вузла в інший стан. Період переходу має стандартне значення `5 хв`.

### Повторне використання ресурсів на рівні вузла {#reclaim-node-resources}

Kubelet намагається повторно використовувати ресурси на рівні вузла перед виселенням контейнерів кінцевих користувачів.

Коли вузол повідомляє про умову `DiskPressure`, kubelet відновлює ресурси на рівні вузла на основі файлових систем на вузлі.

#### Без `imagefs` або `containerfs` {#without-imagefs-or-containerfs}

Якщо на вузлі є лише файлова система `nodefs`, яка відповідає пороговим значенням для виселення, kubelet звільняє диск у наступному порядку:

1. Виконує збір сміття для померлих Podʼів і контейнерів.
2. Видаляє непотрібні образи.

#### З використанням `imagefs` {#with-imagefs}

Якщо на вузлі є окрема файлова система `imagefs`, яку використовують інструменти управління контейнерами, то kubelet робить наступне:

- Якщо файлова система `nodefs` відповідає пороговим значенням для виселення, то kubelet видаляє мертві Podʼи та мертві контейнери.
- Якщо файлова система `imagefs` відповідає пороговим значенням для виселення, то kubelet видаляє всі невикористані образи.

#### З `imagefs` та `containerfs` {#with-imagefs-and-containerfs}

Якщо на вузлі налаштовані окремі файлові системи `containerfs` разом із файловою системою `imagefs`, яку використовують середовища виконання контейнерів, то kubelet намагатиметься відновити ресурси наступним чином:

- Якщо файлова система `containerfs` досягає порогових значень для виселення, kubelet виконує збір сміття метрвих Podʼів і контейнерів.

- Якщо файлова система `imagefs` досягає порогових значень для виселення, kubelet видаляє всі непотрібні образи.

### Вибір Podʼів для виселення kubelet {#pod-selection-for-kubelet-eviction}

Якщо спроби kubelet відновити ресурси на рівні вузла не знижують сигнал виселення нижче порогового значення, kubelet починає виселяти Podʼи кінцевих користувачів.

Kubelet використовує наступні параметри для визначення порядку виселення Podʼів:

1. Чи перевищує використання ресурсів Podʼа їх запити
1. [Пріоритет Podʼа](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
1. Використання ресурсів Podʼа в порівнянні з запитами

В результаті kubelet ранжує та виселяє Podʼи в наступному порядку:

1. Podʼи `BestEffort` або `Burstable`, де використання перевищує запити. Ці Podʼи виселяються на основі їх пріоритету, а потім на те, наскільки їхнє використання перевищує запит.
2. Podʼи `Guaranteed` та `Burstable`, де використання менше за запити, виселяються останніми, на основі їх пріоритету.

{{<note>}}
Kubelet не використовує клас [QoS Podʼа](/docs/concepts/workloads/pods/pod-qos/) для визначення порядку виселення. Ви можете використовувати клас QoS для оцінки найбільш ймовірного порядку виселення Podʼа при відновленні ресурсів, таких як памʼять. Класифікація QoS не застосовується до запитів EphemeralStorage, тому вищезазначений сценарій не буде застосовуватися, якщо вузол, наприклад, перебуває під `DiskPressure`.
{{</note>}}

Podʼи `Guaranteed` гарантовані лише тоді, коли для всіх контейнерів вказано запити та обмеження, і вони є однаковими. Ці Podʼи ніколи не будуть виселятися через споживання ресурсів іншим Podʼом. Якщо системний демон (наприклад, `kubelet` та `journald`) споживає більше ресурсів, ніж було зарезервовано за допомогою розподілів `system-reserved` або `kube-reserved`, і на вузлі залишилися лише `Guaranteed` або `Burstable` Podʼи, які використовують менше ресурсів, ніж запити, то kubelet повинен вибрати Pod для виселення для збереження стабільності вузла та обмеження впливу нестачі ресурсів на інші Podʼи. У цьому випадку він вибере Podʼи з найнижчим пріоритетом першими.

Якщо ви використовуєте [статичний Pod](/docs/concepts/workloads/pods/#static-pods) і хочете уникнути його виселення під час нестачі речурсів, встановіть поле `priority` для цього Podʼа безпосередньо. Статичні Podʼи не підтримують поле `priorityClassName`.

Коли kubelet виселяє Podʼи у відповідь на вичерпання inode або ідентифікаторів процесів, він використовує відносний пріоритет Podʼів для визначення порядку виселення, оскільки для inode та PID не існує запитів.

Kubelet сортує Podʼи по-різному залежно від того, чи є на вузлі відведений файловий ресурс `imagefs` чи `containerfs`:

#### Без `imagefs` або `containerfs` (`nodefs` і `imagefs` використовують одну і ту ж файлову систему) {#without-imagefs}

- Якщо `nodefs` ініціює виселення, kubelet сортує Podʼи на основі їх загального використання диска (`локальні томи + логи та записуваний шар усіх контейнерів`).

#### З `imagefs` (`nodefs` і `imagefs` — окремі файлові системи) {#with-imagefs}

- Якщо `nodefs` ініціює виселення, kubelet сортує Podʼи на основі використання `nodefs` (`локальні томи + логи всіх контейнерів`).

- Якщо `imagefs` ініціює виселення, kubelet сортує Podʼи на основі використання записуваного шару всіх контейнерів.

#### З `imagefs` та `containerfs` (`imagefs` та `containerfs` розділені) {#with-containersfs}

- Якщо `containerfs` ініціює виселення, kubelet сортує Podʼи на основі використання `containerfs` (`локальні томи + логи та записуваний шар усіх контейнерів`).

- Якщо `imagefs` ініціює виселення, kubelet сортує Podʼи на основі ранжування `storage of images`, яке відображає використання диска для даного образу.

### Мінімальна кількість ресурсів для відновлення після виселення {#minimum-eviction-reclaim}

{{<note>}}
У Kubernetes v{{< skew currentVersion >}}, ви не можете встановити власне значення для метрики `containerfs.available`. Конфігурація для цієї конкретної метрики буде встановлена автоматично, щоб відображати значення, задані для `nodefs` або `imagefs`, залежно від конфігурації.
{{</note>}}

У деяких випадках виселення підтримує лише невелику кількість виділеного ресурсу. Це може призвести до того, що kubelet постійно стикається з пороговими значеннями для виселення та виконує кілька виселень.

Ви можете використовувати прапорець `--eviction-minimum-reclaim` або [файл конфігурації kubelet](/docs/tasks/administer-cluster/kubelet-config-file/) для налаштування мінімальної кількості відновлення для кожного ресурсу. Коли kubelet помічає нестачу ресурсу, він продовжує відновлення цього ресурсу до тих пір, поки не відновить кількість, яку ви вказали.

Наприклад, наступна конфігурація встановлює мінімальні кількості відновлення:

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
evictionHard:
  memory.available: "500Mi"
  nodefs.available: "1Gi"
  imagefs.available: "100Gi"
evictionMinimumReclaim:
  memory.available: "0Mi"
  nodefs.available: "500Mi"
  imagefs.available: "2Gi"
```

У цьому прикладі, якщо сигнал `nodefs.available` досягає порога виселення, kubelet відновлює ресурс до тих пір, поки сигнал не досягне порога в 1 ГіБ, а потім продовжує відновлення мінімальної кількості 500 МіБ до тих пір, поки доступне значення nodefs збереження не досягне 1,5 ГіБ.

Аналогічно, kubelet намагається відновити ресурс `imagefs` до тих пір, поки значення `imagefs.available` не досягне `102 ГіБ`, що представляє 102 ГіБ доступного сховища для образів контейнерів. Якщо кількість сховища, яку може відновити kubelet, менше за 2 ГіБ, kubelet нічого не відновлює.

Стандартно `eviction-minimum-reclaim` є `0` для всіх ресурсів.

## Поведінка в разі вичерпання памʼяті на вузлі {#node-out-of-memory-behavior}

Якщо вузол зазнає події _out of memory_ (OOM) до того, як kubelet зможе відновити памʼять, вузол залежить від реагування [oom_killer](https://lwn.net/Articles/391222/).

Kubelet встановлює значення `oom_score_adj` для кожного контейнера на основі QoS для Podʼа.

| Якість обслуговування | `oom_score_adj`                                                                   |
|--------------------|-----------------------------------------------------------------------------------|
| `Guaranteed`       | -997                                                                              |
| `BestEffort`       | 1000                                                                              |
| `Burstable`        | _мін(макс(2, 1000 - (1000 × memoryRequestBytes) / machineMemoryCapacityBytes), 999)_ |

{{<note>}}
Kubelet також встановлює значення `oom_score_adj` рівне `-997` для будь-яких контейнерів у Podʼах, які мають пріоритет `system-node-critical`.
{{</note>}}

Якщо kubelet не може відновити памʼять до того, як вузол зазнає OOM, `oom_killer` обчислює оцінку `oom_score` на основі відсотка памʼяті, яку він використовує на вузлі, а потім додає `oom_score_adj`, щоб отримати ефективну оцінку `oom_score` для кожного контейнера. Потім він примусово завершує роботу контейнера з найвищим показником.

Це означає, що спочатку припиняється робота контейнерів у Podʼах низького QoS, які споживають велику кількість памʼяті в порівнянні з їх запитами на планування.

На відміну від виселення Podʼа, якщо роботу контейнера було припинено через вичерпання памʼяті, kubelet може перезапустити його  відповідно до його `restartPolicy`.

## Поради {#node-pressure-eviction-good-practices}

У наступних розділах описано найкращі практики конфігурації виселення.

### Ресурси, доступні для планування, та політики виселення {#schedulable-resources-and-eviction-policies}

При конфігурації kubelet з політикою виселення слід переконатися, що планувальник не буде розміщувати Podʼи, які спричинять виселення через те, що вони безпосередньо викликають тиск на памʼять.

Розгляньте такий сценарій:

- Обсяг памʼяті вузла: 10 ГіБ
- Оператор бажає зарезервувати 10% обсягу памʼяті для системних служб (ядро, `kubelet`, тощо)
- Оператор бажає виселяти Podʼи при використанні памʼяті на рівні 95%, щоб зменшити випадки виснаження системи.

Для цього kubelet запускається наступним чином:

```none
--eviction-hard=memory.available<500Mi
--system-reserved=memory=1.5Gi
```

У цій конфігурації прапорець `--system-reserved` резервує 1,5 ГіБ памʼяті для системи, що становить `10% від загальної памʼяті + кількість порогу виселення`.

Вузол може досягти порогу виселення, якщо Pod використовує більше, ніж його запит, або якщо система використовує більше 1 ГіБ памʼяті, що знижує значення `memory.available` нижче 500 МіБ і спричиняє спрацювання порогу.

### DaemonSets та виселення через тиск на вузол {#daemonset}

Пріоритет Podʼа — це основний фактор при прийнятті рішень про виселення. Якщо вам не потрібно, щоб kubelet виселяв Podʼи, які належать до DaemonSet, встановіть цим Podʼам достатньо високий пріоритет, вказавши відповідний `priorityClassName` в специфікації Podʼа. Ви також можете використовувати менший пріоритет або типове значення, щоб дозволити запускати Podʼи з цього DaemonSet тільки тоді, коли є достаньо ресурсів.

## Відомі проблеми {#known-issues}

У наступних розділах описані відомі проблеми, повʼязані з обробкою вичерпання ресурсів.

### kubelet може не сприймати тиск на памʼять одразу {#kubelet-may-not-observe-memory-pressure-right-away}

Стандартно, kubelet опитує cAdvisor, щоб регулярно збирати статистику використання памʼяті. Якщо використання памʼяті збільшується протягом цього інтервалу швидко, kubelet може не помітити `MemoryPressure` настільки швидко, і буде викликаний `OOM killer`.

Ви можете використовувати прапорець `--kernel-memcg-notification`, щоб увімкнути API сповіщення `memcg` в kubelet і отримувати повідомлення негайно, коли перетинається поріг.

Якщо ви не намагаєтеся досягти екстремального використання, але не перескочити розумний рівень перевикористання, розумним обходом для цієї проблеми є використання прапорців `--kube-reserved` та `--system-reserved` для виділення памʼяті для системи.

### active_file памʼять не вважається доступною памʼяттю {#active-file-memory-is-not-considered-as-available-memory}

В Linux ядро відстежує кількість байтів файлової памʼяті в активному списку останніх використаних (LRU) як статистику `active_file`. Kubelet розглядає області памʼяті `active_file` як непіддаються вилученню. Для робочих навантажень, що інтенсивно використовують локальне сховище з блоковою підтримкою, включаючи ефемерне локальне сховище, кеші ядра файлів та блоків означає, що багато недавно доступних сторінок кеша, ймовірно, будуть враховані як `active_file`. Якщо достатньо цих буферів блоків ядра перебувають на активному списку LRU, kubelet може сприйняти це як високе використання ресурсів та позначити вузол як такий, що страждає від тиску на памʼять — спровокувавши виселення Podʼів.

Для отримання додаткової інформації дивіться [https://github.com/kubernetes/kubernetes/issues/43916](https://github.com/kubernetes/kubernetes/issues/43916).

Ви можете обійти цю проблему, встановивши для контейнерів, які ймовірно виконують інтенсивну діяльність з операцій введення/виведення, однакове обмеження памʼяті та запит памєяті. Вам потрібно оцінити або виміряти оптимальне значення обмеження памʼяті для цього контейнера.

## {{% heading "whatsnext" %}}

- Дізнайтеся про [Виселення, ініційоване API](/docs/concepts/scheduling-eviction/api-eviction/)
- Дізнайтеся про [Пріоритет та випередження Podʼів](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
- Дізнайтеся про [PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/)
- Дізнайтеся про [Якість обслуговування](/docs/tasks/configure-pod-container/quality-service-pod/) (QoS)
- Подивіться на  [Eviction API](/docs/reference/generated/kubernetes-api/{{<param "version">}}/#create-eviction-pod-v1-core)
