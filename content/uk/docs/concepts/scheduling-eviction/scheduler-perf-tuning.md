---
title: Налаштування продуктивності планувальника
content_type: concept
weight: 70
---

<!-- overview -->

{{< feature-state for_k8s_version="v1.14" state="beta" >}}

[kube-scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler) — стандартний планувальник для Kubernetes. Він відповідає за розміщення Podʼів на вузлах кластера.

Вузли кластера, які відповідають вимогам планування Podʼа, називаються _відповідними_ вузлами для Podʼа. Планувальник знаходить відповідні вузли для Podʼа, а потім виконує набір функцій для оцінки цих вузлів, вибираючи вузол з найвищим балом серед відповідних для запуску Podʼа. Планувальник потім повідомляє API-серверу про це рішення в процесі, що називається _звʼязування_.

На цій сторінці пояснюються оптимізації налаштування продуктивності, які є актуальними для великих кластерів Kubernetes.

<!-- body -->

У великих кластерах ви можете налаштувати роботу планувальника, збалансовуючи результати планування між часом відгуку (нові Podʼи розміщуються швидко) та точністю (планувальник рідко приймає погані рішення про розміщення).

Ви можете налаштувати це налаштування через параметр kube-scheduler `percentageOfNodesToScore`. Це налаштування KubeSchedulerConfiguration визначає поріг для планування вузлів у вашому кластері.

### Налаштування порога {#setting-the-threshold}

Опція `percentageOfNodesToScore` приймає цілі числові значення від 0 до 100. Значення 0 є спеціальним числом, яке позначає, що kube-scheduler повинен використовувати типовав вбудоване значення. Якщо ви встановлюєте `percentageOfNodesToScore` більше 100, kube-scheduler діє так, ніби ви встановили значення 100.

Щоб змінити значення, відредагуйте [файл конфігурації kube-scheduler](/docs/reference/config-api/kube-scheduler-config.v1/) і перезапустіть планувальник. У багатьох випадках файл конфігурації можна знайти за шляхом `/etc/kubernetes/config/kube-scheduler.yaml`.

Після внесення цих змін ви можете виконати

```bash
kubectl get pods -n kube-system | grep kube-scheduler
```

щоб перевірити, що компонент kube-scheduler працює належним чином.

## Поріг оцінки вузлів {#percentage-of-nodes-to-score}

Для поліпшення продуктивності планування kube-scheduler може припинити пошук відповідних вузлів, як тільки він знайде їх достатню кількість. У великих кластерах це заощаджує час порівняно з підходом, який би враховував кожен вузол.

Ви вказуєте поріг для того, яка кількість вузлів є достатньою, у відсотках від усіх вузлів у вашому кластері. Kube-scheduler перетворює це в ціле число вузлів. Під час планування, якщо kube-scheduler визначив достатню кількість відповідних вузлів, щоб перевищити налаштований відсоток, він припиняє пошук додаткових відповідних вузлів і переходить до [фази оцінки](/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation).

У розділі [Як планувальник проходиться по вузлах](#how-the-scheduler-iterates-over-nodes) детально описано цей процес.

### Стандартний поріг {#default-threshold}

Якщо ви не вказуєте поріг, Kubernetes розраховує значення за допомогою лінійної формули, яка дає 50% для кластера зі 100 вузлами та 10% для кластера з 5000 вузлів. Нижня межа для автоматичного значення — 5%.

Це означає, що kube-scheduler завжди оцінює принаймні 5% вашого кластера, незалежно від розміру кластера, якщо ви явно не встановили `percentageOfNodesToScore` менше ніж 5.

Якщо ви хочете, щоб планувальник оцінював всі вузли у вашому кластері, встановіть `percentageOfNodesToScore` на 100.

## Приклад {#example}

Нижче наведено приклад конфігурації, яка встановлює `percentageOfNodesToScore` на 50%.

```yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
algorithmSource:
  provider: DefaultProvider

...

percentageOfNodesToScore: 50
```

## Налаштування `percentageOfNodesToScore` {#tuning-percentageOfNodesToScore}

`percentageOfNodesToScore` повинен бути значенням від 1 до 100, а стандартне значення розраховується на основі розміру кластера. Також існує зашите мінімальне значення в 100 вузлів.

{{< note >}}
У кластерах з менш ніж 100 можливими вузлами планувальник все ще перевіряє всі вузли, тому що недостатньо можливих вузлів для того, щоб рано зупинити пошук планувальника.

У великому кластері, якщо ви встановите низьке значення для `percentageOfNodesToScore`, ваші зміни майже не матимуть ефекту, зі схожої причини.

Якщо у вашому кластері кілька сотень вузлів або менше, залиште цю опцію конфігурації у стандартному значенні. Ймовірно, внесення змін не суттєво покращить продуктивність планувальника.
{{< /note >}}

Важливою деталлю, яку слід врахувати при встановленні цього значення, є те, що при меншій кількості вузлів у кластері, які перевіряються на придатність, деякі вузли не надсилаються для оцінки для вказаного Podʼа. В результаті вузол, який можливо міг би набрати більший бал для запуску вказаного Podʼа, може навіть не надійти до фази оцінки. Це призведе до менш ідеального розміщення Podʼа.

Вам слід уникати встановлення `percentageOfNodesToScore` дуже низьким, щоб kube-scheduler не часто приймав неправильні рішення щодо розміщення Podʼа. Уникайте встановлення відсотка нижче 10%, якщо продуктивність планувальника критична для вашого застосунку та оцінка вузлів не є важливою. Іншими словами, ви віддаєте перевагу запуску Podʼа на будь-якому вузлі, який буде придатним.

## Як планувальник проходиться по вузлах {#how-the-scheduler-iterates-over-nodes}

Цей розділ призначений для тих, хто бажає зрозуміти внутрішні деталі цієї функції.

Щоб всі вузли кластера мали рівні можливості бути враховані для запуску Podʼів, планувальник проходиться по вузлах по колу. Ви можете уявити, що вузли знаходяться у масиві. Планувальник починає з початку масиву вузлів і перевіряє придатність вузлів, поки не знайде достатню кількість відповідних вузлів, як вказано у `percentageOfNodesToScore`. Для наступного Podʼа планувальник продовжує з точки в масиві вузлів, де він зупинився при перевірці придатності вузлів для попереднього Podʼа.

Якщо вузли знаходяться в кількох зонах, планувальник проходиться по вузлах у різних зонах, щоб забезпечити, що враховуються вузли з різних зон. Наприклад, розгляньте шість вузлів у двох зонах:

```none
Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
```

Планувальник оцінює придатність вузлів у такому порядку:

```none
Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
```

Після переходу усіх вузлів він повертається до Вузла 1.

## Увімкнення випадкової пакетної обробки {#enabling-opportunistic-batching}

{{< feature-state feature_gate_name="OpportunisticBatching" >}}

При плануванні великих робочих навантажень визначення подів зазвичай ідентичні та вимагають від планувальника виконання одних і тих же операцій знову і знову. Функція [Opportunistic Batching](/docs/reference/command-line-tools-reference/feature-gates/#OpportunisticBatching) дозволяє планувальнику повторно використовувати результати фільтрації та оцінювання між циклами планування, що значно прискорює процес планування.

В основному ця функція працює так:

1. Планувальник планує pod-1 і кешує результат планування.
1. Планувальник планує pod-2, 3, ... з кешованими результатами.
1. Кеш закінчується через 0,5 секунди. Планувальник планує наступний pod, який створює новий кеш.

Поди з еквівалентними обмеженнями планування повинні послідовно повертатися до циклу планування. Коли планувальник планує под з іншими обмеженнями, кеш не використовується, а замінюється новим.

Ми застосовуємо це пакетне планування до конкретних подів, які:

1. Не мають міжподової спорідненості/антиспорідненості
1. Не мають обмежень щодо розподілу топології
1. Не мають DRA (тобто не мають жодних вимог до ресурсів)
1. Плануються виключно на вузлах (тобто розміщення більше ніж одного пода на одному вузлі робить кеш недійсним)

Також, щоб увімкнути цю функцію, конфігурація планувальника повинна:

1. Вимкнути [типовий розподіл топології](/docs/concepts/scheduling-eviction/topology-spread-constraints/#internal-default-constraints) (встановити порожнє значення)
2. Вимкнути функцію [DRAExtendedResource](/docs/reference/command-line-tools-reference/feature-gates/#DRAExtendedResource).
3. Встановити `IgnorePreferredTermsOfExistingPods` [InterPodAffinityArgs](/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-InterPodAffinityArgs) на `true`, щоб зробити пакетну обробку більш ефективною

Зверніть увагу, що в таких випадках:

1. Поточні поди використовують обмеження спорідненості подів, які відповідають будь-яким міткам запланованих подів, функція може не принести користі.
1. Використовуються власні втулки, які повинні реалізовувати точку розширення Signature.

Очікується, що обмеження та умови будуть змінюватися в майбутніх версіях.

## {{% heading "whatsnext" %}}

* Перегляньте [довідку конфігурації kube-scheduler (v1)](/docs/reference/config-api/kube-scheduler-config.v1/)
