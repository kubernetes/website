---
title: Job
api_metadata:
- apiVersion: "batch/v1"
  kind: "Job"
content_type: concept
description: >-
  Job – є одноразовим завданням, що виконується до моменту його завершення.
feature:
  title: Пакетне виконання
  description: >
    На додачу до сервісів, Kubernetes може керувати пакетними та CI завданнями, замінюючи, якщо треба, контейнери, які зазнали збоїв.
weight: 50
hide_summary: true # Listed separately in section index
---

<!-- overview -->

Завдання (Job) створює один або кілька Podʼів і буде продовжувати повторювати виконання Podʼів, поки їх певна кількість успішно не завершиться. При успішному завершенні Podʼів Завдання відстежує ці успішні завершення. Коли досягнута вказана кількість успішних завершень, завдання (тобто Job) завершується. Видалення завдання буде видаляти Podʼи, які воно створило. При призупиненні завдання буде видаляти активні Podʼи, доки завдання знову не буде відновлене.

У простому випадку можна створити один обʼєкт Job для надійного запуску одного Podʼа, який працюватиме доки не завершить виконання роботи. Обʼєкт Job буде запускати новий Pod, якщо перший Pod зазнає невдачі або видаляється (наприклад, через відмову апаратного забезпечення вузла або перезавантаження вузла).

Також можна використовувати Job для запуску кількох Podʼів паралельно.

Якщо ви хочете запустити завдання (або одне завдання, або декілька паралельно) за розкладом, див. [CronJob](/docs/concepts/workloads/controllers/cron-jobs/).

<!-- body -->

## Запуск прикладу Job {#running-an-example-job}

Ось конфігурація прикладу Job. Тут обчислюється число π з точністю до 2000 знаків і виконується його вивід. Виконання зазвичай займає близько 10 секунд.

{{% code_sample file="controllers/job.yaml" %}}

Ви можете запустити цей приклад за допомогою наступної команди:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
```

Вивід буде схожим на це:

```none
job.batch/pi created
```

Перевірте статус Job за допомогою `kubectl`:

{{< tabs name="Check status of Job" >}}
{{< tab name="kubectl describe job pi" codelang="bash" >}}
Name:           pi
Namespace:      default
Selector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                batch.kubernetes.io/job-name=pi
                ...
Annotations:    batch.kubernetes.io/job-tracking: ""
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           batch.kubernetes.io/job-name=pi
  Containers:
   pi:
    Image:      perl:5.34.0
    Port:       <none>
    Host Port:  <none>
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4
  Normal  Completed         18s   job-controller  Job completed
{{< /tab >}}
{{< tab name="kubectl get job pi -o yaml" codelang="bash" >}}
apiVersion: batch/v1
kind: Job
metadata:
  annotations: batch.kubernetes.io/job-tracking: ""
             ...
  creationTimestamp: "2022-11-10T17:53:53Z"
  generation: 1
  labels:
    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
    batch.kubernetes.io/job-name: pi
  name: pi
  namespace: default
  resourceVersion: "4751"
  uid: 204fb678-040b-497f-9266-35ffa8716d14
spec:
  backoffLimit: 4
  completionMode: NonIndexed
  completions: 1
  parallelism: 1
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
  suspend: false
  template:
    metadata:
      creationTimestamp: null
      labels:
        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
        batch.kubernetes.io/job-name: pi
    spec:
      containers:
      - command:
        - perl
        - -Mbignum=bpi
        - -wle
        - print bpi(2000)
        image: perl:5.34.0
        imagePullPolicy: IfNotPresent
        name: pi
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  active: 1
  ready: 0
  startTime: "2022-11-10T17:53:57Z"
  uncountedTerminatedPods: {}
{{< /tab >}}
{{< /tabs >}}

Щоб переглянути завершені Podʼи Job, використовуйте `kubectl get pods`.

Щоб вивести всі Podʼи, які належать Job у машинночитаній формі, ви можете використовувати таку команду:

```shell
pods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')
echo $pods
```

Вивід буде схожим на це:

```none
pi-5rwd7
```

Тут, селектор збігається з селектором, який використовується в Job. Параметр `--output=jsonpath` зазначає вираз з назвою з кожного Pod зі списку.

Перегляньте стандартний вивід одного з podʼів:

```shell
kubectl logs $pods
```

Іншим варіантом є використання `kubectl logs` для виводу логів з кожного Pod.

```shell
kubectl logs job/pi
```

Вивід буде схожим на це:

```none
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
```

### Створення специфікації Job {#writing-a-job-spec}

Як і з будь-якою іншою конфігурацією Kubernetes, у Job мають бути вказані поля `apiVersion`, `kind` та `metadata`.

При створенні панеллю управління нових Podʼів для Job, поле `.metadata.name` Job є частиною основи для надання імен цим Podʼам. Імʼя Job повинно бути дійсним значенням [DNS-піддомену](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names), але це може призводити до неочікуваних результатів для імен хостів Podʼів. Для найкращої сумісності імʼя повинно відповідати більш обмеженим правилам для [DNS-мітки](/docs/concepts/overview/working-with-objects/names#dns-label-names). Навіть якщо імʼя є DNS-піддоменом, імʼя повинно бути не довше 63 символів.

Також у Job повинен бути розділ [`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).

### Мітки для Job {#job-labels}

Мітки Job повинні мати префікс `batch.kubernetes.io/` для `job-name` та `controller-uid`.

### Шаблон Podʼа {#pod-template}

`.spec.template` — єдине обовʼязкове поле `.spec`.

`.spec.template` — це [шаблон Podʼа](/docs/concepts/workloads/pods/#pod-templates). Він має точно таку ж схему, як {{< glossary_tooltip text="Pod" term_id="pod" >}}, окрім того, що він вкладений і не має `apiVersion` чи `kind`.

Окрім обовʼязкових полів для Podʼа , шаблон Podʼа в Job повинен вказати відповідні мітки (див. [селектор Podʼа](#pod-selector)) та відповідну політику перезапуску.

Дозволяється лише [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy), що дорівнює `Never` або `OnFailure`.

### Селектор Podʼа {#pod-selector}

Поле `.spec.selector` є необовʼязковим. У майже всіх випадках ви не повинні його вказувати. Дивіться розділ [вказування вашого власного селектора Podʼа](#specifying-your-own-pod-selector).

### Паралельне виконання для Jobs {#parallel-jobs}

Існують три основних типи завдань, які підходять для виконання в якості Job:

1. Непаралельні Jobs
   - зазвичай запускається лише один Pod, якщо тільки він не виходить з ладу.
   - Job завершується, як тільки робота його Podʼа завершується успішно.

1. Паралельні Jobs із *фіксованою кількістю завершень*
   - вкажіть ненульове позитивне значення для `.spec.completions`.
   - Job являє собою загальне завдання і є завершеним, коли кількість успішних Podʼів відповідає крітерію `.spec.completions`.
   - при використанні `.spec.completionMode="Indexed"`, кожен Pod отримує різний індекс у діапазоні від 0 до `.spec.completions-1`.

1. Паралельні Jobs із *чергою завдань*
   - не вказуйте `.spec.completions`, типово встановлюється `.spec.parallelism`.
   - Podʼи повинні координувати свої дії між собою або із зовнішнім сервісом, щоб визначити над чим кожен з них має працювати. Наприклад, Pod може отримати набір з N елементів з черги завдань.
   - кожен Pod може незалежно визначити, чи завершилися всі його партнери, і, таким чином, що весь Job завершений.
   - коли *будь-який* Pod з Job завершується успішно, нові Podʼи не створюються.
   - як тільки хоча б один Pod завершився успішно і всі Podʼи завершені, тоді Job завершується успішно.
   - як тільки будь-який Pod завершився успішно, жоден інший Pod не повинен виконувати роботу для цього завдання чи записувати будь-який вивід. Вони всі мають бути в процесі завершення.

Для *непаралельного* Job ви можете залишити як `.spec.completions`, так і `.spec.parallelism` не встановленими. Коли обидва не встановлені, обидва типово встановлюються на 1.

Для *Job із фіксованою кількістю завершень* вам слід встановити `.spec.completions` на кількість необхідних завершень. Ви можете встановити `.spec.parallelism` чи залишити його не встановленим, і він буде встановлено типово на 1.

Для *Job із чергою завдань* ви повинні залишити `.spec.completions` не встановленим і встановити `.spec.parallelism` на не відʼємне ціле число.

За докладною інформацією про використання різних типів Job, див. розділ [патерни Job](#job-patterns).

#### Контроль паралелізму {#controlling-parallelism}

Запитаний паралелізм (`.spec.parallelism`) може бути встановлений на будь-яке не відʼємне значення. Якщо значення не вказано, стандартно воно дорівнює 1. Якщо вказано як 0, то Job ефективно призупинено до тих пір, поки воно не буде збільшене.

Фактичний паралелізм (кількість Podʼів, які працюють в будь-який момент) може бути більше чи менше, ніж запитаний паралелізм, з різних причин:

- Для *Job із фіксованою кількістю завершень*, фактична кількість Podʼів, які працюють паралельно, не буде перевищувати кількість залишених завершень. Значення `.spec.parallelism` більше чи менше ігнорується.
- Для *Job із чергою завдань*, жоден новий Pod не запускається після успішного завершення будь-якого Podʼа — залишені Podʼи мають право завершити роботу.
- Якщо у {{< glossary_tooltip term_id="controller" text="контролера" >}} Job не було часу відреагувати.
- Якщо контролер Job не зміг створити Podʼи з будь-якої причини (нестача `ResourceQuota`, відсутність дозволу і т.д.), тоді може бути менше Podʼів, ніж запитано.
- Контролер Job може обмежувати створення нових Podʼів через надмірну кількість збоїв попередніх Podʼів у цьому ж Job.
- Коли Pod завершується належним чином, на його зупинку потрібен час.

### Режим завершення {#completion-mode}

{{< feature-state for_k8s_version="v1.24" state="stable" >}}

Для завдань з *фіксованою кількістю завершень* — тобто завдань, які мають не нульове поле `.spec.completions` — може бути встановлено режим завершення, який вказується в `.spec.completionMode`:

- `NonIndexed` (стандартно): Завдання вважається завершеним, коли є кількість успішно завершених Podʼів вкзана в `.spec.completions`. Іншими словами, завершення кожного Podʼа гомологічне одне одному. Зверніть увагу, що Завдання, у яких `.spec.completions` дорівнює null, неявно є `NonIndexed`.
- `Indexed`: Podʼи Завдання отримують асоційований індекс завершення від 0 до `.spec.completions-1`. Індекс доступний через чотири механізми:
  - Анотація Podʼа `batch.kubernetes.io/job-completion-index`.
  - Мітка Podʼа `batch.kubernetes.io/job-completion-index` (для v1.28 і новіших). Зверніть увагу, що для використання цієї мітки механізм feature gate `PodIndexLabel` повинен бути увімкнений, і він є типово увімкненим.
  - Як частина імені хоста Podʼа, за шаблоном `$(job-name)-$(index)`. Коли ви використовуєте Індексоване Завдання у поєднанні з {{< glossary_tooltip term_id="Service" >}}, Podʼи всередині Завдання можуть використовувати детерміністичні імена хостів для адресації одне одного через DNS. Докладні відомості щодо того, як налаштувати це, див. [Завдання з комунікацією від Podʼа до Podʼа](/docs/tasks/job/job-with-pod-to-pod-communication/).
  - З контейнера завдання, в змінній середовища `JOB_COMPLETION_INDEX`.

  Завдання вважається завершеним, коли є успішно завершений Pod для кожного індексу. Докладні відомості щодо того, як використовувати цей режим, див. [Індексоване завдання для паралельної обробки зі статичним призначенням роботи](/docs/tasks/job/indexed-parallel-processing-static/).

{{< note >}}
Хоча це і рідко, може бути запущено більше одного Podʼа для одного і того ж індексу (з різних причин, таких як відмови вузла, перезапуски kubelet чи виселення Podʼа). У цьому випадку лише перший успішно завершений Pod буде враховуватися при підрахунку кількості завершень та оновленні статусу завдання. Інші Podʼи, які працюють чи завершили роботу для того ж самого індексу, будуть видалені контролером завдання, як тільки він їх виявлять.
{{< /note >}}

## Обробка відмов Podʼа та контейнера {#handling-pod-and-container-failures}

Контейнер у Podʼі може вийти з ладу з ряду причин, таких як завершення процесу з ненульовим кодом виходу або примусове припинення роботи контейнера через перевищення ліміту памʼяті та таке інше. Якщо це стається і `.spec.template.spec.restartPolicy = "OnFailure"`, тоді Pod залишається на вузлі, але контейнер перезапускається. Отже, ваш застосунок повинен обробляти випадок, коли він перезапускається локально, або вказувати `.spec.template.spec.restartPolicy = "Never"`. Докладні відомості про `restartPolicy` див. в розділі [життєвий цикл Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/#example-states).

Весь Pod також може вийти з ладу з низки причин, таких як коли Pod виштовхується з  вузла (вузол оновлюється, перезавантажується, видаляється тощо), або якщо контейнер Podʼа  виходить з ладу і `.spec.template.spec.restartPolicy = "Never"`. Коли Pod виходить з ладу, контролер  завдання запускає новий Pod. Це означає, що ваш застосунок повинен обробляти випадок, коли він перезапускається у новому Podʼі. Зокрема, він повинен обробляти тимчасові файли, блокування, неповний вивід та інше, викликане попередніми запусками.

Типово кожна відмова Podʼа враховується в ліміті `.spec.backoffLimit`, див. [політика відмови Podʼа](#pod-backoff-failure-policy). Однак ви можете налаштовувати обробку відмов Podʼа, встановлюючи [політику відмови Podʼа](#pod-failure-policy) для завдання.

Додатково ви можете вирішити враховувати відмови Podʼа незалежно для кожного індексу в [Індексованому](#completion-mode) Завданні, встановлюючи поле `.spec.backoffLimitPerIndex` (докладні відомості див. [ліміт затримки на індекс](#backoff-limit-per-index)).

Зверніть увагу, що навіть якщо ви вказали `.spec.parallelism = 1` і `.spec.completions = 1` і `.spec.template.spec.restartPolicy = "Never"`, той самий програмний код може іноді бути запущений двічі.

Якщо ви вказали як `.spec.parallelism`, так і `.spec.completions` більше ніж 1, то може бути запущено кілька Podʼів одночасно. Таким чином, ваші Podʼи також повинні бути стійкими до паралелізму.

Якщо ви вказуєте поле `.spec.podFailurePolicy`, контролер Job не вважає Pod, який перебуває у стані завершення (Pod з встановленим полем `.metadata.deletionTimestamp`), за помилку до того, як Pod стане термінальним (його `.status.phase` є `Failed` або `Succeeded`). Однак контролер Job створює замінний Pod, як тільки стає очевидним, що Pod перебуває в процесі завершення. Після того як Pod завершиться, контролер Job оцінює `.backoffLimit` і `.podFailurePolicy` для відповідного Job, враховуючи тепер уже завершений Pod.

Якщо хоча б одна з цих вимог не виконана, контролер завдання рахує завершення Podʼа негайною відмовою, навіть якщо цей Pod пізніше завершить `phase: "Succeeded"`.

### Політика збою Podʼа для повторних спроб {#pod-backoff-failure-policy}

Трапляються ситуації, коли ви хочете, щоб Завдання зазнало збою після певної кількості спроб через логічну помилку в конфігурації тощо. Для цього встановіть `.spec.backoffLimit`, щоб вказати кількість повторних спроб, після яких Завдання буде вважатися невдалим.

Стандартне значення `.spec.backoffLimit` дорівнює 6, якщо не вказано [ліміт повторів на індекс](#backoff-limit-per-index) (тільки Індексовані Завдання). Якщо вказано `.spec.backoffLimitPerIndex`, то `.spec.backoffLimit` стандартно дорівнює 2147483647 (MaxInt32).

Повʼязані з Завданням несправні Podʼи відтворюються контролером Job з експоненціальною затримкою (10s, 20s, 40s …), яка обмежена шістьма хвилинами.

Кількість спроб обчислюється двома способами:

- Кількість Podʼів з `.status.phase = "Failed"`.
- При використанні `restartPolicy = "OnFailure"` кількість спроб у всіх   контейнерах Podʼів з `.status.phase`, що дорівнює `Pending` або `Running`.

Якщо хоча б один із розрахунків досягне значення `.spec.backoffLimit`, Завдання вважається невдалим.

{{< note >}}
Якщо ваше завдання має `restartPolicy = "OnFailure"`, майте на увазі, що Pod, який виконує Завдання, буде припинений, як тільки ліміт затримки завдання буде досягнуто. Це може ускладнити налагодження виконуваного Завдання. Ми радимо встановлювати `restartPolicy = "Never"` під час налагодження Завдання або використовувати систему логування, щоб забезпечити, що вивід з невдалого Завдання не буде втрачено випадково.
{{< /note >}}

### Ліміт затримки для кожного індексу {#backoff-limit-per-index}

{{< feature-state feature_gate_name="JobBackoffLimitPerIndex" >}}

Коли ви запускаєте [індексоване](#completion-mode) Завдання, ви можете вибрати обробку спроб для відмов Podʼа незалежно для кожного індексу. Для цього встановіть `.spec.backoffLimitPerIndex`, щоб вказати максимальну кількість невдалих спроб Podʼа для кожного індексу.

Коли ліміт затримки для кожного індексу перевищується для певного індексу, Kubernetes вважає цей індекс невдалим і додає його до поля `.status.failedIndexes`. Індекси, які виконались успішно, реєструються в полі `.status.completedIndexes`, незалежно від того, чи ви встановили поле `backoffLimitPerIndex`.

Зауважте, що невдалий індекс не перериває виконання інших індексів. Щойно всі індекси завершаться для завдання, в якому ви вказали ліміт затримки для кожного індексу, якщо хоча б один з цих індексів виявився невдалим, контролер завдань позначає загальне завдання як невдале, встановлюючи умову `Failed` в статусі. Завдання отримує позначку "невдало", навіть якщо деякі, можливо, усі індекси були
оброблені успішно.

Ви також можете обмежити максимальну кількість позначених невдалих індексів, встановивши поле `.spec.maxFailedIndexes`. Коли кількість невдалих індексів перевищує значення поля `maxFailedIndexes`, контролер Job запускає завершення всіх Podʼів, що залишаються запущеними в цьому Завданні. Щойно всі Podʼи будуть завершені, контролер Job позначає все Завдання як невдале, встановлюючи умову `Failed` в статусі Завдання.

Ось приклад маніфесту для Завдання, яке визначає `backoffLimitPerIndex`:

{{< code_sample file="/controllers/job-backoff-limit-per-index-example.yaml" >}}

У вищенаведеному прикладі контролер Job дозволяє один перезапуск для кожного з індексів. Коли загальна кількість невдалих індексів перевищує 5, тоді все Завдання припиняється.

Після завершення роботи стан Завдання виглядає наступним чином:

```sh
kubectl get -o yaml job job-backoff-limit-per-index-example
```

```yaml
  status:
    completedIndexes: 1,3,5,7,9
    failedIndexes: 0,2,4,6,8
    succeeded: 5          # 1 succeeded pod for each of 5 succeeded indexes
    failed: 10            # 2 failed pods (1 retry) for each of 5 failed indexes
    conditions:
    - message: Job has failed indexes
      reason: FailedIndexes
      status: "True"
      type: FailureTarget
    - message: Job has failed indexes
      reason: FailedIndexes
      status: "True"
      type: Failed
```

Контролер Job додає умову `FailureTarget` до Job для запуску [завершення та очищення Job](#job-termination-and-cleanup). Коли всі Podʼи Job завершуються, контролер Job додає умову `Failed` з тими ж значеннями для `reason` і `message`, що й у умови `FailureTarget`. Для деталей дивіться [Завершення Podʼів Завдання](#termination-of-job-pods).

Додатково ви можете використовувати ліміт затримки для кожного індексу разом з [політикою збоїв Podʼа](#pod-failure-policy). Коли використовується ліміт затримки для кожного індексу, доступний новій дії `FailIndex`, який дозволяє вам уникати непотрібних повторів всередині індексу.

### Політика збою Podʼа {#pod-failure-policy}

{{< feature-state feature_gate_name="JobPodFailurePolicy" >}}

Політика збою Podʼа, визначена за допомогою поля `.spec.podFailurePolicy`, дозволяє вашому кластеру обробляти відмови Podʼа на основі кодів виходу контейнера та умов Podʼа.

У деяких ситуаціях вам може знадобитися кращий контроль обробки відмов Podʼа, ніж контроль, який надає [Політика backoff відмови Podʼа](#pod-backoff-failure-policy), яка базується на `.spec.backoffLimit` Завдання. Ось деякі приклади використання:

- Для оптимізації витрат на виконання робочих навантажень, уникнення непотрібних перезапусків Podʼа, ви можете завершити Завдання, як тільки один із його Podʼів відмовить із кодом виходу, що вказує на помилку програмного забезпечення.
- Щоб гарантувати, що ваше завдання завершиться, навіть якщо є розлади, ви можете ігнорувати відмови Podʼа, спричинені розладами (такими як {{< glossary_tooltip text="випередження" term_id="preemption" >}}, {{< glossary_tooltip text="виселення, ініційоване API" term_id="api-eviction" >}} або виселення на підставі {{< glossary_tooltip text="маркування" term_id="taint" >}}), щоб вони не враховувалися при досягненні `.spec.backoffLimit` ліміту спроб.

Ви можете налаштувати політику збою Podʼа в полі `.spec.podFailurePolicy`, щоб відповідати вищенаведеним використанням. Ця політика може обробляти відмови Podʼа на основі кодів виходу контейнера та умов Podʼа.

Ось маніфест для Завдання, яке визначає `podFailurePolicy`:

{{% code_sample file="/controllers/job-pod-failure-policy-example.yaml" %}}

У вищенаведеному прикладі перше правило політики збою Podʼа вказує, що Завдання слід позначити як невдале, якщо контейнер `main` завершиться кодом виходу 42. Наступні правила стосуються саме контейнера `main`:

- код виходу 0 означає, що контейнер виконався успішно
- код виходу 42 означає, що **все Завдання** виконалося невдало
- будь-який інший код виходу вказує, що контейнер виконався невдало, і, отже, весь Pod буде створений заново, якщо загальна кількість перезапусків менше `backoffLimit`. Якщо `backoffLimit` досягнуте, **все Завдання** виконалося невдало.

{{< note >}}
Оскільки в шаблоні Podʼа вказано `restartPolicy: Never`, kubelet не перезапускає контейнер `main` в тому конкретному Podʼі.
{{< /note >}}

Друге правило політики збою Podʼа, що вказує дію `Ignore` для невдалих Podʼів з умовою `DisruptionTarget`, виключає Podʼи, які взяли участь в розладах, з розрахунку ліміту спроб `.spec.backoffLimit`.

{{< note >}}
Якщо Завдання виявилося невдалим, будь-то через політику збою Podʼа, чи через політику затримки збою Podʼа, і завдання виконується декількома Podʼами, Kubernetes завершує всі Podʼи в цьому Завданні, які все ще перебувають у статусах Pending або Running.
{{< /note >}}

Ось деякі вимоги та семантика API:

- якщо ви хочете використовувати поле `.spec.podFailurePolicy` для Job, вам також слід визначити шаблон podʼа цього Завдання з `.spec.restartPolicy`, встановленим на `Never`.
- правила політики збою Podʼа, які ви визначаєте у `spec.podFailurePolicy.rules`, оцінюються послідовно. Якщо одне з правил відповідає збою Podʼа, інші правила ігноруються. Якщо жодне правило не відповідає збою Podʼа, застосовується типова обробка.
- ви можете бажати обмежити правило певним контейнером, вказавши його імʼя у `spec.podFailurePolicy.rules[*].onExitCodes.containerName`. Якщо не вказано, правило застосовується до всіх контейнерів. Якщо вказано, воно повинно відповідати імені одного з контейнерів або `initContainer` у шаблоні Podʼа.
- ви можете вказати дію, яка виконується, коли політика відмов Podʼа отримує збіг, у `spec.podFailurePolicy.rules[*].action`. Можливі значення:
  - `FailJob`: використовуйте це, щоб вказати, що завдання Podʼа має бути позначене як Failed та всі запущені Podʼи повинні бути припинені.
  - `Ignore`: використовуйте це, щоб вказати, що значення лічильника, що рахує `.spec.backoffLimit`, не повине збільшуватися, і має бути створений Pod-заміна.
  - `Count`: використовуйте це, щоб вказати, що Pod повинен бути оброблений типовим способом. Лічильник `.spec.backoffLimit` повинен збільшитися.
  - `FailIndex`: використовуйте цю дію разом із [лімітом backoff на кожен індекс](#backoff-limit-per-index) для уникнення непотрібних повторних спроб в межах індексу невдалого Podʼа.

{{< note >}}
Коли використовується `podFailurePolicy`, контролер завдань враховує лише Podʼи у фазі `Failed`. Podʼи з відміткою про видалення, які не знаходяться в термінальній фазі (`Failed` чи `Succeeded`), вважаються таким що примусово припиняють свою роботу. Це означає, що такі Podʼи зберігають [завершувачі відстеження](#job-tracking-with-finalizers) доки не досягнуть термінальної фази. З Kubernetes 1.27 Kubelet переводить видалені Podʼи в термінальну фазу (див.: [Фаза Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)). Це забезпечує видалення завершувачів контролером Job.
{{< /note >}}

{{< note >}}
Починаючи з Kubernetes v1.28, коли використовується політика збою Podʼа, контролер Job відтворює Podʼи, що припиняються примусово, лише тоді, коли ці Podʼи досягли термінальної фази `Failed`. Це подібно до політики заміщення Podʼа: `podReplacementPolicy: Failed`. Для отримання додаткової інформації див. [Політика заміщення Podʼа](#pod-replacement-policy).
{{< /note >}}

Коли ви використовуєте `podFailurePolicy`, і Job зазнає невдачі через Pod, що відповідає правилу з дією `FailJob`, контролер Job запускає процес завершення Job, додаючи умову `FailureTarget`. Для деталей дивіться [Завершення та очищення Job](#job-termination-and-cleanup).

## Політика успіху {#success-policy}

При створенні Індексованого Завдання ви можете визначити, коли Завдання може бути визнане успішним за допомогою `.spec.successPolicy`, на основі успішних Podʼів.

Типово Завдання вважається успішним, коли кількість успішних Podʼів дорівнює `.spec.completions`. Існують ситуації, коли вам може знадобитися додатковий контроль для визначення Завдання успішним:

- Під час виконання симуляцій з різними параметрами, вам можуть не знадобитися всі симуляції для успішного завершення загального Завдання.
- При використанні шаблону лідер-виконавець, успіх лідера визначає успіх чи невдачу Завдання. Прикладами цього є фреймворки, такі як MPI та PyTorch тощо.

Ви можете налаштувати політику успіху у полі `.spec.successPolicy`, щоб задовольнити вищезазначені випадки використання. Ця політика може керувати успіхом Завдання на основі успішних Podʼів. Після того, як Завдання відповідає політиці успіху, контролер завдань припиняє Podʼи, робота яких затягнулась. Політика успіху визначається правилами. Кожне правило може мати одну з наступних форм:

- Коли ви вказуєте лише `succeededIndexes`, одразу після успішного завершення всіх індексів, вказаних у `succeededIndexes`, контролер завдань позначає завдання як успішне. `succeededIndexes` повинен бути списком інтервалів між 0 і `.spec.completions-1`.
- Коли ви вказуєте лише `succeededCount`, як тільки кількість успішних індексів досягне `succeededCount`, контролер завдань позначає завдання як успішне.
- Коли ви вказуєте як `succeededIndexes`, так і `succeededCount`, як тільки кількість успішних індексів з підмножини індексів, вказаних у `succeededIndexes`, досягне `succeededCount`, контролер завдань позначає завдання як успішне.

Зверніть увагу, що коли ви вказуєте кілька правил у `.spec.successPolicy.rules`, контролер завдань оцінює правила послідовно. Після того, як завдання задовольняє правило, контролер завдань ігнорує решту правил.

Ось приклад маніфеста для Завдання із `successPolicy`:

{{% code_sample file="/controllers/job-success-policy.yaml" %}}

У вищенаведеному прикладі були вказані як `succeededIndexes`, так і `succeededCount`. Тому контролер завдань позначить завдання як успішне і завершить залишкові Podʼи після успіху будь-яких зазначених індексів, 0, 2 або 3. Завдання, яке відповідає політиці успіху, отримує умову `SuccessCriteriaMet` з причиною `SuccessPolicy`. Після видалення залишкових Podʼів, завдання отримує стан `Complete`.

Зверніть увагу, що `succeededIndexes` представлено як інтервали, розділені дефісом. Номери перераховані у вигляді першого та останнього елементів серії, розділених дефісом.

{{< note >}}
Коли ви вказуєте як політику успіху, так і деякі політики завершення, такі як `.spec.backoffLimit` і `.spec.podFailurePolicy`, одного разу, коли завдання задовольняє будь-яку політику, контролер завдань дотримується політики завершення і ігнорує політику успіху.
{{< /note >}}

## Завершення Job та очищення {#job-termination-and-cleanup}

Коли Завдання завершується, Podʼи більше не створюються, але Podʼи [зазвичай](#pod-backoff-failure-policy) також не видаляються. Тримання їх в наявності допомагає переглядати логи завершених Podʼів для перевірки наявності помилок, попереджень чи іншого діагностичного виводу. Обʼєкт завдання також залишається після завершення, щоб ви могли переглядати його статус. Користувач може видаляти старі завдання, попередньо переглянувши їхній статус. Видаліть завдання за допомогою `kubectl` (наприклад, `kubectl delete jobs/pi` або `kubectl delete -f ./job.yaml`). Коли ви видаляєте завдання за допомогою `kubectl`, всі його створені Podʼи також видаляються.

Стандартно Завдання буде виконуватися безперервно, якщо тільки Pod не вийде з ладу (`restartPolicy=Never`) або контейнер не вийде з ладу з помилкою (`restartPolicy=OnFailure`), після чого Завдання дотримується `.spec.backoffLimit`, описаного вище. Як тільки досягнуто `.spec.backoffLimit`, Завдання буде позначене як невдале, і будь-які запущені Podʼи будуть завершені.

Іншим способом завершити Завдання є встановлення граничного терміну виконання. Це робиться шляхом встановлення в поле `.spec.activeDeadlineSeconds` Завдання кількості секунд. `activeDeadlineSeconds` застосовується до тривалості завдання, незалежно від кількості створених Podʼів. Як тільки Job досягає `activeDeadlineSeconds`, всі його запущені Podʼи завершуються, і статус завдання стане `type: Failed` з `reason: DeadlineExceeded`.

Зауважте, що `.spec.activeDeadlineSeconds` Job має пріоритет перед його `.spec.backoffLimit`. Отже, Завдання, яке повторно запускає один або кілька невдалих Podʼів, не розпочне розгортання додаткових Podʼів, якщо `activeDeadlineSeconds` вже досягнуто, навіть якщо `backoffLimit` не досягнуто.  Приклад:

```yaml
apiVersion: batch/v1 kind: Job
metadata:
  name: pi-with-timeout
Стандартноimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

Зверніть увагу, що як саме специфікація Job, так і [специфікація шаблону Pod](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) у межах завдання мають поле `activeDeadlineSeconds`. Переконайтеся, що ви встановлюєте це поле на відповідному рівні.

Памʼятайте, що `restartPolicy` застосовується до Podʼа, а не до самого Завдання: автоматичного перезапуску Завдання не відбудеться, якщо статус завдання `type: Failed`. Іншими словами, механізми завершення Завдання, активовані за допомогою `.spec.activeDeadlineSeconds` і `.spec.backoffLimit`, призводять до постійних збоїв Завдання, що вимагає ручного втручання для вирішення.

### Термінальні умови Завдання {#terminal-job-conditions}

Job має два можливих термінальних стани, кожен з яких має відповідну умову завдання:

- Успішне виконання: умова Завдання `Complete`
- Невдача: умова Завдання `Failed`

Завдання зазнають невдачі з наступних причин:

- Кількість невдач Podʼа перевищила вказане `.spec.backoffLimit` у специфікації Job. Для деталей див. [Політика відмови Podʼа](#pod-backoff-failure-policy).
- Час виконання Завдання перевищив вказане `.spec.activeDeadlineSeconds`.
- Індексоване Завдання, яке використовує `.spec.backoffLimitPerIndex`, має збійні індекси. Для деталей див. [Ліміт backoff на індекс](#backoff-limit-per-index).
- Кількість невдалих індексів у Завданні перевищила вказане значення `spec.maxFailedIndexes`. Для деталей див. [Ліміт backoff на індекс](#backoff-limit-per-index).
- Невдалий Pod відповідає правилу у `.spec.podFailurePolicy`, яке має дію `FailJob`. Для деталей про те, як правила політики невдач Podʼа можуть вплинути на оцінку невдачі, див. [Політика збою Podʼа](#pod-failure-policy).

Завдання успішні з наступних причин:

- Кількість успішних Podʼів досягла вказаного `.spec.completions`.
- Виконані критерії, зазначені у `.spec.successPolicy`. Для деталей див. [Політика успіху](#success-policy).

У Kubernetes v1.31 та пізніших версіях контролер Job затримує додавання термінальних умов, `Failed` або `Complete`, поки всі Podʼи Завдання не будуть завершені.

У Kubernetes v1.30 та раніших версіях контролер Job додавав термінальні умови `Complete` або `Failed` відразу після того, як був запущений процес завершення Завдання та всі завершувачі Podʼа були видалені. Однак деякі Podʼи все ще могли працювати або завершуватися в момент додавання термінальної умови.

У Kubernetes v1.31 та пізніших версіях контролер додає термінальні умови Завдання лише *після* завершення всіх Podʼів. Ви можете увімкнути цю поведінку, використовуючи [функціональні можливості](/docs/reference/command-line-tools-reference/feature-gates/) `JobManagedBy` та `JobPodReplacementPolicy` (обидві стандартно увімкнені).

### Завершення podʼів Завдання {#termination-of-job-pods}

Контролер Job додає умову `FailureTarget` або умову `SuccessCriteriaMet` до Завдання, щоб запустити завершення Podʼа після того, як завдання відповідає критеріям успіху або невдачі.

Фактори, такі як `terminationGracePeriodSeconds`, можуть збільшити час від моменту додавання контролером Job умови `FailureTarget` або умови `SuccessCriteriaMet` до моменту, коли всі Podʼи Завдання завершаться і контролер Job додасть [термінальну умову](#terminal-job-conditions) (`Failed` або `Complete`).

Ви можете використовувати умову `FailureTarget` або умову `SuccessCriteriaMet`, щоб оцінити, чи Завдання зазнало невдачі чи досягло успіху, без необхідності чекати, поки контролер додасть термінальну умову.

Наприклад, ви можете захотіти вирішити, коли створити замінний Job, що замінює невдалий. Якщо ви заміните невдале Завдання, коли з’являється умова `FailureTarget`, ваше замінне Завдання запуститься раніше, але це може призвести до того, що Podʼи з невдалого та замінного Завдання будуть працювати одночасно, використовуючи додаткові обчислювальні ресурси.

Альтернативно, якщо ваш кластер має обмежену ресурсну ємність, ви можете вибрати чекати, поки з’явиться умова `Failed` на завданні, що затримає ваше замінне Завдання, але забезпечить збереження ресурсів, чекаючи, поки всі невдалі Podʼи не будуть видалені.

## Автоматичне очищення завершених завдань {#clean-up-finished-jobs-automatically}

Зазвичай завершені Завданя вже не потрібні в системі. Зберігання їх в системі може створювати тиск на сервер API. Якщо Завдання керується безпосередньо контролером вищого рівня, таким як [CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), Завдання можна очищати за допомогою CronJobs на основі визначеної політики очищення з урахуванням місткості.

### Механізм TTL для завершених Завдань {#ttl-mechanism-for-finished-jobs}

{{< feature-state for_k8s_version="v1.23" state="stable" >}}

Ще один спосіб автоматично очищати завершені завдання (якщо вони `Complete` або `Failed`) — це використовувати механізм TTL, наданий
[контролером TTL](/docs/concepts/workloads/controllers/ttlafterfinished/) для завершених ресурсів, вказуючи поле `.spec.ttlSecondsAfterFinished` у Job.

Коли контролер TTL очищує Job, він каскадно видаляє Job, тобто видаляє його залежні обʼєкти, такі як Podʼи, разом з Job. Зверніть увагу що при видаленні Job будуть дотримані гарантії його життєвого циклу, такі як завершувачі.

Наприклад:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

Job `pi-with-ttl` буде призначено для автоматичного видалення через `100` секунд після завершення.

Якщо поле встановлено в `0`, Job буде призначено для автоматичного видалення негайно після завершення. Якщо поле не вказане, Job не буде очищено контролером TTL після завершення.

{{< note >}}
Рекомендується встановлювати поле `ttlSecondsAfterFinished`, оскільки завдання без нагляду (завдання, які ви створили безпосередньо, а не опосередковано через інші API робочого навантаження такі як CronJob) мають станадартну політику видалення `orphanDependents`, що призводить до того, що Podʼи, створені некерованим Job, залишаються після того, як Job повністю видалено. Навіть якщо {{< glossary_tooltip text="панель управління" term_id="control-plane" >}} в кінцевому рахунку [збирає сміття](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection) Podʼів з видаленого Job після того, як вони або не вдались, або завершились, іноді ці залишки Podʼів можуть призводити до погіршення продуктивності кластера або в найгіршому випадку можуть спричинити виходу кластера з ладу через цю деградацію.

Ви можете використовувати [LimitRanges](/docs/concepts/policy/limit-range/) та [ResourceQuotas](/docs/concepts/policy/resource-quotas/) щоб обмежити кількість ресурсів, які певний простір імен може споживати.
{{< /note >}}

## Патерни використання завдань (Job) {#job-patterns}

Обʼєкт Завдання (Job) може використовуватися для обробки набору незалежних, але повʼязаних *робочих одиниць*. Це можуть бути електронні листи для надсилання, кадри для рендерінгу, файли для перекодування, діапазони ключів у базі даних NoSQL для сканування і так далі.

У складній системі може бути кілька різних наборів робочих одиниць. Тут ми розглядаємо лише один набір робочих одиниць, якими користувач хоче управляти разом — *пакетне завдання*.

Існує кілька різних патернів для паралельних обчислень, кожен з власними перевагами та недоліками. Компроміси:

- Один обʼєкт Job для кожної робочої одиниці або один обʼєкт Job для всіх робочих одиниць. Один Job на кожну робочу одиницю створює деяку накладну роботу для користувача та системи при управлінні великою кількістю обʼєктів Job. Один Job для всіх робочих одиниць підходить краще для великої кількості одиниць.
- Кількість створених Podʼів дорівнює кількості робочих одиниць або кожен Pod може обробляти кілька робочих одиниць. Коли кількість Podʼів дорівнює кількості робочих одиниць, Podʼи зазвичай вимагають менше змін у наявному коді та контейнерах. Кожен Pod, що обробляє кілька робочих одиниць, підходить краще для великої кількості одиниць.
- Декілька підходів використовують чергу робіт. Це вимагає запуску служби черги та модифікацій існуючої програми чи контейнера, щоб зробити його сумісним із чергою робіт. Інші підходи легше адаптувати до наявного контейнеризованого застосунку.
- Коли Job повʼязаний із [headless Service](/docs/concepts/services-networking/service/#headless-services), ви можете дозволити Podʼам у межах Job спілкуватися один з одним для спільних обчислень.

Переваги та недоліки узагальнено у таблиці нижче, де стовпці з 2 по 4 відповідають зазначеним вище питанням. Імена патернів також є посиланнями на приклади та більш детальний опис.

| Патерн                                        | Один обʼєкт Job | Кількість Podʼів менша за робочі одиниці? | Використовувати застосунок без модифікацій? |
| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|
| [Черга, Pod на одиницю роботи]                 |         ✓         |                             |      іноді           |
| [Черга змінної кількості Podʼів]                   |         ✓         |             ✓               |                     |
| [Індексоване Завдання із статичним призначенням роботи]       |         ✓         |                                          |          ✓          |
| [Завдання із спілкуванням від Pod до Pod]             |         ✓         |         іноді              |      іноді           |
| [Розширення шаблону Job]                        |                   |                             |          ✓          |

Коли ви вказуєте завершення з `.spec.completions`, кожний Pod, створений контролером Job, має ідентичний [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status). Це означає, що всі Podʼи для завдання матимуть однакову командну строку та один і той же шаблон, та (майже) ті ж самі змінні середовища. Ці патерни — це різні способи організації Podʼів для роботи над різними завданнями.

Ця таблиця показує обовʼязкові налаштування для `.spec.parallelism` та `.spec.completions` для кожного з патернів. Тут `W` — це кількість робочих одиниць.

|             Патерн                             | `.spec.completions` |  `.spec.parallelism` |
| ----------------------------------------------- |:-------------------:|:--------------------:|
| [Черга, Pod на одиницю роботи]                 |          W          |        будь-яка        |
| [Черга змінної кількості Podʼів]                   |         null        |        будь-яка        |
| [Індексоване Завдання із статичним призначенням роботи]       |          W          |        будь-яка        |
| [Завдання із спілкуванням від Pod до Pod]             |          W          |         W            |
| [Розширення шаблону Job]                        |          1          |     повинно бути 1   |

[Черга, Pod на одиницю роботи]: /docs/tasks/job/coarse-parallel-processing-work-queue/
[Черга змінної кількості Podʼів]: /docs/tasks/job/fine-parallel-processing-work-queue/
[Індексоване Завдання із статичним призначенням роботи]: /docs/tasks/job/indexed-parallel-processing-static/
[Завдання із спілкуванням від Pod до Pod]: /docs/tasks/job/job-with-pod-to-pod-communication/
[Розширення шаблону Job]: /docs/tasks/job/parallel-processing-expansion/

## Розширене використання завдань (Job) {#advanced-usage}

### Призупинення Завдання {#suspending-a-job}

{{< feature-state for_k8s_version="v1.24" state="stable" >}}

Коли створюється Job, контролер Job негайно починає створювати Podʼи, щоб відповісти вимогам Job і продовжує це робити, доки Job не завершиться. Однак іноді ви можете хотіти тимчасово призупинити виконання Job та відновити його пізніше, або створити Завдання у призупиненому стані та мати власний контролер, який вирішить, коли їх запустити.

Щоб призупинити Job, ви можете оновити поле `.spec.suspend` Job на значення true; пізніше, коли ви захочете відновити його, оновіть його на значення false. Створення Job з `.spec.suspend` встановленим в true створить його в призупиненому стані.

При відновленні Job з призупинення йому буде встановлено час початку `.status.startTime` в поточний час. Це означає, що таймер `.spec.activeDeadlineSeconds` буде зупинений і перезапущений, коли Job буде призупинено та відновлено.

Коли ви призупиняєте Job, будь-які запущені Podʼи, які не мають статусу `Completed`, будуть [завершені](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) з сигналом SIGTERM. Буде дотримано період належного завершення ваших Podʼів, і вашим Podʼам слід обробити цей сигнал протягом цього періоду. Це може включати в себе збереження прогресу на майбутнє або скасування змін. Podʼи, завершені цим чином, не враховуватимуться при підрахунку `completions` Job.

Приклад визначення Job в призупиненому стані може виглядати так:

```shell
kubectl get job myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  suspend: true
  parallelism: 1
  completions: 5
  template:
    spec:
      ...
```

Ви також можете перемикати призупинення Job, використовуючи командний рядок.

Призупиніть активний Job:

```shell
kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":true}}'
```

Відновіть призупинений Job:

```shell
kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":false}}'
```

Статус Job може бути використаний для визначення того, чи Job призупинено чи його було зупинено раніше:

```shell
kubectl get jobs/myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
# .metadata and .spec пропущено
status:
  conditions:
  - lastProbeTime: "2021-02-05T13:14:33Z"
    lastTransitionTime: "2021-02-05T13:14:33Z"
    status: "True"
    type: Suspended
  startTime: "2021-02-05T13:13:48Z"
```

Умова Job типу "Suspended" зі статусом "True" означає, що Job призупинено; поле `lastTransitionTime` може бути використане для визначення того, як довго Job було призупинено. Якщо статус цієї умови є "False", то Job було раніше призупинено і зараз він працює. Якщо такої умови не існує в статусі Job, то Job ніколи не був зупинений.

Також створюються події, коли Job призупинено та відновлено:

```shell
kubectl describe jobs/myjob
```

```none
Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
```

Останні чотири події, зокрема події "Suspended" та "Resumed", є прямим наслідком перемикання поля `.spec.suspend`. Протягом цього часу ми бачимо, що жоден Pod не був створений, але створення Podʼів розпочалося знову, як тільки Job було відновлено.

### Змінні директиви планування {#mutable-scheduling-directives}

{{< feature-state for_k8s_version="v1.27" state="stable" >}}

У більшості випадків, паралелльні завдання вимагатимуть щоб їхні Podʼи запускались з певними обмеженнями, типу "всі в одній зоні" або "всі на GPU моделі x або y", але не комбінація обох.

[Поле призупинення](#suspending-a-job) є першим кроком для досягнення цих семантик. Призупинення дозволяє власному контролеру черги вирішити, коли повинне початися завдання; Однак після того, як завдання престає бути призупиненим, власний контролер черги не впливає на те, де насправді буде розташований pod завдання.

Ця функція дозволяє оновлювати директиви планування Job до запуску, що дає власному контролеру черги змогу впливати на розташування Podʼів, а в той самий час здійснювати власне призначення Podʼів вузлам в kube-scheduler. Це дозволяється тільки для призупинених Завдань, які ніколи не переставали бути призупиненими раніше.

Поля в шаблоні podʼа Job, які можна оновити, це приналежність до вузла, селектор вузла, толерантності, мітки, анотації та [вікно планування](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).

### Вказування власного селектора Podʼа {#specifying-your-own-pod-selector}

Зазвичай, коли ви створюєте обʼєкт Job, ви не вказуєте `.spec.selector`. Логіка системного визначення типових значень додає це поле під час створення Завдання. Вона обирає значення селектора, яке не буде перекриватися з будь-яким іншим завданням.

Однак у деяких випадках вам може знадобитися перевизначити цей автоматично встановлений селектор. Для цього ви можете вказати `.spec.selector` Job.

Будьте дуже уважні при цьому. Якщо ви вказуєте селектор міток, який не є унікальним для Podʼів цього Завдання, і який відповідає неповʼязаним Podʼам, то Podʼи з неповʼязаним Завданням можуть бути видалені, або це Завдання може вважати інші Podʼи завершеними, або одне чи обидва Завдання можуть відмовитися від створення Podʼів або виконуватись до завершення роботи. Якщо вибираєте неунікальний селектор, то інші контролери (наприклад, ReplicationController) та їхні Podʼи можуть проявляти непередбачувану поведінку також. Kubernetes не зупинить вас від того, що ви можете зробити помилку при зазначені `.spec.selector`.

Ось приклад ситуації, коли вам може знадобитися використовувати цю функцію.

Скажімо, Завдання `old` вже запущене. Ви хочете, щоб наявні Podʼи продовжували працювати, але ви хочете, щоб решта Podʼів, які воно створює, використовували інший шаблон Pod і щоб у Завдання було нове імʼя. Ви не можете оновити Завдання, оскільки ці поля не можна оновлювати. Отже, ви видаляєте Завдання `old`, але *залишаєте його Podʼи запущеними*, використовуючи `kubectl delete jobs/old --cascade=orphan`. Перед видаленням ви робите помітку, який селектор використовувати:

```shell
kubectl get job old -o yaml
```

Виввід схожий на цей:

```yaml
kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

Потім ви створюєте нове Завдання з імʼям `new` і явно вказуєте той самий селектор. Оскільки наявні Podʼи мають мітку `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`, вони також контролюються Завданням `new`.

Вам потрібно вказати `manualSelector: true` в новому Завданні, оскільки ви не використовуєте селектор, який система зазвичай генерує автоматично.

```yaml
kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

Саме нове Завдання матиме інший uid від `a8f3d00d-c6d2-11e5-9f87-42010af00002`. Встановлення `manualSelector: true` говорить системі, що ви розумієте, що робите, і дозволяє це неспівпадіння.

### Відстеження Завдання за допомогою завершувачів {#job-tracking-with-finalizers}

{{< feature-state for_k8s_version="v1.26" state="stable" >}}

Панель управління стежить за Podʼами, які належать до будь-якого Завдання, і виявляє, чи Pod був видалений з сервера API. Для цього контролер Job створює Podʼи з завершувачем `batch.kubernetes.io/job-tracking`. Контролер видаляє завершувач тільки після того, як Pod був врахований в стані Завдання, що дозволяє видалити Pod іншими контролерами або користувачами.

{{< note >}}
Див. [Мій Pod залишається в стані завершення](/docs/tasks/debug/debug-application/debug-pods/), якщо ви спостерігаєте, що Podʼи з Завдання залишаються з завершувачем відстеження.
{{< /note >}}

### Еластичні Індексовані Завдання {#elastic-indexed-jobs}

{{< feature-state feature_gate_name="ElasticIndexedJob" >}}

Ви можете масштабувати Indexed Job вгору або вниз, змінюючи як `.spec.parallelism`, так і `.spec.completions` разом, так щоб `.spec.parallelism == .spec.completions`. При масштабуванні вниз Kubernetes видаляє Podʼи з вищими індексами.

Сценарії використання для еластичних Індексованих Завдань включають пакетні робочі навантаження, які вимагають масштабування Індексованого Завдання, такі як MPI, Horovod, Ray, та PyTorch.

### Відкладене створення замінних Podʼів {#pod-replacement-policy}

{{< feature-state feature_gate_name="JobPodReplacementPolicy" >}}

Типово контролер Job створює Podʼи якнайшвидше, якщо вони або зазнають невдачі, або знаходяться в стані завершення (мають відмітку видалення). Це означає, що в певний момент часу, коли деякі з Podʼів знаходяться в стані завершення, кількість робочих Podʼів для Завдання може бути більшою, ніж `parallelism` або більше, ніж один Pod на індекс (якщо ви використовуєте Індексоване Завданя).

Ви можете обрати створення замінних Podʼів лише тоді, коли Podʼи, які знаходиться в стані завершення, повністю завершені (мають `status.phase: Failed`). Для цього встановіть `.spec.podReplacementPolicy: Failed`. Типова політика заміщення залежить від того, чи Завдання має встановлену політику відмови Podʼів. Якщо для Завдання не визначено політику відмови Podʼів, відсутність поля `podReplacementPolicy` вибирає політику заміщення `TerminatingOrFailed`: панель управління створює Podʼи заміни негайно після видалення Podʼів (як тільки панель управління бачить, що Pod для цього Завдання має встановлене значення `deletionTimestamp`). Для Завдань із встановленою політикою відмови Podʼів стандартне значення `podReplacementPolicy` — це `Failed`, інших значень не передбачено. Докладніше про політики відмови Podʼів для Завдань можна дізнатися у розділі [Політика збою Podʼа](#pod-failure-policy).

```yaml
kind: Job
metadata:
  name: new
  ...
spec:
  podReplacementPolicy: Failed
  ...
```

При увімкненій функціоанльній можливості у вашому кластері ви можете перевірити поле `.status.terminating` Завдання. Значення цього поля — це кількість Podʼів, якими володіє Завдання, які зараз перебувають у стані завершення.

```shell
kubectl get jobs/myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
# .metadata та .spec опущено
status:
  terminating: 3 # три Podʼи, які перебувають у стані завершення і ще не досягли стану Failed
```

### Делегування управління обʼєктом Job зовнішньому контролеру {#delegation-of-managing-a-job-object-to-external-controller}

{{< feature-state feature_gate_name="JobManagedBy" >}}

{{< note >}}
Ви можете встановити поле `managedBy` для Job лише в разі увімкнення [функціональної можливості](/docs/reference/command-line-tools-reference/feature-gates/) `JobManagedBy` (типово увімкнено).
{{< /note >}}

Ця функція дозволяє вам вимкнути вбудований контролер Job для конкретного Завдання і делегувати узгодження цього Завдання зовнішньому контролеру.

Ви вказуєте контролер, який узгоджує Завдання, встановлюючи власне значення для поля `spec.managedBy` — будь-яке значення окрім `kubernetes.io/job-controller`. Значення поля є незмінним.

{{< note >}}
При використанні цієї функції переконайтеся, що контролер, вказаний у полі, встановлено, інакше Завдання може не бути узгоджене взагалі.
{{< /note >}}

{{< note >}}
При розробці зовнішнього контролера Job, будьте уважні: ваш контролер повинен працювати відповідно до визначень специфікації API та полів статусу обʼєкта Job.

Докладно ознайомтесь з ними в [API Job](/docs/reference/kubernetes-api/workload-resources/job-v1/). Ми також рекомендуємо запустити наскрізні тести сумісності для обʼєкта Job, щоб перевірити вашу реалізацію.

Нарешті, при розробці зовнішнього контролера Job переконайтеся, що він не використовує завершувач `batch.kubernetes.io/job-tracking`, зарезервований для вбудованого контролера.
{{< /note >}}

{{< warning >}}
Якщо ви думаєте про вимкення `JobManagedBy`, або зниження версії кластера до версії без увімкненої функціональної можливості, перевірте, чи є у вас завдання з власним значенням поля `spec.managedBy`. Якщо такі завдання існують, існує ризик того, що після виконання операції вони можуть бути узгоджені двома контролерами: вбудованим контролером Job і зовнішнім контролером, вказаним значенням поля.
{{< /warning >}}

## Альтернативи {#alternatives}

### Тільки Podʼи {#bare-pods}

Коли вузол, на якому працює Pod, перезавантажується або виходить з ладу, Pod завершується і не буде перезапущений. Однак Завдання створить нові Podʼи для заміщення завершених. З цієї причини ми рекомендуємо використовувати Завдання замість використання тільки Podʼів, навіть якщо ваш застосунок вимагає тільки одного Podʼа.

### Контролер реплікації {#replication-controller}

Завдання є компліментарними до [контролера реплікації](/docs/concepts/workloads/controllers/replicationcontroller/). Контролер реплікації керує Podʼами, які не повинні завершуватися (наприклад, вебсервери), а Завдання керує Podʼами, які очікують завершення (наприклад, пакетні задачі).

Якщо врахувати [життєвий цикл Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` *лише* придатний для Podʼів з `RestartPolicy`, рівним `OnFailure` або `Never`. (Примітка: Якщо `RestartPolicy` не встановлено, стандартне значення — `Always`.)

### Один Job запускає контролер Podʼів {#single-job-starts-controller-pod}

Ще один підхід — це те, що одне Завдання створює Podʼи, які своєю чергою створюють інші Podʼи, виступаючи як свого роду власний контролер для цих Podʼів. Це дає найбільшу гнучкість, але може бути дещо складним для початку використання та пропонує меншу інтеграцію з Kubernetes.

Перевагою цього підходу є те, що загальний процес отримує гарантію завершення обʼєкта Job, але при цьому повністю контролюється те, які Podʼи створюються і яке навантаження їм призначається.

## {{% heading "whatsnext" %}}

- Дізнайтеся про [Podʼи](/docs/concepts/workloads/pods).
- Дізнайтеся про різні способи запуску Завдань:
  - [Груба паралельна обробка за допомогою черги роботи](/docs/tasks/job/coarse-parallel-processing-work-queue/)
  - [Тонка паралельна обробка за допомогою черги роботи](/docs/tasks/job/fine-parallel-processing-work-queue/)
  - Використовуйте [Індексовані Завдання для паралельної обробки зі статичним призначенням навантаження](/docs/tasks/job/indexed-parallel-processing-static/)
  - Створіть кілька Завдань на основі шаблону: [Паралельна обробка з використанням розширень](/docs/tasks/job/parallel-processing-expansion/)
- Перейдіть за посиланням [Автоматичне очищення завершених завдань](#clean-up-finished-jobs-automatically), щоб дізнатися більше про те, як ваш кластер може очищати завершені та/або завдання, що зазнали збою.
- `Job` є частиною REST API Kubernetes. Ознайомтесь з визначенням обʼєкта {{< api-reference page="workload-resources/job-v1" >}}, щоб зрозуміти API для завдань.
- Прочитайте про [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), який ви можете використовувати для визначення серії Завдань, які будуть виконуватися за розкладом, схожим на інструмент UNIX `cron`.
- Попрактикуйтесь в налаштувані обробки відновлюваних і невідновлюваних збоїв Podʼів за допомогою `podFailurePolicy` на основі покрокових [прикладів](/docs/tasks/job/pod-failure-policy/).
