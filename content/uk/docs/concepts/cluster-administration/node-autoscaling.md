---
title: Автомасштабування Node
linkTitle: Автомасштабування Node
description: >-
  Автоматично надавайте та консолідуйте вузли у вашому кластері, щоб адаптуватися до попиту та оптимізувати витрати.
content_type: concept
weight: 15
---

Для того, щоб запускати робочі навантаження у вашому кластері, вам потрібні {{< glossary_tooltip text="Вузли" term_id="node" >}}. Вузли у вашому кластері можуть бути _автомасштабовані_ - динамічно [_виділені_](#provisioning), або [_консолідовані_](#consolidation), щоб забезпечити необхідну потужність при оптимізації витрат. Автомасштабування виконується [_автомасштабувальниками_](#autoscalers) Вузлів.

## Виділення Вузлів {#provisioning}

Якщо в кластері є Podʼи, які не можуть бути заплановані на існуючих вузлах, нові вузли можуть бути автоматично додані до кластера, щоб розмістити ці Podʼи. Це особливо корисно, якщо кількість Podʼів змінюється з часом, наприклад, в результаті [поєднання горизонтального робочого навантаження з автомасштабуванням Вузлів](#horizontal-workload-autoscaling).

Автомасштабувальники забезпечують роботу Вузлів, створюючи та видаляючи ресурси хмарного провайдера, що їх підтримують. Найчастіше ресурсами, які забезпечують роботу Вузлів, є віртуальні машини.

Основна мета резервування - зробити так, щоб всі Podʼи можна було розмістити. Ця мета не завжди досяжна через різні обмеження, включаючи досягнення налаштованих лімітів виділення ресурсів, несумісність конфігурації виділення ресурсів з певним набором вузлів або нестачу потужностей хмарного провайдера. Під час виділення ресурсів автомасштабувальник Вузлів часто намагається досягти додаткових цілей (наприклад, мінімізувати вартість виділених Вузлів або збалансувати кількість Вузлів між доменами відмов).

При виділенні вузлів у автомасштабувальника Вузлів  є два основних параметри вхідних даних — [Обмеження планування вузлів](#provisioning-pod-constraints) та [Обмеження вузлів, що накладаються конфігурацією автомасштабування](#provisioning-node-constraints).

Конфігурація автомасштабування може також включати інші тригери виділення вузлів (наприклад, кількість вузлів, що падає нижче налаштованого мінімального ліміту).

{{< note >}}
Раніше виділення ресурсів було відоме як _масштабування_ у Cluster Autoscaler.
{{< /note >}}

### Обмеження планування вузлів {#provisioning-pod-constraints}

Podʼи можуть виражати [обмеження планування](/docs/concepts/scheduling-eviction/assign-pod-node/), щоб накласти обмеження на тип вузлів, на яких вони можуть бути заплановані. Автомасштабувальники Вузлів враховують ці обмеження, щоб гарантувати, що очікуючі Podʼи можуть бути заплановані на передбачені для них Вузли.

Найпоширенішим типом обмежень планування є запити на ресурси, визначені контейнерами Podʼа. Автомасштабувальники переконаються, що надані вузли мають достатньо ресурсів, щоб задовольнити ці запити. Однак, вони не враховують безпосередньо реальне використання ресурсів Podʼами після того, як вони почнуть працювати. Для того, щоб автоматично масштабувати вузли на основі фактичного використання ресурсів робочим навантаженням, ви можете поєднати [горизонтальне автомасштабування робочого навантаження](#horizontal-workload-autoscaling) з автомасштабуванням Вузлів.

Інші поширені обмеження планування Podʼів включають [спорідненість вузлів](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity), [між-Podʼова спорідненість](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) або вимогу до певного [обсягу сховища](/docs/concepts/storage/volumes/).

### Обмеження Вузлів, що накладаються конфігурацією автомасштабувальника {#provisioning-node-constraints}

Специфіка виділених Вузлів (наприклад, кількість ресурсів, наявність певної мітки) залежить від конфігурації автомасштабування. Автомасштабування може або вибирати їх із заздалегідь визначеного набору конфігурацій вузлів, або використовувати [автоматичне виділення ресурсів](#autoprovisioning).

### Автоматичне виділення ресурсів {#autoprovisioning}

Автоматичне виділення вузлів - це режим виділення, в якому користувачеві не потрібно повністю налаштовувати характеристики вузлів, які можуть бути виділені. Замість цього автомасштабувальник динамічно вибирає конфігурацію Вузла на основі очікуючих Podʼів, на які він реагує, а також попередньо налаштованих обмежень (наприклад, мінімальна кількість ресурсів або потреба в певній мітці).

## Консолідація Вузлів {#consolidation}

Основним моментом при експлуатації кластера є забезпечення запуску всіх запланованих вузлів, при цьому вартість кластера має бути якомога нижчою. Щоб досягти цього, запити на ресурси від Podʼів повинні використовувати якомога більше ресурсів Вузлів. З цієї точки зору, загальне споживання Вузлів у кластері може бути використано як проксі для визначення того, наскільки економічно ефективним є кластер.

{{< note >}}
Правильне налаштування запитів на ресурси для ваших Podʼів так само важливе для загальної економічної ефективності кластера, як і оптимізація використання Вузлів. Поєднання автомасштабування Вузлів з [вертикальним автомасштабуванням робочого навантаження](#vertical-workload-autoscaling) може допомогти вам досягти цього.
{{< /note >}}

Вузли у вашому кластері можуть бути автоматично _консолідовані_, щоб покращити загальне використання Вузлів, і, в свою чергу, економічну ефективність кластера. Консолідація відбувається шляхом видалення з кластера набору недовикористовуваних Вузлів. За бажанням, інший набір Вузлів може бути [виділенй] (#provisioning) для їх заміни.

Консолідація, як і резервування, при прийнятті рішень враховує лише запити на ресурси від Podʼів, а не реальне використання ресурсів.

Для цілей консолідації Вузол вважається _порожнім_, якщо на ньому запущено лише DaemonSet та статичні Podʼи. Видалення порожніх Вузлів під час консолідації простіше, ніж непорожніх, і автомасштабувальники часто мають оптимізацію, призначену спеціально для консолідації порожніх Вузлів.

Видалення непорожніх вузлів під час консолідації може призвести до збоїв у роботі, оскільки запущені на них Podʼи припиняють свою роботу, і, можливо, їх доведеться створювати заново (наприклад, за допомогою Deployment). Однак, всі такі відтворені Podʼи повинні мати можливість плануватися на існуючих вузлах кластера або на запасних Вузлах, наданих в рамках консолідації. __Зазвичай жоден Pod не повинен перебувати у стані очікування в результаті консолідації.__

{{< note >}}
Автомасштабувальники передбачають, як відтворений Pod, ймовірно, буде запланований після резервування або консолідації вузла, але вони не контролюють фактичне планування. Через це деякі Podʼи можуть перейти в стан очікування в результаті консолідації — якщо, наприклад, під час консолідації зʼявляється абсолютно новий Pod.
{{< /note >}}

Конфігурація автомасштабування може також дозволяти запускати консолідацію за іншими умовами (наприклад, за часом, що минув з моменту створення Вузла), щоб оптимізувати різні властивості (наприклад, максимальну тривалість життя Вузлів у кластері).

Деталі того, як виконується консолідація, залежать від конфігурації конкретного автомасштабувальника.

{{< note >}}
Консолідація раніше була відома як _зменшення масштабу_ у Cluster Autoscaler.
{{< /note >}}

## Автомасштабувальники {#autoscalers}

Функціональність, описану в попередніх розділах, забезпечують автоматичні масштабувальники Вузлів (Node _autoscalers_). На додаток до API Kubernetes, автомасштабувальники також повинні взаємодіяти з API хмарних провайдерів для виділення та консолідації Вузлів. Це означає, що вони повинні бути явно інтегровані з кожним підтримуваним хмарним провайдером. Продуктивність і набір функцій певного автомасштабувальника можуть відрізнятися в залежності від інтеграції з хмарним провайдером.

{{< mermaid >}}
graph TD
    na[Автомасштабувальник Вузлів]
    k8s[Kubernetes]
    cp[Хмарний провайдер]

    k8s --> |get Pods/Nodes|na
    na --> |drain Nodes|k8s
    na --> |create/remove resources backing Nodes|cp
    cp --> |get resources backing Nodes|na

    classDef white_on_blue fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef blue_on_white fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class na blue_on_white;
    class k8s,cp white_on_blue;
{{</ mermaid >}}

### Імплементація автомасштабувальника {#autoscaler-implementations}

[Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) та [Karpenter](https://github.com/kubernetes-sigs/karpenter) — це два автомасштабувальники вузлів, які наразі спонсоруються [SIG Autoscaling](https://github.com/kubernetes/community/tree/master/sig-autoscaling).

З точки зору користувача кластера, обидва автомасштабувальники повинні надавати схожий досвід автомасштабування Вузлів. Обидва надаватимуть нові вузли для незапланованих Podʼів, і обидва консолідуватимуть Вузли, які більше не використовуються оптимально.

Різні засоби автомасштабування можуть також надавати можливості, що виходять за рамки автомасштабування Вузлів, описані на цій сторінці, і ці додаткові можливості можуть відрізнятися між собою.

Consult the sections below, and the linked documentation for the individual autoscalers to decide which autoscaler fits your use case better.

#### Cluster Autoscaler {#cluster-autoscaler}

Cluster Autoscaler додає або видаляє Вузли до попередньо сконфігурованих _груп Вузлів_. Групи Вузлів зазвичай зіставляються з певною групою ресурсів хмарного провайдера (найчастіше з групою Віртуальних машин). Один екземпляр Cluster Autoscaler може одночасно керувати кількома групами Вузлів. Під час виділення ресурсів Cluster Autoscaler додає Вузли до тієї групи, яка найкраще відповідає запитам Podʼів, що знаходяться в очікуванні. Під час консолідації Cluster Autoscaler завжди вибирає конкретні Вузли для видалення, а не просто змінює розмір базової групи ресурсів хмарного провайдера.

Додатково:

* [Огляд документації](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md)
* [Інтеграція з хмарними провайдерами](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md#faqdocumentation)
* [ЧаПи Cluster Autoscaler](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md)
* [Контакти](https://github.com/kubernetes/community/tree/master/sig-autoscaling#contact)

#### Karpenter {#karpenter}

Karpenter автоматично виділяє вузли на основі конфігурацій [NodePool](https://karpenter.sh/docs/concepts/nodepools/), наданих оператором кластера. Karpenter керує всіма аспектами життєвого циклу вузла, а не лише автоматичним масштабуванням. Це включає автоматичне оновлення вузлів, коли вони досягають певного терміну служби, і автоматичне оновлення Вузлів, коли випускаються нові образи робочих Вузлів. Він працює безпосередньо з індивідуальними ресурсами хмарного провайдера (найчастіше з окремими віртуальними машинами) і не покладається на групи ресурсів хмарного провайдера.

Додатково:

* [Документація](https://karpenter.sh/)
* [Інтеграція з хмарними провайдерами](https://github.com/kubernetes-sigs/karpenter?tab=readme-ov-file#karpenter-implementations)
* [ЧаПи Karpenter](https://karpenter.sh/docs/faq/)
* [Контакти](https://github.com/kubernetes-sigs/karpenter#community-discussion-contribution-and-support)

#### Порівняння реалізацій {#implementation-comparison}

Основні відмінності між Cluster Autoscaler та Karpenter:

* Cluster Autoscaler надає можливості, повʼязані лише з автомасштабуванням Вузлів. Karpenter має ширшу сферу застосування, а також надає функції, призначені для управління життєвим циклом Вузлів в цілому (наприклад, використання порушень для автоматичного відтворення Вузлів, коли вони досягають певного часу життя, або автоматичного оновлення до нових версій).
* Cluster Autoscaler не підтримує автоматичне резервування, групи Вузлів, з яких він може надавати ресурси, мають бути попередньо налаштовані. Karpenter підтримує автоматичне резервування, тому користувачеві потрібно лише налаштувати набір обмежень для Вузлів, що резервуються, замість того, щоб повністю налаштовувати однорідні групи.
* Cluster Autoscaler надає інтеграцію з хмарними провайдерами безпосередньо, що означає, що вони є частиною проєкту Kubernetes. Для Karpenter проєкт Kubernetes публікує Karpenter як бібліотеку, з якою провайдери хмарних обчислень можуть інтегруватися для створення автомасштабування вузлів.
* Cluster Autoscaler забезпечує інтеграцію з багатьма хмарними провайдерами, в тому числі з невеликими і менш популярними провайдерами. Існує невелика кількість хмарних провайдерів, які інтегруються з Karpenter, зокрема [AWS](https://github.com/aws/karpenter-provider-aws) та [Azure](https://github.com/Azure/karpenter-provider-azure).

## Поєднання робочого навантаження та автомасштабування Вузлів {#combine-workload-and-node-autoscaling}

### Горизонтальне автомасштабування робочого навантаження {#horizontal-workload-autoscaling}

Автомасштабування вузлів зазвичай працює у відповідь на появу Podʼів: воно створює нові Вузли для розміщення незапланованих Podʼів, а потім консолідує Вузли, коли в них відпадає потреба.

[Горизонтальне автомасштабування робочого навантаження](/docs/concepts/workloads/autoscaling#scaling-workloads-horizontally) автоматично масштабує кількість реплік робочого навантаження для підтримки бажаного середнього використання ресурсів між репліками. Іншими словами, воно автоматично створює нові Podʼи у відповідь на навантаження програми, а потім видаляє Podʼи, як тільки навантаження зменшується.

Ви можете використовувати автомасштабування Вузлів разом з горизонтальним автомасштабуванням робочого навантаження для автоматичного масштабування Вузлів у вашому кластері на основі середнього реального використання ресурсів ваших Podʼів.

Якщо навантаження застосунку зростає, середнє використання його Podʼів також має зростати, що спонукає до автомасштабування робочого навантаження для створення нових Podʼів. Автомасштабування вузлів повинно забезпечити нові Вузли для розміщення нових Podʼів.

Як тільки навантаження застосунку зменшиться, автомасштабування робочого навантаження має видалити непотрібні Podʼи. Автомасштабування вузлів має, у свою чергу, консолідувати Вузли, які більше не потрібні.

При правильному налаштуванні цей шаблон гарантує, що ваш застосунок завжди матиме достатньо потужності Вузла, щоб впоратися зі сплесками навантаження, якщо це буде потрібно, але вам не доведеться платити за потужність, коли вона не потрібна.

### Вертикальне автоматичне масштабування робочого навантаження {#vertical-workload-autoscaling}

При використанні автомасштабування Вузлів важливо правильно встановити запити на ресурси Podʼа. Якщо запити певного Podʼа занадто низькі, створення для нього нового Вузла може не допомогти йому запуститися. Якщо запити певного Podʼа занадто високі, це може некоректно запобігти консолідації його Вузла.

[Вертикальне автоматичне масштабування робочого навантаження] (/docs/concepts/workloads/autoscaling#scaling-workloads-vertically) автоматично коригує запити на ресурси ваших Podʼів на основі їхнього історичного використання ресурсів.

Ви можете використовувати автомасштабування Вузлів разом з вертикальним автомасштабуванням робочого навантаження, щоб регулювати запити на ресурси ваших Podʼів, зберігаючи при цьому можливості автомасштабування Вузлів у вашому кластері.

{{< caution >}}
При використанні автомасштабування Вузлів не рекомендується налаштовувати вертикальне автомасштабування робочого навантаження для DaemonSet Podʼів. Автомасштабування має передбачити, як виглядатимуть DaemonSet Podʼи на новому Вузлі, щоб спрогнозувати доступні ресурси Вузла. Вертикальне автомасштабування робочого навантаження може зробити ці прогнози ненадійними, що призведе до неправильних рішень щодо масштабування.
{{</ caution >}}

## Супутні компоненти {#related-components}

У цьому розділі описано компоненти, що надають функціональність, повʼязану з автомасштабуванням Вузлів.

### Descheduler {#descheduler}

Планувальник [descheduler](https://github.com/kubernetes-sigs/descheduler) — це компонент, що надає функціональність консолідації Вузлів на основі власних політик користувачів, а також інші можливості, повʼязані з оптимізацією Вузлів та Podʼів (наприклад, видалення Podʼів, що часто перезавантажуються).

### Автомасштабування навантаження на основі розміру кластера {#workload-autoscalers-based-on-cluster-size}

[Cluster Proportional Autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler) та [Cluster Proportional Vertical Autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler) забезпечують горизонтальне та вертикальне автомасштабування робочого навантаження на основі кількості Вузлів у кластері. Ви можете прочитати більше у статті [автомасштабування на основі розміру кластера](/docs/concepts/workloads/autoscaling#autoscaling-based-on-cluster-size).

## {{% heading "whatsnext" %}}

* Прочитайте про [автоматичне масштабування на рівні робочого навантаження](/docs/concepts/workloads/autoscaling/)
