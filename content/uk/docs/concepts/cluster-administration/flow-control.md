---
title: API Priority та Fairness
content_type: concept
min-kubernetes-server-version: v1.18
weight: 110
---

<!-- overview -->

{{< feature-state state="stable" for_k8s_version="v1.29" >}}

Контроль поведінки сервера API Kubernetes у ситуації перевантаження є основним завданням для адміністраторів кластера. У {{< glossary_tooltip term_id="kube-apiserver" text="kube-apiserver" >}} є деякі елементи керування (тобто прапорці командного рядка `--max-requests-inflight` та `--max-mutating-requests-inflight`), щоб обмежити обсяг невиконаної роботи, яка буде прийнята, запобігаючи перевантаженню потоку вхідних запитів та потенційному збою сервера API, але цих прапорців недостатньо для забезпечення проходження найважливіших запитів у період високого навантаження.

Елементи API Priority та Fairness (APF) є альтернативою, яка покращує зазначені обмеження щодо максимального числа невиконаних завдань. APF класифікує та ізолює запити більш деталізованим способом. Вони також вводять обмеження на черги, щоб ніякі запити не були відхилені в разі дуже коротких вибухів. Запити надсилаються з черг за допомогою техніки справедливих черг, так що, наприклад, {{< glossary_tooltip text="контролер" term_id="controller" >}}, який погано поводиться не має вичерпати інші ресурси (навіть на тому ж рівні пріоритету).

Ця функція розроблена для співпраці зі стандартними контролерами, які використовують інформатори та реагують на невдачі запитів API з експоненційним збільшенням часу очікування, та іншими клієнтами, які також працюють таким чином.

{{< caution >}}
Деякі запити, класифіковані як "тривалі", такі як виконання віддалених команд або отримання останніх записів з логів, не є предметом фільтрації API Priority та Fairness. Це також стосується прапорця командного рядка `--max-requests-inflight` без увімкненої функціональності API Priority та Fairness. API Priority та Fairness _застосовується_ до **watch** запитів. Коли функціональність API Priority та Fairness вимкнена, **watch** запити не підлягають обмеженню `--max-requests-inflight`.
{{< /caution >}}

<!-- body -->

## Увімкнення/вимкнення API Priority та Fairness {#enabling-disabling-api-priority-and-fairness}

API Priority та Fairness контролюється прапорцем командного рядка та є типово увімкненим. Див. [Опції](/docs/reference/command-line-tools-reference/kube-apiserver/#options) для загального пояснення доступних опцій командного рядка `kube-apiserver` та того, як їх увімкнути та вимкнути. Назва параметра командного рядка для APF — "--enable-priority-and-fairness". Ця функція також включає {{<glossary_tooltip term_id="api-group" text="API Group" >}}, яка має: (a) стабільну версію `v1`, введена у версії 1.29, і типово увімкнена, (b) версію `v1beta3`, типово увімкнена та застаріла у версії v1.29. Ви можете вимкнути бета-версію групи API `v1beta3`, додавши наступні параметри командного рядка до запуску `kube-apiserver`:

```shell
kube-apiserver \
--runtime-config=flowcontrol.apiserver.k8s.io/v1beta3=false \
 # …і інші прапорці, як завжди
```

Параметр командного рядка `--enable-priority-and-fairness=false` вимкне API Priority та Fairness.

## Рекурсивні сценарії сервера {#recursive-server-scenarios}

API Priority and Fairness повинні використовуватися обережно в рекурсивних сценаріях сервера. Це сценарії, в яких сервер A, обробляючи запит, надсилає допоміжний запит на сервер B. Можливо, сервер B може навіть зробити подальший допоміжний виклик назад до сервера A. У ситуаціях, коли контроль Пріоритету і Справедливості застосовується як до оригінального запиту, так і до деяких допоміжних, незалежно від глибини рекурсії, існує небезпека пріоритетних інверсій та/або взаємоблокувань.

Прикладом рекурсії є випадок, коли `kube-apiserver` робить виклик admission webhook на сервер B, і під час обробки цього виклику сервер B робить подальший допоміжний запит назад до `kube-apiserver`. Інший приклад рекурсії — коли обʼєкт `APIService` спрямовує `kube-apiserver` делегувати запити щодо певної групи API на власний зовнішній сервер B користувача (це один з аспектів, які називаються "агрегацією").

Коли відомо, що оригінальний запит належить до певного рівня пріоритету, і допоміжні контрольовані запити класифікуються на вищі рівні пріоритету, це може бути одним з можливих рішень. Коли оригінальні запити можуть належати до будь-якого рівня пріоритету, допоміжні контрольовані запити повинні бути звільнені від обмежень Priority and Fairness. Один із способів це зробити — за допомогою обʼєктів, які налаштовують класифікацію та обробку, обговорених нижче. Інший спосіб — повністю відключити Priority and Fairness на сервері B, використовуючи техніки, обговорені вище. Третій спосіб, який найпростіше використовувати, коли сервер B не є `kube-apiserver`, це побудувати сервер B з відключеним у коді Priority and Fairness.

## Концепції {#concepts}

API Priority та Fairness включає кілька різних функцій. Вхідні запити класифікуються за атрибутами запиту за допомогою _FlowSchemas_, і призначаються пріоритетним рівням. Пріоритетні рівні додають ступінь ізоляції, підтримуючи окремі обмеження паралелізму, так що запити, які призначені для різних пріоритетних рівнів, не можуть позбавляти ресурсів один одного. У межах пріоритетного рівня алгоритм справедливого чергування не дозволяє запитам з різних _потоків_ позбавляти ресурсів один одного, і дозволяє чергувати запити, щоб запобігти відмовам від запитів у випадках, коли середнє навантаження прийнятно низьке.

### Рівні пріоритету {#priority-levels}

Без увімкненої функції APF загальний паралелізм в API-сервері обмежується прапорцями `--max-requests-inflight` та `--max-mutating-requests-inflight` у `kube-apiserver`. З увімкненою функцією APF ліміти паралелізму, визначені цими прапорцями, сумуються, а потім сума розподіляється серед налаштованого набору _рівнів пріоритету_. Кожен вхідний запит призначається одному з рівнів пріоритету, і кожен рівень пріоритету буде обробляти лише стільки одночасних запитів, скільки дозволяє його конкретний ліміт.

Наприклад, у типовій конфігурації існують окремі рівні пріоритету для запитів на вибір лідера, запитів від вбудованих контролерів і запитів від Podʼів. Це означає, що Podʼи, які поводяться погано і завалюють API-сервер запитами, не можуть перешкодити вибору лідера або виконанню дій вбудованими контролерами.

Межі паралелізму рівнів пріоритету періодично коригуються, що дозволяє недостатньо використовуваним рівням пріоритету тимчасово надавати паралелізм сильно використовуваних рівнів. Ці обмеження базуються на номінальних обмеженнях та межах того, скільки паралелізму може позичити рівень пріоритету та скільки він може позичити, всі отримані з обʼєктів конфігурації, згаданих нижче.

### Місця, зайняті за запитом {#seats-occupied-by-a-request}

Наведений вище опис управління паралелізмом є базовою поведінкою. Запити мають різну тривалість, але враховуються однаково в будь-який момент при порівнянні з лімітом паралелізму на рівні пріоритету. У базовій поведінці кожен запит займає одну одиницю паралелізму. Слово "місце" (seat) використовується для позначення однієї одиниці паралелізму, під впливом від того, як кожен пасажир у поїзді або літаку займає одне з фіксованих місць.

Але деякі запити займають більше, ніж одне місце. Деякі з них — це запити **list**, які сервер оцінює, що повертають велику кількість обʼєктів. Вони виявилися надзвичайно важкою ношею для сервера. З цієї причини сервер оцінює кількість обʼєктів, які будуть повернуті, і вважає, що запит займає кількість місць, пропорційну цій оцінці.

### Коригування часу виконання для запитів watch {#execution-time-tweaks-for-watch-requests}

API Priority та Fairness керує запитами **watch**, але це передбачає ще кілька відступів від базової поведінки. Перше стосується того, як довго запит **watch** вважається таким, що займає своє місце. Залежно від параметрів запиту, відповідь на запит на **watch** може або не може розпочинатися зі сповіщень **create** для всіх відповідних попередніх обʼєктів. API Priority та Fairness вважає, що запит **watch** завершено зі своїм місцем, як тільки цей початковий вибух сповіщень, якщо такі є, закінчився.

Нормальні сповіщення надсилаються у паралельному сплеску на всі потрібні потоки відповіді на **watch**, коли сервер отримує сповіщення про створення/оновлення/видалення обʼєкта. Для врахування цієї роботи API Priority та Fairness вважає, що кожен записний запит витрачає додатковий час, займаючи місця після завершення фактичного запису. Сервер оцінює кількість сповіщень, які будуть надіслані, і коригує кількість місць та час зайнятості місця для записного запиту, щоб врахувати цю додаткову роботу.

### Черги {#queuing}

Навіть на рівні пріоритету може бути велика кількість різних джерел трафіку. У ситуації перевантаження важливо запобігти тому, щоб один потік вичерпував ресурси інших (зокрема, у відносно поширеному випадку, коли один помилковий клієнт переповнює kube-apiserver запитами, цей помилковий клієнт в ідеалі взагалі не мав би помітного впливу на інших клієнтів). Це обробляється за допомогою алгоритму справедливої черги для обробки запитів, яким присвоєно однаковий рівень пріоритету. Кожен запит присвоюється потоку, ідентифікованому за назвою відповідної FlowSchema плюс розрізнювач потоку - який є або запитуючим користувачем, або простором імен цільового ресурсу, або нічим - і система намагається надати приблизно однакову вагу запитам у різних потоках одного і того ж рівня пріоритету. Щоб забезпечити чітку обробку різних екземплярів, контролери, які мають багато екземплярів, повинні автентифікуватися за допомогою різних імен користувачів

Це забезпечується за допомогою алгоритму справедливих черг обробки запитів, які призначені для того ж рівня пріоритету. Кожен запит призначається для _потоку_, ідентифікованого імʼям відповідного FlowSchema плюс _розрізнювач потоку_ — яким є або користувач-абонент, або простір імен цільового ресурсу, або нічого — і система намагається надати приблизно рівну вагу запитам в різних потоках того ж рівня пріоритету.

Після класифікації запиту у потік, API Priority та Fairness може призначити запит до черги. Це призначення використовує техніку, відому як {{< glossary_tooltip term_id="shuffle-sharding" text="shuffle sharding" >}}, яка досить ефективно використовує черги для ізоляції потоків низької інтенсивності від потоків високої інтенсивності.

Деталі алгоритму черг налаштовуються для кожного рівня пріоритету і дозволяють адміністраторам збалансувати використання памʼяті, справедливість (властивість того, що незалежні потоки всі рухатимуться вперед, коли загальний трафік перевищує потужність), толерантність до бурхливого трафіку та доданий час очікування, що виникає через черги.

### Ексклюзивні запити {#extempt-requests}

Деякі запити вважаються достатньо важливими, щоб не підлягати жодним обмеженням, які накладає ця функція. Ці винятки запобігають тому, щоб неправильно налаштована конфігурація керування потоком повністю вимкнула API-сервер.

## Ресурси {#resources}

API керування потоком включає два види ресурсів. [Конфігурації рівня пріоритету](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#prioritylevelconfiguration-v1-flowcontrol-apiserver-k8s-io) визначають доступні рівні пріоритету, частку доступного бюджету паралелізму, яку кожен може обробити, та дозволяють налаштовувати поведінку черги. [FlowSchemas](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#flowschema-v1-flowcontrol-apiserver-k8s-io) використовуються для класифікації окремих вхідних запитів, відповідність кожного з PriorityLevelConfiguration.

### PriorityLevelConfiguration

PriorityLevelConfiguration представляє один рівень пріоритету. Кожен PriorityLevelConfiguration має незалежні обмеження на кількість невиконаних запитів та обмеження на кількість запитів у черзі.

Номінальне обмеження паралелізму для PriorityLevelConfiguration не визначається в абсолютній кількості місць, а в "номінальних частках паралелізму". Загальне обмеження паралелізму для API-сервера розподіляється серед наявних конфігурацій рівня пріоритету пропорційно цим часткам, щоб кожен рівень отримав своє номінальне обмеження у вигляді місць. Це дозволяє адміністратору кластера збільшувати або зменшувати загальний обсяг трафіку до сервера, перезапускаючи `kube-apiserver` з іншим значенням для `--max-requests-inflight` (або `--max-mutating-requests-inflight`), і всі PriorityLevelConfiguration побачать, що їх максимально допустимий паралелізм збільшиться (або зменшиться) на ту саму частку.

{{< caution >}}
У версіях до `v1beta3` відповідне поле PriorityLevelConfiguration має назву "assured concurrency shares" замість "nominal concurrency shares". Також у версії Kubernetes 1.25 та раніше не проводилося періодичних налаштувань: номінальні/забезпечені обмеження застосовувалися завжди без коригування.
{{< /caution >}}

Межі того, скільки паралелізму може надати рівень пріоритету та скільки він може позичити, виражені в PriorityLevelConfiguration у відсотках від номінального обмеження рівня. Ці значення перетворюються на абсолютні числа місць, множачи їх на номінальне обмеження / 100.0 та округлюючи. Динамічно кориговане обмеження паралелізму рівня пріоритету обмежене нижньою межею його номінального обмеження мінус кількість місць, які він може позичити, та верхньою межею його номінального обмеження плюс місця, які він може позичити. Під час кожного налаштування динамічні межі визначаються кожним рівнем пріоритету, відновлюючи будь-які позичені місця, на які востаннє зʼявлявся попит, та спільно відповідаючи на попит за місцями на рівнях пріоритету в межах вищезазначених меж.

{{< caution >}}
При увімкненні функції Priority та Fairness загальне обмеження паралелізму для сервера встановлюється як сума `--max-requests-inflight` та `--max-mutating-requests-inflight`. Більше не робиться різниці між змінними та незмінними запитами; якщо ви хочете обробляти їх окремо для певного ресурсу, створіть окремі FlowSchemas, які відповідають відповідним діям.
{{< /caution >}}

Коли обсяг вхідних запитів, які призначені для одного PriorityLevelConfiguration, перевищує його дозволений рівень паралелізму, поле `type` його специфікації визначає, що відбудеться з додатковими запитами. Тип `Reject` означає, що зайвий трафік буде негайно відхилено з помилкою HTTP 429 (Too Many Requests). Тип `Queue` означає, що запити, які перевищують поріг, будуть ставитися у чергу, із використанням технік випадкового розміщення та справедливої черги для збалансування прогресу між потоками запитів.

Конфігурація черги дозволяє налаштувати алгоритм справедливої черги для рівня пріоритету. Деталі алгоритму можна знайти у [пропозиції про вдосконалення](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness), ось скорочено:

- Збільшення `queues` зменшує швидкість зіткнень між різними потоками, коштом збільшення використання памʼяті. Значення 1 тут фактично вимикає логіку справедливої черги, але все ще дозволяє ставити запити в чергу.

- Збільшення `queueLengthLimit` дозволяє підтримувати більші потоки трафіку без відкидання будь-яких запитів, коштом збільшення затримки та використання памʼяті.

- Зміна `handSize` дозволяє вам налаштувати ймовірність зіткнень між різними потоками та загальний паралелізм, доступний для одного потоку в ситуації перевантаження.

  {{< note >}}
  Більше значення `handSize` дає менше ймовірності, що два окремі потоки зіткнуться (і, отже, один зможе відібрати ресурси в іншого), але більше ймовірності того, що невелика кількість потоків може домінувати в apiserver. Більше значення `handSize` також потенційно збільшує кількість затримок, які може спричинити один потік з високим трафіком. Максимальна кількість запитів, які можна поставити в чергу від одного потоку, становить `handSize * queueLengthLimit`.
  {{< /note >}}

Нижче наведена таблиця з цікавою колекцією конфігурацій випадкового розміщення, що показує для кожної ймовірність те, що певна миша (потік низької інтенсивності) буде розчавлена слонами (потоками високої інтенсивності) для ілюстративної колекції числа слонів. Див.
<https://play.golang.org/p/Gi0PLgVHiUg>, який обчислює цю таблицю.

{{< table caption = "Приклади конфігурацій випадкового розміщення" >}}
HandSize | Черги | 1 слон | 4 слони | 16 слонів
|----------|-----------|------------|----------------|--------------------|
| 12 | 32 | 4.428838398950118e-09 | 0.11431348830099144 | 0.9935089607656024 |
| 10 | 32 | 1.550093439632541e-08 | 0.0626479840223545 | 0.9753101519027554 |
| 10 | 64 | 6.601827268370426e-12 | 0.00045571320990370776 | 0.49999929150089345 |
| 9 | 64 | 3.6310049976037345e-11 | 0.00045501212304112273 | 0.4282314876454858 |
| 8 | 64 | 2.25929199850899e-10 | 0.0004886697053040446 | 0.35935114681123076 |
| 8 | 128 | 6.994461389026097e-13 | 3.4055790161620863e-06 | 0.02746173137155063 |
| 7 | 128 | 1.0579122850901972e-11 | 6.960839379258192e-06 | 0.02406157386340147 |
| 7 | 256 | 7.597695465552631e-14 | 6.728547142019406e-08 | 0.0006709661542533682 |
| 6 | 256 | 2.7134626662687968e-12 | 2.9516464018476436e-07 | 0.0008895654642000348 |
| 6 | 512 | 4.116062922897309e-14 | 4.982983350480894e-09 | 2.26025764343413e-05 |
| 6 | 1024 | 6.337324016514285e-16 | 8.09060164312957e-11 | 4.517408062903668e-07 |
{{< /table >}}

### FlowSchema

FlowSchema — це визначення, яке відповідає деяким вхідним запитам і призначає їх певному рівню пріоритету. Кожний вхідний запит перевіряється на відповідність FlowSchemas, починаючи з тих, що мають найменше числове значення `matchingPrecedence`, і працюючи вгору. Перший збіг перемагає.

{{< caution >}}
Лише перший збіг FlowSchema для даного запиту має значення. Якщо для одного вхідного запиту відповідає декілька FlowSchema, він буде призначений відповідно до того, що має найвищий `matchingPrecedence`. Якщо кілька FlowSchema з рівним `matchingPrecedence` відповідають тому ж запиту, переможе та, яка має лексикографічно менший `name`, але краще не покладатися на це, а замість цього переконатися, що жодні дві FlowSchema не мають одного й того ж `matchingPrecedence`.
{{< /caution >}}

FlowSchema відповідає даному запиту, якщо принаймні одне з `rules` має збіг. Правило має збіг, якщо принаймні один з його `subjects` _та_ принаймні один з його `resourceRules` або `nonResourceRules` (залежно від того, чи є вхідний запит для ресурсу, чи нересурсного URL) відповідає запиту.

Для поля `name` в підсумках та полів `verbs`, `apiGroups`, `resources`, `namespaces` та `nonResourceURLs` правил ресурсів та нересурсних правил можна вказати маску `*`, щоб відповідати всім значенням для вказаного поля, яке фактично видаляє його з використання.

Тип `distinguisherMethod.type` FlowSchema визначає, як запити, які відповідають цій схемі, будуть розділені на потоки. Це може бути `ByUser`, коли один користувач має можливість позбавити інших користувачів можливості використовувати ресурси; `ByNamespace`, коли запити до ресурсів в одному просторі імен не можуть позбавити запити до ресурсів в інших просторах імен можливості використовувати їх; або порожнє значення (або `distinguisherMethod` може бути пропущений повністю), коли всі запити, які відповідають цій FlowSchema, будуть вважатися частиною одного потоку. Правильний вибір для даної FlowSchema залежить від ресурсу та вашого конкретного середовища.

## Стандартні {#defaults}

Кожен kube-apiserver підтримує два різновиди обʼєктів API Priority та Fairnes: обовʼязкові та рекомендовані.

### Обовʼязкові обʼєкти конфігурації {#mandatory-configuration-objects}

Чотири обовʼязкові обʼєкти конфігурації відображають фіксовану вбудовану поведінку обмежень безпеки. Це поведінка, яку сервери мають до того, як ці обʼєкти почнуть існувати, і коли ці обʼєкти існують, їх специфікації відображають цю поведінку. Чотири обовʼязкові обʼєкти є такими:

- Обовʼязковий пріоритетний рівень `exempt` використовується для запитів, які взагалі не підлягають контролю потоку: вони завжди будуть надсилатися негайно. Обовʼязкова FlowSchema `exempt` класифікує всі запити з групи `system:masters` у цей рівень пріоритету. Ви можете визначити інші FlowSchemas, які направляють інші запити на цей рівень пріоритету, якщо це доцільно.

- Обовʼязковий пріоритетний рівень `catch-all` використовується в поєднанні з обовʼязковою FlowSchema `catch-all`, щоб переконатися, що кожен запит отримує якийсь вид класифікації. Зазвичай ви не повинні покладатися на цю універсальну `catch-all` конфігурацію і повинні створити свою власну FlowSchema та PriorityLevelConfiguration для обробки всіх запитів. Оскільки очікується, що він не буде використовуватися нормально, обовʼязковий рівень пріоритету `catch-all` має дуже невелику частку паралелізму і не ставить запити в чергу.

### Рекомендовані обʼєкти конфігурації {#suggest-configuration-objects}

Рекомендовані FlowSchema та PriorityLevelConfigurations складають обґрунтовану типову конфігурацію. Ви можете змінювати їх або створювати додаткові обʼєкти конфігурації за необхідності. Якщо ваш кластер може зазнати високого навантаження, то варто розглянути, яка конфігурація працюватиме краще.

Рекомендована конфігурація групує запити в шість рівнів пріоритету:

- Рівень пріоритету `node-high` призначений для оновлень стану вузлів.

- Рівень пріоритету `system` призначений для запитів, які не стосуються стану справності, з групи `system:nodes`, тобто Kubelets, які повинні мати можливість звʼязатися з сервером API, щоб завдання могли плануватися на них.

- Рівень пріоритету `leader-election` призначений для запитів на вибір лідера від вбудованих контролерів (зокрема, запити на `endpoints`, `configmaps` або `leases`, що надходять від користувачів та службових облікових записів з групи `system:kube-controller-manager` або `system:kube-scheduler` в просторі імен `kube-system`). Ці запити важливо ізолювати від іншого трафіку, оскільки невдачі у виборі лідера призводять до збоїв їх контролерів і перезапуску, що в свою чергу призводить до більших витрат трафіку, оскільки нові контролери синхронізують свої сповіщувачі.

- Рівень пріоритету `workload-high` призначений для інших запитів від вбудованих контролерів.

- Рівень пріоритету `workload-low` призначений для запитів від будь-якого іншого службового облікового запису, що зазвичай включає всі запити від контролерів, що працюють в капсулах.

- Рівень пріоритету `global-default` обробляє весь інший трафік, наприклад, інтерактивні команди `kubectl`, запущені непривілейованими користувачами.

Рекомендовані FlowSchema служать для направлення запитів на вищезазначені рівні пріоритету, і тут вони не перераховані.

### Підтримка обовʼязкових та рекомендованих обʼєктів конфігурації {#maintaining-of-mandatory-and-suggested-configuration-objects}

Кожен `kube-apiserver` незалежно підтримує обовʼязкові та рекомендовані обʼєкти конфігурації, використовуючи початкову та періодичну поведінку. Отже, в ситуації з сумішшю серверів різних версій може відбуватися турбулентність, поки різні сервери мають різні погляди на відповідний вміст цих обʼєктів.

Кожен `kube-apiserver` робить початковий обхід обовʼязкових та рекомендованих обʼєктів конфігурації, а після цього здійснює періодичну обробку (один раз на хвилину) цих обʼєктів.

Для обовʼязкових обʼєктів конфігурації підтримка полягає в тому, щоб забезпечити наявність обʼєкта та, якщо він існує, мати правильну специфікацію. Сервер відмовляється дозволити створення або оновлення зі специфікацією, яка не узгоджується з поведінкою огородження сервера.

Підтримка рекомендованих обʼєктів конфігурації призначена для того, щоб дозволити перевизначення їх специфікації. Видалення, з іншого боку, не враховується: підтримка відновить обʼєкт. Якщо вам не потрібен рекомендований обʼєкт конфігурації, то вам потрібно залишити його, але встановити його специфікацію так, щоб мінімально впливати на систему. Підтримка рекомендованих обʼєктів також призначена для підтримки автоматичної міграції при виході нової версії `kube-apiserver`, хоча, можливо, з турбулентністю при змішаній популяції серверів.

Підтримка рекомендованого обʼєкта конфігурації включає його створення, зі специфікацією, яку пропонує сервер, якщо обʼєкт не існує. З іншого боку, якщо обʼєкт вже існує, поведінка підтримка залежить від того, чи контролюють обʼєкт `kube-apiservers` чи користувачі. У першому випадку сервер забезпечує, що специфікація обʼєкта відповідає тому, що пропонує сервер; у другому випадку специфікація залишається без змін.

Питання про те, хто контролює обʼєкт, вирішується спочатку шляхом пошуку анотації з ключем `apf.kubernetes.io/autoupdate-spec`. Якщо така анотація є і її значення `true`, то обʼєкт контролюється `kube-apiservers`. Якщо така анотація існує і її значення `false`, то обʼєкт контролюють користувачі. Якщо жодна з цих умов не виконується, тоді перевіряється `metadata.generation` обʼєкта. Якщо вона дорівнює 1, то обʼєкт контролюють `kube-apiservers`. В іншому випадку обʼєкт контролюють користувачі. Ці правила були введені у версії 1.22, і врахування ними `metadata.generation` призначена для міграції з більш простої попередньої поведінки. Користувачі, які хочуть контролювати рекомендований обʼєкт конфігурації, повинні встановити анотацію `apf.kubernetes.io/autoupdate-spec` в `false`.

Підтримка обовʼязкового або рекомендованого обʼєкта конфігурації також включає забезпечення наявності анотації `apf.kubernetes.io/autoupdate-spec`, яка точно відображає те, чи контролюють `kube-apiservers` обʼєкт.

Підтримка також включає видалення обʼєктів, які не є обовʼязковими або рекомендованими, анотованих як `apf.kubernetes.io/autoupdate-spec=true`.

## Звільнення від паралелізму перевірки справності {#health-check-сoncurrency-exemption}

Запропонована конфігурація не надає спеціального поводження запитам на перевірку стану здоровʼя на `kube-apiservers` від їхніх локальних kubelets, які зазвичай використовують захищений порт, але не надають жодних облікових даних. Запити цього типу вказаної конфігурації призначаються до FlowSchema `global-default` і відповідного пріоритетного рівня `global-default`, де інші запити можуть їх перенавантажувати.

Якщо ви додасте наступний додатковий FlowSchema, це звільнить ці запити від обмежень.

{{< caution >}}
Ці зміни, також дозволяють будь-якому, навіть зловмисникам надсилати запити перевірки стану справності, які відповідають цьій FlowSchema, у будь-якому обсязі. Якщо у вас є фільтр вебтрафіку або подібний зовнішній механізм безпеки для захисту сервера API вашого кластера від загального зовнішнього трафіку, ви можете налаштувати правила для блокування будь-яких запитів на перевірку стану справності, які походять ззовні вашого кластера.
{{< /caution >}}

{{% code_sample file="priority-and-fairness/health-for-strangers.yaml" %}}

## Спостережуваність {#observability}

### Метрики {#metrics}

{{< note >}}
У версіях Kubernetes перед v1.20 мітки `flow_schema` та `priority_level` мали нестабільні назви `flowSchema` та `priorityLevel`, відповідно. Якщо ви використовуєте версії Kubernetes v1.19 та раніше, вам слід звертатися до документації для вашої версії.
{{< /note >}}

Після увімкнення функції API Priority та Fairness, `kube-apiserver` експортує додаткові метрики. Моніторинг цих метрик може допомогти вам визначити, чи правильно ваша конфігурація обробляє важливий трафік, або знайти проблемні робочі навантаження, які можуть шкодити справності системи.

#### Рівень зрілості BETA {#maturity-level-beta}

- `apiserver_flowcontrol_rejected_requests_total` — це вектор лічильника (накопичувальний з моменту запуску сервера) запитів, які були відхилені, розбитий за мітками `flow_schema` (показує той, який підходить до запиту), `priority_level` (показує той, до якого був призначений запит) і `reason`. Мітка `reason` буде мати одне з наступних значень:

  - `queue-full`, що вказує на те, що занадто багато запитів вже було в черзі.
  - `concurrency-limit`, що вказує на те, що конфігурація PriorityLevelConfiguration налаштована відхиляти замість того, щоб ставити в чергу зайві запити.
  - `time-out`, що вказує на те, що запит все ще перебував в черзі, коли його обмеження часу в черзі закінчилося.
  - `cancelled`, що вказує на те, що запит не закріплений і викинутий з черги.

- `apiserver_flowcontrol_dispatched_requests_total` — це вектор лічильника (накопичувальний з моменту запуску сервера) запитів, які почали виконуватися, розбитий за `flow_schema` та `priority_level`.

- `apiserver_flowcontrol_current_inqueue_requests` — це вектор мітки миттєвого числа запитів, які знаходяться в черзі (не виконуються), розбитий за `priority_level` та `flow_schema`.

- `apiserver_flowcontrol_current_executing_requests` — це вектор мітки миттєвого числа виконуваних (не очікуючих в черзі) запитів, розбитий за `priority_level` та `flow_schema`.

- `apiserver_flowcontrol_current_executing_seats` — це вектор мітки миттєвого числа зайнятих місць, розбитий за `priority_level` та `flow_schema`.

- `apiserver_flowcontrol_request_wait_duration_seconds` — це вектор гістограми того, скільки часу запити провели в черзі, розбитий за мітками `flow_schema`, `priority_level` та `execute`. Мітка `execute` вказує, чи почалося виконання запиту.

  {{< note >}}
  Оскільки кожна FlowSchema завжди призначає запити для одного PriorityLevelConfiguration, ви можете додати гістограми для всіх FlowSchemas для одного рівня пріоритету, щоб отримати ефективну гістограму для запитів, призначених для цього рівня пріоритету.
  {{< /note >}}

- `apiserver_flowcontrol_nominal_limit_seats` — це вектор вектор, що містить номінальну межу паралелізму кожного рівня пріоритету, обчислювану з загального ліміту паралелізму сервера API та налаштованих номінальних спільних ресурсів паралелізму рівня пріоритету.

#### Рівень зрілості ALPHA {#maturity-level-alpha}

- `apiserver_current_inqueue_requests` — це вектор вимірювання останніх найвиших значень кількості запитів, обʼєднаних міткою `request_kind`, значення якої можуть бути `mutating` або `readOnly`. Ці величини описують найбільше значення спостереження в односекундному вікні процесів, що недавно завершилися. Ці дані доповнюють старий графік `apiserver_current_inflight_requests`, що містить високу точку кількості запитів в останньому вікні, які активно обслуговуються.

- `apiserver_current_inqueue_seats` — це вектор мітки миттєвого підсумку запитів, які будуть займати найбільшу кількість місць, згрупованих за мітками з назвами `flow_schema` та `priority_level`.

- `apiserver_flowcontrol_read_vs_write_current_requests` — це гістограма спостережень, зроблених в кінці кожної наносекунди, кількості запитів, розбита за мітками `phase` (приймає значення `waiting` та `executing`) та `request_kind` (приймає значення `mutating` та `readOnly`). Кожне спостереження є коефіцентом, від 0 до 1, кількості запитів, поділеним на відповідний ліміт кількості запитів (обмеження обсягу черги для очікування та обмеження паралелізму для виконання).

- `apiserver_flowcontrol_request_concurrency_in_use` — це вектор мітки миттєвого числа зайнятих місць, розбитий за `priority_level` та `flow_schema`.

- `apiserver_flowcontrol_priority_level_request_utilization` — це гістограма спостережень, зроблених в кінці кожної наносекунди, кількості запитів, розбита за мітками `phase` (яка приймає значення `waiting` та `executing`) та `priority_level`. Кожне спостереження є відношенням, від 0 до 1, кількості запитів поділеної на відповідний ліміт кількості запитів (обмеження обсягу черги для очікування та обмеження паралелізму для виконання).

- `apiserver_flowcontrol_priority_level_seat_utilization` — це гістограма спостережень, зроблених в кінці кожної наносекунди, використання обмеження пріоритету паралелізму, розбита за `priority_level`. Це використання є часткою (кількість зайнятих місць) / (обмеження паралелізму). Цей показник враховує всі етапи виконання (як нормальні, так і додаткові затримки в кінці запису для відповідного сповіщення про роботу) всіх запитів, крім WATCH; для них він враховує лише початковий етап, що доставляє сповіщення про попередні обʼєкти. Кожна гістограма в векторі також має мітку `phase: executing` (для фази очікування не існує обмеження на кількість місць).

- `apiserver_flowcontrol_request_queue_length_after_enqueue` — це гістограма вектора довжини черги для черг, розбита за `priority_level` та `flow_schema`, як вибірка за запитом у черзі. Кожен запит, який потрапив в чергу, вносить один зразок в свою гістограму, що вказує довжину черги негайно після додавання запиту. Зверніть увагу, що це дає інші показники, ніж неупереджене опитування.

  {{< note >}}
  Сплеск у значенні гістограми тут означає, що ймовірно, один потік (тобто запити від одного користувача або для одного простору імен, в залежності від конфігурації) затоплює сервер API та піддається обмеженню. На відміну, якщо гістограма одного рівня пріоритету показує, що всі черги для цього рівня пріоритету довші, ніж ті для інших рівнів пріоритету, може бути доцільно збільшити частки паралелізму цієї конфігурації пріоритету.
  {{< /note >}}

- `apiserver_flowcontrol_request_concurrency_limit` такий же, як `apiserver_flowcontrol_nominal_limit_seats`. До введення пралелізму запозичення між рівнями пріоритету, це завжди дорівнювало `apiserver_flowcontrol_current_limit_seats` (який не існував як окремий метричний показник).

- `apiserver_flowcontrol_lower_limit_seats` — це вектор мітки миттєвої нижньої межі динамічного обмеження паралелізму для кожного рівня пріоритету.

- `apiserver_flowcontrol_upper_limit_seats` — це вектор мітки миттєвої верхньої межі динамічного обмеження паралелізму для кожного рівня пріоритету.

- `apiserver_flowcontrol_demand_seats` — це гістограма вектора підрахунку спостережень, в кінці кожної наносекунди, відношення кожного рівня пріоритету до (вимоги місць) / (номінального обмеження паралелізму). Вимога місць на рівні пріоритету — це сума, як віконаних запитів, так і тих, які перебувають у початковій фазі виконання, максимум кількості зайнятих місць у початкових та кінцевих фазах виконання запиту.

- `apiserver_flowcontrol_demand_seats_high_watermark` — це вектор мітки миттєвого максимального запиту на місця, що спостерігався протягом останнього періоду адаптації до позичання паралелізму для кожного рівня пріоритету.

- `apiserver_flowcontrol_demand_seats_average` - це вектор мітки миттєвого часового середнього значення вимог місць для кожного рівня пріоритету, що спостерігався протягом останнього періоду адаптації до позичання паралелізму.

- `apiserver_flowcontrol_demand_seats_stdev` - це вектор мітки миттєвого стандартного відхилення популяції вимог місць для кожного рівня пріоритету, що спостерігався протягом останнього періоду адаптації до позичання паралелізму.

- `apiserver_flowcontrol_demand_seats_smoothed` - це вектор мітки миттєвого вирівняного значення вимог місць для кожного рівня пріоритету, визначений на останній період адаптації.

- `apiserver_flowcontrol_target_seats` - це вектор мітки миттєвого усередненого значення вимог місць для кожного рівня пріоритету, яке виходить в алокації позичання.

- `apiserver_flowcontrol_seat_fair_frac` - це мітка миттєвого значення частки справедливого розподілу, визначеного при останній адаптації позичання.

- `apiserver_flowcontrol_current_limit_seats` - це вектор мітки миттєвого динамічного обмеження паралелізму для кожного рівня пріоритету, яке було виведено в останню корекцію.

- `apiserver_flowcontrol_request_execution_seconds` - це гістограма, яка показує, скільки часу займає виконання запитів, розбита за `flow_schema` та `priority_level`.

- `apiserver_flowcontrol_watch_count_samples` - це гістограма вектора кількості активних WATCH-запитів, що стосуються певного запису, розбита за `flow_schema` та `priority_level`.

- `apiserver_flowcontrol_work_estimated_seats` - це гістограма вектора кількості оцінених місць (максимум початкової та кінцевої стадії виконання) для запитів, розбита за `flow_schema` та `priority_level`.

- `apiserver_flowcontrol_request_dispatch_no_accommodation_total` - це лічильник вектора кількості подій, які в принципі могли призвести до відправки запиту, але не зробили цього через відсутність доступного паралелізму, розбитий за `flow_schema` та `priority_level`.

- `apiserver_flowcontrol_epoch_advance_total` - це лічильник вектора кількості спроб зрушити вимірювач прогресу рівня пріоритету назад, щоб уникнути переповнення числових значень, згрупований за `priority_level` та `success`.

## Поради щодо використання API Priority та Fairness {#good-practices-for-using-api-priority-and-fairness}

Коли певний рівень пріоритету перевищує свій дозволений паралелізм, запити можуть зазнавати збільшеного часу очікування або бути відкинуті з HTTP 429 (Забагато запитів) помилкою. Щоб запобігти цим побічним ефектам APF, ви можете змінити ваше навантаження або налаштувати ваші налаштування APF, щоб забезпечити достатню кількість місць для обслуговування ваших запитів.

Щоб виявити, чи відкидаються запити через APF, перевірте наступні метрики:

- apiserver_flowcontrol_rejected_requests_total — загальна кількість відкинутих запитів за FlowSchema та PriorityLevelConfiguration.
- apiserver_flowcontrol_current_inqueue_requests — поточна кількість запитів в черзі за FlowSchema та PriorityLevelConfiguration.
- apiserver_flowcontrol_request_wait_duration_seconds — затримка, додана до запитів, що очікують у черзі.
- apiserver_flowcontrol_priority_level_seat_utilization — використання місць на рівні пріоритету за PriorityLevelConfiguration.

### Зміни в навантаженні {#good-practice-workload-modifications}

Щоб запобігти встановленню запитів у чергу та збільшенню затримки або їх відкиданню через APF, ви можете оптимізувати ваші запити шляхом:

- Зменшення швидкості виконання запитів. Менша кількість запитів протягом фіксованого періоду призведе до меншої потреби у місцях в будь-який момент часу.
- Уникання видачі великої кількості дороговартісних запитів одночасно. Запити можна оптимізувати, щоб вони використовували менше місць або мали меншу затримку, щоб ці запити утримували ці місця протягом коротшого часу. Запити списку можуть займати більше 1 місця в залежності від кількості обʼєктів, отриманих під час запиту. Обмеження кількості обʼєктів, отриманих у запиті списку, наприклад, за допомогою розбиття на сторінки (pagination), буде використовувати менше загальних місць протягом коротшого періоду. Крім того, заміна запитів списку запитами перегляду потребуватиме менше загальних часток паралелізму, оскільки запити перегляду займають лише 1 місце під час свого початкового потоку повідомлень. Якщо використовується потоковий перегляд списків у версіях 1.27 і пізніше, запити перегляду займатимуть таку ж кількість місць, як і запит списку під час його початкового потоку повідомлень, оскільки всі стани колекції мають бути передані по потоку. Зауважте, що в обох випадках запит на перегляд не буде утримувати жодних місць після цієї початкової фази.

Памʼятайте, що встановлення запитів у чергу або їх відкидання через APF можуть бути викликані як збільшенням кількості запитів, так і збільшенням затримки для існуючих запитів. Наприклад, якщо запити, які зазвичай виконуються за 1 секунду, починають виконуватися за 60 секунд, це може призвести до того, що APF почне відкидати запити, оскільки вони займають місця протягом тривалого часу через це збільшення затримки. Якщо APF починає відкидати запити на різних рівнях пріоритету без значного змінення навантаження, можливо, існує проблема з продуктивністю контролера, а не з навантаженням або налаштуваннями APF.

### Налаштування Priority та fairness {#good-practice-apf-settings}

Ви також можете змінити типові обʼєкти FlowSchema та PriorityLevelConfiguration або створити нові обʼєкти цих типів, щоб краще врахувати ваше навантаження.

Налаштування APF можна змінити для:

- Надання більшої кількості місць для запитів високого пріоритету.
- Ізоляція необовʼязкових або дорогих запитів, які можуть забрати рівень паралелізму, якщо він буде спільно використовуватися з іншими потоками.

#### Надання більшої кількості місць для запитів високого пріоритету {#give-more-seats-to-high-priority-requests}

1. У разі можливості кількість місць, доступних для всіх рівнів пріоритету для конкретного `kube-apiserver`, можна збільшити, збільшивши значення прапорців `max-requests-inflight` та `max-mutating-requests-inflight`. Іншим варіантом є горизонтальне масштабування кількості екземплярів `kube-apiserver`, що збільшить загальну одночасність на рівень пріоритету по всьому кластеру за умови належного балансування навантаження запитів.
2. Ви можете створити новий FlowSchema, який посилається на PriorityLevelConfiguration з більшим рівнем паралелізму. Ця нова PriorityLevelConfiguration може бути існуючим рівнем або новим рівнем зі своїм набором номінальних поширень паралелізму. Наприклад, новий FlowSchema може бути введений для зміни PriorityLevelConfiguration для ваших запитів з global-default на workload-low з метою збільшення кількості місць, доступних для ваших користувачів. Створення нової PriorityLevelConfiguration зменшить кількість місць, призначених для існуючих рівнів. Нагадаємо, що редагування типових FlowSchema або PriorityLevelConfiguration потребує встановлення анотації `apf.kubernetes.io/autoupdate-spec` в значення false.
3. Ви також можете збільшити значення NominalConcurrencyShares для PriorityLevelConfiguration, що обслуговує ваші запити високого пріоритету. Як альтернативу, для версій 1.26 та пізніших, ви можете збільшити значення LendablePercent для конкуруючих рівнів пріоритету, щоб даний рівень пріоритету мав вищий пул місць, які він може позичати.

#### Ізоляція неважливих запитів, для попередження виснаження інших потоків {#isolate-non-esential-requests-from-starving-other-flows}

Для ізоляції запитів можна створити FlowSchema, чий субʼєкт збігається з користувачеві, що робить ці запити, або створити FlowSchema, яка відповідає самому запиту (відповідає resourceRules). Далі цю FlowSchema можна зіставити з PriorityLevelConfiguration з низьким пулом місць.

Наприклад, припустимо, що запити на перегляд подій з Podʼів, що працюють в стандартному просторі імен, використовують по 10 місць кожен і виконуються протягом 1 хвилини. Щоб запобігти впливу цих дорогих запитів на інші запити від інших Podʼів, які використовують поточну FlowSchema для сервісних облікових записів, можна застосувати таку FlowSchema для ізоляції цих викликів списків від інших запитів.

Приклад обʼєкта FlowSchema для ізоляції запитів на перегляд подій:

{{% code_sample file="priority-and-fairness/list-events-default-service-account.yaml" %}}

- FlowSchema захоплює всі виклики списків подій, виконані типовим сервісним обліковим записом у стандартному просторі імен. Пріоритет відповідності 8000 менший за значення 9000, що використовується поточною FlowSchema для сервісних облікових записів, тому ці виклики списків подій будуть відповідати list-events-default-service-account, а не service-accounts.
- Використовується конфігурація пріоритету catch-all для ізоляції цих запитів. Пріоритетний рівень catch-all має дуже малий пул місць і не ставить запитів в чергу.

## {{% heading "whatsnext" %}}

- Ви можете переглянути [довідку з контролювання потоків](/docs/reference/debug-cluster/flow-control/) для отримання додаткової інформації для розвʼязання проблем.
- Для отримання довідкової інформації про деталі дизайну для пріоритету та справедливості API див. [пропозицію про вдосконалення](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness).
- Ви можете надсилати пропозиції та запити на функціонал через [SIG API Machinery](https://github.com/kubernetes/community/tree/master/sig-api-machinery) або через [канал у Slack](https://kubernetes.slack.com/messages/api-priority-and-fairness).
