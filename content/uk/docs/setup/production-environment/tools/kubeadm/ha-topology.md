---
title: Варіанти топології високої доступності (HA)
content_type: concept
weight: 50
---

<!-- overview -->

Ця сторінка пояснює два варіанти конфігурації топології ваших високо доступних (HA) кластерів Kubernetes.

Ви можете налаштувати стійкий кластер:

- З вузлами панелі управління, де вузли etcd розташовані разом із вузлами панелі управління.
- З зовнішніми вузлами etcd, де etcd працює на окремих вузлах від вузлів панелі управління.

Перед налаштуванням стійкого кластера слід ретельно розглянути переваги та недоліки кожної топології.

{{< note >}}
kubeadm статично розгортає кластер etcd. Ознайомтесь з [Керівництвом з кластеризації](https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static).
{{< /note >}}

## Топологія etcd зі спільним розміщенням {#stacked-etcd-topology}

Високодоступний кластер зі спільним розміщенням — це [топологія](https://en.wikipedia.org/wiki/Network_topology), де розподілений кластер зберігання даних, який забезпечує etcd, розміщується поверх кластера, сформованого вузлами, керованими kubeadm, які запускають компоненти панелі управління.

Кожен вузол панелі управління запускає екземпляр `kube-apiserver`, `kube-scheduler` та `kube-controller-manager`. `kube-apiserver` доступний для робочих вузлів за допомогою балансувальника навантаження.

Кожен вузол панелі управління створює локального члена etcd, і цей член etcd спілкується лише з `kube-apiserver` цього вузла. Те ж саме стосується локальних екземплярів `kube-controller-manager` та `kube-scheduler`.

Ця топологія зʼєднує вузли панелі управління з членами etcd на тих самих вузлах. Вона є простішою для налаштування, ніж кластер зі зовнішніми вузлами etcd, і простішою для управління реплікацією.

Проте такий кластер має ризик втрати зʼєднання. Якщо один вузол вийде з ладу, втратиться як член etcd, так і екземпляр панелі управління, і резервні можливості будуть скомпрометовані. Цей ризик можна зменшити, додавши більше вузлів панелі управління.

Отже, слід запускати мінімум три вузли панелі управління зі стековим розміщенням для високодоступного кластера.

Це типова топологія в kubeadm. Локальний член etcd створюється автоматично
на вузлах панелі управління при використанні `kubeadm init` та `kubeadm join --control-plane`.

![Топологія etcd зі стековим розміщенням](/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg)

## Топологія зовнішнього розміщення etcd {#external-etcd-topology}

Стійкий кластер із зовнішньою etcd — це [топологія](https://en.wikipedia.org/wiki/Network_topology), де розподілений кластер зберігання даних, наданий etcd, є зовнішнім щодо кластера, сформованого вузлами, які запускають компоненти панелі управління.

Подібно до топології зі стековими etcd, кожен вузол панелі управління в топології зі зовнішньою etcd запускає екземпляр `kube-apiserver`, `kube-scheduler` та `kube-controller-manager`. І `kube-apiserver` доступний для робочих вузлів за допомогою балансувальника навантаження. Однак члени etcd працюють на окремих хостах, і кожен хост etcd спілкується з `kube-apiserver` кожного вузла панелі управління.

Ця топологія відокремлює вузли панелі управління та членів etcd. Таким чином, вона забезпечує стійке налаштування, де втрата екземпляра панелі управління або члена etcd має менший вплив і не впливає на резервування кластера так сильно, як топологія стекового HA.

Однак для цієї топології потрібно удвічі більше хостів, ніж для топології зі стековим etcd. Мінімум три хости для вузлів панелі управління та три хости для вузлів etcd необхідні для стійкого кластера з цією топологією.

![Топологія зовнішнього розташування etcd](/images/kubeadm/kubeadm-ha-topology-external-etcd.svg)

## {{% heading "whatsnext" %}}

- [Встановлення високодоступного кластера з kubeadm](/docs/setup/production-environment/tools/kubeadm/high-availability/)
