---
title: Горизонтальне автомасштабування Podʼів
feature:
  title: Горизонтальне масштабування
  description: >
    Масштабуйте свої застосунки, збільшуйте та зменшуйте кількість їх екземплярів за допомогою простої команди, в графічному інтерфейсі або автоматично на основі використання ЦП.
content_type: concept
weight: 90
math: true
---

<!-- overview -->

У Kubernetes _HorizontalPodAutoscaler_ автоматично оновлює ресурс навантаження (наприклад, {{< glossary_tooltip text="Deployment" term_id="deployment" >}} або {{< glossary_tooltip text="StatefulSet" term_id="statefulset" >}}), з метою автоматичного масштабування робочого навантаження у відповідь на попит.

Горизонтальне масштабування означає, що реакція на збільшення навантаження полягає у розгортанні додаткових {{< glossary_tooltip text="Podʼів" term_id="pod" >}}. Це відрізняється від _вертикального_ масштабування, що для Kubernetes означає призначення додаткових ресурсів (наприклад, памʼяті або CPU) для уже запущених Podʼів робочого навантаження.

Якщо навантаження зменшується, а кількість Podʼів перевищує налаштований мінімум, HorizontalPodAutoscaler інструктує ресурс навантаження (Deployment, StatefulSet або інший схожий ресурс) масштабуватися вниз.

Горизонтальне автоматичне масштабування Podʼів не застосовується до обʼєктів, які не можна масштабувати (наприклад, {{< glossary_tooltip text="DaemonSet" term_id="daemonset" >}}).

HorizontalPodAutoscaler реалізований як ресурс API Kubernetes та {{< glossary_tooltip text="контролер" term_id="controller" >}}. Ресурс визначає поведінку контролера. Контролер горизонтального автоматичного масштабування Podʼів, який працює в {{< glossary_tooltip text="панелі управління" term_id="control-plane" >}} Kubernetes, періодично коригує бажану шкалу своєї цілі (наприклад, Deployment), щоб відповідати спостережуваним метрикам, таким як середнє використання CPU, середня використання памʼяті або будь-яка інша метрика, яку ви вказуєте.

Ось [приклад](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) використання горизонтального автоматичного масштабування Podʼів.

## Як працює HorizontalPodAutoscaler? {#how-does-the-horizontalpodautoscaler-work}

{{< mermaid >}}
graph BT

hpa[Horizontal Pod Autoscaler] --> scale[Scale]

subgraph rc[RC / Deployment]
    scale
end

scale -.-> pod1[Pod 1]
scale -.-> pod2[Pod 2]
scale -.-> pod3[Pod N]

classDef hpa fill:#D5A6BD,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef rc fill:#F9CB9C,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef scale fill:#B6D7A8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef pod fill:#9FC5E8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
class hpa hpa;
class rc rc;
class scale scale;
class pod1,pod2,pod3 pod
{{< /mermaid >}}

Схема 1. HorizontalPodAutoscaler керує масштабуванням Deployment та його ReplicaSet

У Kubernetes горизонтальне автоматичне масштабування Podʼів реалізовано як цикл керування, що працює інтервально (це не постійний процес). Інтервал встановлюється параметром `--horizontal-pod-autoscaler-sync-period` для [`kube-controller-manager`](/docs/reference/command-line-tools-reference/kube-controller-manager/) (а стандартне значення — 15 секунд).

Один раз протягом кожного періоду менеджер контролера запитує використання ресурсів відповідно до метрик, вказаних у визначенні кожного HorizontalPodAutoscaler. Менеджер контролера знаходить цільовий ресурс, визначений за допомогою `scaleTargetRef`, потім вибирає Podʼи на основі міток `.spec.selector` цільового ресурсу та отримує метрики зі специфічних метрик ресурсів API (для метрик ресурсів на кожен Pod) або API власних метрик (для всіх інших метрик).

- Для метрик ресурсів на кожен Pod (наприклад, CPU) контролер отримує метрики з API метрик ресурсів для кожного Pod, на який впливає HorizontalPodAutoscaler. Потім, якщо встановлено значення цільового використання, контролер обчислює значення використання як відсоток еквівалентного [ресурсного запиту](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits) на контейнери в кожному Pod. Якщо встановлено сирцеве цільове значення, використовуються сирі (необроблені) значення метрик. Потім контролер бере середнє значення використання або сире значення (залежно від типу вказаної цілі) для всіх цільових Podʼів і створює співвідношення, яке використовується для масштабування кількості бажаних реплік.

  Зверніть увагу, що якщо деякі контейнери Podʼів не мають відповідного ресурсного запиту, використання CPU для Pod не буде визначене, і автоматичний масштабувальник не буде виконувати жодних дій для цієї метрики. Дивіться розділ [подробиці алгоритму](#algorithm-details) нижче для отримання додаткової інформації про те, як працює алгоритм автомасштабування.

- Для власних метрик на кожен Pod контролер працює аналогічно метрикам ресурсів на кожен Pod, за винятком того, що він працює з сирцевими значеннями, а не значеннями використання.

- Для метрик обʼєктів та зовнішніх метрик витягується одна метрика, яка описує обʼєкт. Цю метрику порівнюють з цільовим значенням, щоб отримати співвідношення, як вище. У версії API `autoscaling/v2` це значення можна за необхідності розділити на кількість Podʼів до порівняння.

Звичайне використання HorizontalPodAutoscaler — налаштувати його на витягування метрик з [агрегованих API](/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server) (`metrics.k8s.io`, `custom.metrics.k8s.io` або `external.metrics.k8s.io`). API `metrics.k8s.io` зазвичай надається Metrics Server, який потрібно запустити окремо. Для отримання додаткової інформації про метрики ресурсів див. [Metrics Server](/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server).

[Підтримка API метрик](#support-for-metrics-apis) пояснює гарантії стабільності та статус підтримки цих різних API.

Контролер HorizontalPodAutoscaler має доступ до відповідних ресурсів робочого навантаження, які підтримують масштабування (такі як Deployment та StatefulSet). Ці ресурси мають субресурс з назвою `scale`, інтерфейс, який дозволяє динамічно встановлювати кількість реплік і переглядати поточний стан кожного з них. Для загальної інформації про субресурси в API Kubernetes див. [Поняття API Kubernetes](/docs/reference/using-api/api-concepts/).

### Алгоритм {#algorithm-details}

По простому, контролер HorizontalPodAutoscaler працює зі співвідношенням між бажаним значенням метрики та поточним значенням метрики:

```math
\begin{equation*}
бажаніРепліки = ceil\left\lceil поточніРепліки \times \frac {поточнеЗначенняМетрики}{бажанеЗначенняМетрики} \right\rceil
\end{equation*}
```

Наприклад, якщо поточне значення метрики — `200м`, а бажане значення — `100м`, кількість реплік буде подвоєна, оскільки \\( { 200.0 \div 100.0 } = 2.0 \\). Якщо поточне значення замість цього — `50м`, кількість реплік буде зменшена вдвічі, оскільки \\( { 50.0 \div 100.0 } = 0.5 \\). Панель управління пропускає будь-яку дію масштабування, якщо співвідношення достатньо близьке до 1,0 (в межах [налаштовуваних допусків](#tolerance), типово 0.1).

Якщо вказано `targetAverageValue` або `targetAverageUtilization`, поточне значення метрики обчислюється шляхом визначення середнього значення вказаної метрики для всіх Podʼів у цільовому масштабі HorizontalPodAutoscaler.

Перед перевіркою допустимості та визначенням кінцевих значень панель управління також розглядає, чи є відсутні будь-які метрики, і скільки Podʼів є [`Ready`](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions). Всі Podʼи зі встановленим відбитками часу видалення (обʼєкти з відбитками часу видалення перебувають у процесі завершення роботи/видалення) ігноруються, а всі збійні Podʼи не враховуються.

Якщо конкретний Pod не маж метрики, його відкладено на потім; Podʼи з відсутніми метриками будуть використані для коригування кінцевої кількості масштабування.

При масштабуванні за CPU, якщо будь-який pod ще не став готовим (він все ще ініціалізується, або можливо, несправний) _або_ остання точка метрики для Pod була до того, як він став готовим, цей Pod також відкладено.

Через технічні обмеження, контролер HorizontalPodAutoscaler не може точно визначити перший раз, коли Pod стає готовим при визначенні, чи відкласти певні метрики CPU. Замість цього вважається, що Pod "ще не готовий", якщо він ще не готовий і перейшов у готовий стан протягом короткого, налаштованого вікна часу з моменту початку. Це значення налаштоване за допомогою прапорця `--horizontal-pod-autoscaler-initial-readiness-delay`, і типово воно складає 30 секунд. Як тільки Pod став готовим, будь-який перехід в готовий стан вважається першим, якщо це сталося протягом довшого, налаштованого часу від початку. Це значення налаштоване за допомогою прапорця
`--horizontal-pod-autoscaler-cpu-initialization-period`, і його стандартне значення — 5 хвилин.

Базове співвідношення масштабу \\( currentMetricValue \over desiredMetricValue \\) потім обчислюється залишковими Podʼами, які не були відкладені або відкинуті вище.

Якщо були відсутні метрики, панель управління перераховує середнє значення більш консервативно, припускаючи, що ці Podʼи споживають 100% бажаного значення у випадку масштабування вниз і 0% у випадку масштабування вгору. Це зменшує величину можливого масштабу.

Крім того, якщо присутні будь-які ще не готові Podʼи, і робоче навантаження мало б масштабуватися вгору без врахування відсутніх метрик або ще не готових Podʼів, контролер консервативно припускає, що ще не готові Podʼи споживають 0% бажаної метрики, зменшуючи величини масштабу.

Після врахування ще не готових Podʼів та відсутніх метрик контролер повторно обчислює відношення використання. Якщо нове співвідношення змінює напрямок масштабування або знаходиться в межах допустимості, контролер не вживає жодних дій щодо масштабування. У інших випадках нове співвідношення використовується для прийняття будь-яких змін кількості Podʼів.

Зверніть увагу, що _початкове_ значення для середнього використання повертається через статус HorizontalPodAutoscaler, без врахування ще не готових Podʼів або відсутніх метрик, навіть коли використовується нове відношення використання.

Якщо в HorizontalPodAutoscaler вказано кілька метрик, цей розрахунок виконується для кожної метрики, а потім обирається найбільше з бажаних кількостей реплік. Якщо будь-яка з цих метрик не може бути переведена у бажану кількість реплік (наприклад, через помилку отримання метрик з  API метрик) і запропоновано масштабування вниз за метриками, які можна витягнути, масштабування пропускається. Це означає, що HPA все ще може масштабуватися вгору, якщо одна або декілька метрик дають значення `desiredReplicas`, більше, ніж поточне значення.

Нарешті, прямо перед тим, як HPA масштабує ціль, рекомендація масштабування записується. Контролер розглядає всі рекомендації в налаштованому вікні та обирає найвищу рекомендацію в межах цього вікна. Це значення можна налаштувати за допомогою прапорця `--horizontal-pod-autoscaler-downscale-stabilization`, який стандартно складає 5 хвилин. Це означає, що зменшення масштабу відбуватиметься поступово, згладжуючи вплив метрик, що швидко змінюються.

### Готовіність Podʼа та метрики автомасштабування {#pod-readiness-and-autoscaling-metrics}

Контролер HorizontalPodAutoscaler (HPA) містить два прапорці, які впливають на те, як збираються показники CPU з Podʼів під час запуску:

1. `--horizontal-pod-autoscaler-cpu-initialization-period` (стандартно: 5 хвилин)

  Цей параметр визначає часовий проміжок після запуску Podʼа, протягом якого **використання CPU ігнорується**, крім випадків, коли це необхідно:
    - Pod перебуває у стані `Ready` **та**
    - Зразок метрики було взято повністю протягом періоду, коли він перебував у стані `Ready`.

  Цей прапорець допомагає **виключити високе споживання CPU** Podʼів, що ініціалізуються (напр., Java-застосунки, що перебувають у своїй підготовчій фазі) з рішень HPA щодо масштабування.

1. `--horizontal-pod-autoscaler-initial-readiness-delay` (стангдартно: 30 секунд)

  Цей прапорець визначає короткі періоди затримки після запуску Podʼа, протягом якого контролер HPA розглядає Podʼи, що перебувають у стані `Ready`, як такі, що все ще ініціалізуються, **навіть якщо вони раніше ненадовго перейшли у стан `Ready`**.

  Він призначений для цього щоб:
    - Уникнути включення Podʼів, які швидко коливаються між `Ready` та `Unready` під час запуску.
    - Забезпечити стабільність початкового сигналу готовності до того, як HPA вважатиме їх метрики дійсними.

**Ключові моделі поведінки:**

- Якщо Pod перебуває в стані `Ready` і залишається в цьому стані, він може бути зарахований як такий, що надає метрики навіть протягом затримки.
- Якщо Pod швидко перемикається між станом `Ready` та `Unready`, метрики ігноруються, поки він не стане стабільно `Ready`.

#### Поради {#best-practice}

Якщо ваш Pod має фазу запуску з високим використанням CPU:

- Налаштуйте `startupProbe`, який не проходить, поки високе використання CPU не мине, або
- Переконайтеся, що ваш `readinessProbe` повідомляє про `Ready` тільки **після** того, як пік використання CPU спаде, використовуючи `initialDelaySeconds`.

А в ідеалі також встановіть `--horizontal-pod-autoscaler-cpu-initialization-period`, щоб **покрити тривалість запуску**.

## Обʼєкт API {#api-object}

Horizontal Pod Autoscaler є ресурсом API в групі API Kubernetes `autoscaling`. Поточна стабільна версія знаходиться в версії API `autoscaling/v2`, яка включає підтримку масштабування за памʼяттю та власними метриками. Нові поля, введені в `autoscaling/v2`, зберігаються як анотації при роботі з `autoscaling/v1`.

При створенні обʼєкта API HorizontalPodAutoscaler переконайтеся, що вказане імʼя є дійсним [піддоменом DNS](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names). Більше деталей про обʼєкт API можна знайти на сторінці [Обʼєкт HorizontalPodAutoscaler](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#horizontalpodautoscaler-v2-autoscaling).

## Стабільність масштабування робочого навантаження {#flapping}

При керуванні масштабом групи реплік за допомогою HorizontalPodAutoscaler можливі часті коливання кількості реплік через динамічний характер оцінюваних метрик. Це іноді називають _коливанням_, або _flapping_. Це схоже на концепцію _гістерезису_ в кібернетиці.

## Масштабування під час поступового оновлення {#autoscaling-during-rolling-update}

Kubernetes дозволяє виконувати поступове оновлення для Deployment. У цьому випадку Deployment керує підлеглими ReplicaSets за вас. Коли ви налаштовуєте автомасштабування для Deployment, ви звʼязуєте HorizontalPodAutoscaler з одним Deployment. HorizontalPodAutoscaler керує полем `replicas` Deployment. Контролер розгортання відповідає за встановлення `replicas` підлеглих ReplicaSets так, щоб вони складали відповідну кількість під час розгортання, а також після його завершення.

Якщо ви виконуєте поступове оновлення StatefulSet, у якого є автомасштабована кількість реплік, StatefulSet безпосередньо керує своїм набором Podʼів (немає проміжного ресурсу, аналогічного ReplicaSet).

## Підтримка метрик ресурсів {#support-for-resource-metrics}

Будь-який обʼєкт HPA може бути масштабований на основі використання ресурсів у Podʼах цільового масштабування. При визначенні специфікації Podʼа повинні бути вказані запити ресурсів, такі як `cpu` та `memory`. Це використовується для визначення використання ресурсів і використовується контролером HPA для масштабування цілі вгору або вниз. Щоб використовувати масштабування на основі використання ресурсів, вкажіть джерело метрики так:

```yaml
type: Resource
resource:
  name: cpu
  target:
    type: Utilization
    averageUtilization: 60
```

З цією метрикою контролер HPA буде підтримувати середнє використання ресурсів у Podʼах цільового масштабування на рівні 60%. Використання — це співвідношення між поточним використанням ресурсу та запитаними ресурсами Podʼа. Дивіться [Алгоритм](#algorithm-details) для отримання додаткової інформації про те, як обчислюється та усереднюється використання.

{{< note >}}
Оскільки використання ресурсів всіх контейнерів сумується, загальне використання Podʼа може не точно відображати використання ресурсів окремих контейнерів. Це може призвести до ситуацій, коли один контейнер може працювати з високим використанням, а HPA не буде масштабувати його, оскільки загальне використання Podʼа все ще знаходиться в припустимих межах.
{{< /note >}}

### Метрики ресурсів контейнера {#container-resource-metrics}

{{< feature-state feature_gate_name="HPAContainerMetrics" >}}

API HorizontalPodAutoscaler також підтримує джерело метрик контейнера, де HPA може відстежувати використання ресурсів окремих контейнерів у наборі Podʼів, щоб масштабувати цільовий ресурс. Це дозволяє налаштовувати пороги масштабування для контейнерів, які мають найбільше значення в конкретному Pod. Наприклад, якщо у вас є вебзастосунок і контейнер допоміжного сервісу (sidecar), який надає логування, ви можете масштабувати на основі використання ресурсів вебзастосунка, ігноруючи контейнер допоміжного сервісу (sidecar) та його використання ресурсів.

Якщо ви переглянете цільовий ресурс, щоб мати нову специфікацію Pod з іншим набором контейнерів, ви повинні відредагувати специфікацію HPA, якщо цей ново доданий контейнер також має бути використаний для масштабування. Якщо вказаний контейнер у джерелі метрики відсутній або присутній лише в підмножині Podʼів, то ці Podʼи ігноруються, і рекомендація перераховується. Дивіться [Алгоритм](#algorithm-details) для отримання додаткової інформації про обчислення. Щоб використовувати ресурси контейнера для автомасштабування, визначте джерело метрики таким чином:

```yaml
type: ContainerResource
containerResource:
  name: cpu
  container: application
  target:
    type: Utilization
    averageUtilization: 60
```

У вищенаведеному прикладі контролер HPA масштабує ціль так, що середнє використання cpu у контейнері `application` у всіх подах становить 60%.

{{< note >}}
Якщо ви зміните імʼя контейнера, який відстежує HorizontalPodAutoscaler, ви можете зробити цю зміну у певному порядку, щоб забезпечити доступність та ефективність масштабування під час застосування змін. Перед тим, як ви оновите ресурс, який визначає контейнер (наприклад, Deployment), вам слід оновити повʼязаний HPA, щоб відстежувати як нові, так і старі імена контейнерів. Таким чином, HPA може розраховувати рекомендацію масштабування протягом усього процесу оновлення.

Після того, як ви розгорнули зміну імені контейнера на ресурсі навантаження, закінчіть, видаливши старе імʼя контейнера з характеристик HPA.
{{< /note >}}

## Масштабування на основі власних метрик {#scaling-on-custom-metrics}

{{< feature-state for_k8s_version="v1.23" state="stable" >}}

(версія API `autoscaling/v2beta2` раніше надавала цю можливість як бета-функцію)

За умови використання версії API `autoscaling/v2` ви можете налаштувати HorizontalPodAutoscaler на масштабування на основі власної метрики (яка не є вбудованою в Kubernetes або будь-який компонент Kubernetes). Потім контролер HorizontalPodAutoscaler запитує ці власні метрики з Kubernetes API.

Дивіться [Підтримка API метрик](#support-for-metrics-apis) для вимог.

## Масштабування на основі декількох метрик {#scaling-on-multiple-metrics}

{{< feature-state for_k8s_version="v1.23" state="stable" >}}

(версія API `autoscaling/v2beta2` раніше надавала цю можливість як бета-функцію)

За умови використання версії API `autoscaling/v2` ви можете вказати декілька метрик для HorizontalPodAutoscaler для масштабування на їх основі. Потім контролер HorizontalPodAutoscaler оцінює кожну метрику і пропонує новий масштаб на основі цієї метрики. HorizontalPodAutoscaler бере максимальний масштаб, рекомендований для кожної метрики, і встановлює робоче навантаження на такий розмір (за умови, що це не більше загального максимуму, який ви налаштували).

## Підтримка API метрик {#support-for-metrics-apis}

Стандартно контролер HorizontalPodAutoscaler отримує метрики з низки API. Для того щоб мати доступ до цих API, адміністратори кластера повинні забезпечити:

- Увімкнути [шар агрегації API](/docs/tasks/extend-kubernetes/configure-aggregation-layer/).

- Відповідні API мають бути зареєстровані:

  - Для метрик ресурсі це `metrics.k8s.io` [API](/docs/reference/external-api/metrics.v1beta1/), яке зазвичай надає [metrics-server](https://github.com/kubernetes-sigs/metrics-server). Це може бути запущено як надбудова кластера.

  - Для власних метрик це `custom.metrics.k8s.io` [API](/docs/reference/external-api/custom-metrics.v1beta2/). Його надають сервери API "адаптера", які надають постачальники рішень метрик. Перевірте у своєї системи метрик, чи доступний адаптер метрик Kubernetes.

  - Для зовнішніх метрик це `external.metrics.k8s.io` [API](/docs/reference/external-api/external-metrics.v1beta1/). Він може бути наданий адаптерами власних метрик, які наведено вище.

Для отримання додаткової інформації про ці різні шляхи метрик та їх відмінності дивіться відповідні пропозиції дизайну для [HPA V2](https://git.k8s.io/design-proposals-archive/autoscaling/hpa-v2.md), [custom.metrics.k8s.io](https://git.k8s.io/design-proposals-archive/instrumentation/custom-metrics-api.md) та [external.metrics.k8s.io](https://git.k8s.io/design-proposals-archive/instrumentation/external-metrics-api.md).

Для прикладів використання дивіться [посібник з використання власних метрик](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics) та [посібник з використання зовнішніх метрик](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects).

## Налаштований механізм масштабування {#configurable-scaling-behavior}

{{< feature-state for_k8s_version="v1.23" state="stable" >}}

(версія API `autoscaling/v2beta2` раніше надавала цю можливість як бета-функцію)

Якщо ви використовуєте API HorizontalPodAutoscaler `v2`, ви можете використовувати поле `behavior` (див. [довідку API](/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec)) для налаштування окремих поведінок масштабування вгору та вниз. Ви вказуєте ці поведінки, встановлюючи `scaleUp` та/або `scaleDown`
у полі `behavior`.

Політики масштабування також дозволяють контролювати швидкість зміни реплік під час масштабування. Також можна використани два параметри для запобігання [коливанню](#flapping): ви можете вказати _вікно стабілізації_ для чіткіщого підрахунку кількості реплік та допуск для ігнорування незначних коливань метрик нижче вказаного порогового значення.

### Політики масштабування {#scaling-policies}

Одну або декілька політик масштабування можна вказати у розділі `behavior` специфікації. Коли вказано кілька політик, політика, яка дозволяє найбільше змін, є політикою, яка типово вибирається. Наступний приклад показує цю поведінку під час зменшення масштабу:

```yaml
behavior:
  scaleDown:
    policies:
    - type: Pods
      value: 4
      periodSeconds: 60
    - type: Percent
      value: 10
      periodSeconds: 60
```

`periodSeconds` вказує на проміжок часу в минулому, протягом якого політика має бути істинною. Максимальне значення, яке можна встановити для `periodSeconds`, — 1800 (пів години). Перша політика _(Pods)_ дозволяє зменшення максимум до 4 репліки протягом однієї хвилини. Друга політика _(Percent)_ дозволяє зменшити максимум 10% поточних реплік протягом однієї хвилини.

Оскільки стандартно вибирається політика, яка дозволяє найбільше змін, друга політика буде використовуватися лише тоді, коли кількість реплік буде більше ніж 40. З 40 або менше реплік, буде застосована перша політика. Наприклад, якщо є 80 реплік, і ціль — зменшити масштаб до 10 реплік тоді під час першого кроку буде скорочено 8 реплік. На наступній ітерації, коли кількість реплік становить 72, 10% від реплік — 7,2, але число заокруглюється вгору до 8. На кожному кроці контролера автомасштабування перераховується кількість реплік, які мають бути змінені, на основі кількості поточних реплік. Коли кількість реплік падає нижче 40, застосовується перша політика _(Pods)_ і зменшується по 4 репліки за раз.

Вибір політики можна змінити, вказавши поле `selectPolicy` для напрямку масштабування. Встановлення значення `Min` вибере політику, яка дозволяє найменшу зміну кількості реплік. Встановлення значення `Disabled` повністю вимикає масштабування в даному напрямку.

### Вікно стабілізації {#stabilization-window}

Вікно стабілізації використовується для обмеження [перепадів](#flapping) кількості реплік, коли метрики, які використовуються для масштабування, постійно коливаються. Алгоритм автоматичного масштабування використовує це вікно, щоб виявити попередній бажаний стан та уникнути непотрібних змін у масштабі навантаження.

Наприклад, у наступному виразі фрагменту коду вказано вікно стабілізації для `scaleDown`.

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300
```

Коли метрики показують, що потрібно зменшити ціль, алгоритм дивиться на раніше розраховані бажані стани і використовує найвище значення з вказаного інтервалу. У вищезазначеному прикладі всі попередні бажані стани за останні 5 хвилин будуть враховані.

Це наближається до ковзного максимуму і уникає ситуації, коли алгоритм масштабування часто видаляє Podʼи лише для того, щоб викликати повторне створення еквівалентного Podʼа за мить.

### Допуск {#tolerance}

{{< feature-state feature_gate_name="HPAConfigurableTolerance" >}}

Поле `tolerance` налаштовує порогове значення для мінливості метрики, запобігаючи масштабуванню нижче вказанного значення.

Цей допуск визначається як величина відхилення навколо бажаного значення метрики, нижче якої масштабування не відбуватиметься. Наприклад, розглянемо HorizontalPodAutoscaler, налаштований з цільовим споживанням памʼяті 100 МБ та допуском масштабування 5%:

```yaml
behavior:
  scaleUp:
    tolerance: 0.05 # 5% допукс для масштабування
```

За такої конфігурації алгоритм HPA розглядатиме масштабування лише в тому випадку, якщо споживання памʼяті перевищує 105 МБ (тобто на 5% вище цільового значення).

Якщо це поле не встановлено, HPA застосовуватиме стандартний допуск кластера — 10%. Це стандартне значення може бути оновлено як для масштабування вгору так і вниз використовуючи [kube-controller-manager](/docs/reference/command-line-tools-reference/kube-controller-manager/) з аргументом командного рядка `--horizontal-pod-autoscaler-tolerance`. (Ви не можете використовувати API Kubernetes для налашування цього стандартного значення.)

### Стандартна поведінка {#default-behavior}

Для використання власного масштабування не всі поля повинні бути вказані. Можна вказати лише значення, які потрібно налаштувати. Ці власні значення злиті зі стандартними значеннями. Стандартні значення відповідають існуючій поведінці в алгоритмі HPA.

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300
    policies:
    - type: Percent
      value: 100
      periodSeconds: 15
  scaleUp:
    stabilizationWindowSeconds: 0
    policies:
    - type: Percent
      value: 100
      periodSeconds: 15
    - type: Pods
      value: 4
      periodSeconds: 15
    selectPolicy: Max
```

Для зменшення масштабу вікно стабілізації становить _300_ секунд (або значення параметра `--horizontal-pod-autoscaler-downscale-stabilization`, якщо воно вказане). Є лише одна політика для зменшення масштабу, яка дозволяє видалити 100% поточно запущених реплік, що означає, що ціль масштабування може бути зменшена до мінімально допустимих реплік. Для збільшення масштабу вікно стабілізації відсутнє. Коли метрики показують, що ціль повинна бути збільшена, ціль збільшується негайно. Є 2 політики, за якими кожні 15 секунд можна додати не більше 4 Podʼів або 100% поточно запущених реплік до тих пір, поки HPA не досягне стабільного стану.

### Приклад: зміна вікна стабілізації для зменшення масштабу {#example-change-downscale-stabilization-window}

Щоб вказати власне значення вікна стабілізації для зменшення масштабу тривалістю в 1 хвилину, в HPA буде додано наступну поведінку:

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 60
```

### Приклад: обмеження коєфіцієнту зменшення масштабу {#example-limiting-scale-down-rate}

Щоб обмежити коєфіцієнт, з яким Podʼи видаляються HPA, до 10% за хвилину, до HPA додається наступна поведінка:

```yaml
behavior:
  scaleDown:
    policies:
    - type: Percent
      value: 10
      periodSeconds: 60
```

Щоб переконатися, що за хвилину видаляється не більше 5 Podʼів, можна додати другу політику зменшення масштабу з фіксованим розміром 5 та встановити `selectPolicy` на значення `Min`. Встановлення `selectPolicy` на `Min` означає, що автомасштабувальник вибирає політику, яка впливає на найменшу кількість Podʼів:

```yaml
behavior:
  scaleDown:
    policies:
    - type: Percent
      value: 10
      periodSeconds: 60
    - type: Pods
      value: 5
      periodSeconds: 60
    selectPolicy: Min
```

### Приклад: вимкнення зменшення масштабу {#example-disable-scale-down}

Значення `selectPolicy` `Disabled` вимикає масштабування вказаного напрямку. Щоб запобігти зменшенню масштабу, буде використана наступна політика:

```yaml
behavior:
  scaleDown:
    selectPolicy: Disabled
```

## Підтримка HorizontalPodAutoscaler в kubectl {#support-for-horizontalpodautoscaler-in-kubectl}

HorizontalPodAutoscaler, як і кожний ресурс API, підтримується стандартним чином у `kubectl`. Ви можете створити новий автомасштабувальник за допомогою команди `kubectl create`. Ви можете переглянути список автомасштабувальників за допомогою `kubectl get hpa` або отримати детальний опис за допомогою `kubectl describe hpa`. Нарешті, ви можете видалити автомасштабувальник за допомогою `kubectl delete hpa`.

Крім того, є спеціальна команда `kubectl autoscale` для створення обʼєкта HorizontalPodAutoscaler. Наприклад, виконання `kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80` створить автомасштабувальник для ReplicaSet _foo_, з цільовим використанням процесора, встановленим на `80%` і кількістю реплік від 2 до 5.

## Неявне деактивування режиму підтримки {#implicit-maintenance-mode-deactivation}

Ви можете неявно деактивувати HPA для цільового обʼєкта без необхідності змінювати конфігурацію HPA самостійно. Якщо бажана кількість реплік цільового обʼєкта встановлена на 0, а мінімальна кількість реплік HPA більше 0, HPA припиняє коригування цільового обʼєкта (і встановлює умову `ScalingActive` на собі в `false`) до тих пір, поки ви не активуєте його вручну, змінивши бажану кількість реплік цільового обʼєкта або мінімальну кількість реплік HPA.

### Перехід Deployment та StatefulSet на горизонтальне масштабування {#migrating-deployments-and-statefulsets-to-horizontal-pod-autoscaler}

При увімкненні HPA рекомендується видалити значення `spec.replicas` з Deployment та/або StatefulSet в їхніх {{< glossary_tooltip text="маніфестах" term_id="manifest" >}}. Якщо цього не зроблено, будь-яка зміна в цьому обʼєкті, наприклад за допомогою `kubectl apply -f deployment.yaml`, буде інструкцією Kubernetes масштабувати поточну кількість Podʼів до значення ключа `spec.replicas`. Це може бути небажаним і призводити до проблем, коли HPA активно працює.

Майте на увазі, що видалення `spec.replicas` може спричинити одноразове зниження кількості Podʼів, оскільки стандартне значення цього ключа — 1 (див. [Репліки Deployment](/docs/concepts/workloads/controllers/deployment#replicas)). Після оновлення всі Podʼи, крім одного, розпочнуть процедури їхнього завершення. Після цього будь-яке подальше розгортання застосунку буде працювати як звичайно і буде дотримуватися конфігурації плавного оновлення за бажанням. Ви можете уникнути цього зниження, обравши один із двох наступних методів в залежності від того, як ви модифікуєте свої розгортання:

{{< tabs name="fix_replicas_instructions" >}}
{{% tab name="Застосування на боці клієнта (стандартно)" %}}

1. `kubectl apply edit-last-applied deployment/<deployment_name>`
2. У редакторі видаліть `spec.replicas`. Після збереження та виходу з редактора, `kubectl` застосовує оновлення. На цьому етапі не відбувається змін кількості Podʼів.
3. Тепер ви можете видалити `spec.replicas` з маніфеста. Якщо ви використовуєте систему управління вихідним кодом, також зафіксуйте ваші зміни або виконайте будь-які інші кроки для перегляду вихідного коду, які відповідають вашому способу відстеження оновлень.
4. Відтепер ви можете запускати `kubectl apply -f deployment.yaml`

{{% /tab %}}
{{% tab name="Застосування на боці сервера" %}}

При використанні [Server-Side Apply](/docs/reference/using-api/server-side-apply/)
ви можете дотримуватися [вказівок щодо передачі власності](/docs/reference/using-api/server-side-apply/#transferring-ownership), які охоплюють цей саме випадок використання.

{{% /tab %}}
{{< /tabs >}}

## {{% heading "whatsnext" %}}

Якщо ви налаштовуєте автомасштабування у вашому кластері, вам також може бути корисно розглянути використання [автомасштабування вузлів](/docs/concepts/cluster-administration/node-autoscaling/) для забезпечення того, що ви запускаєте правильну кількість вузлів.

Для отримання додаткової інформації про HorizontalPodAutoscaler:

- Прочитайте [приклад](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) для автоматичного горизонтального масштабування Podʼів.
- Прочитайте документацію для [`kubectl autoscale`](/docs/reference/generated/kubectl/kubectl-commands/#autoscale).
- Якщо ви бажаєте написати власний адаптер для власних метрик, перегляньте [початковий код](https://github.com/kubernetes-sigs/custom-metrics-apiserver), щоб почати.
- Ознайомтесь з [Довідкою API](/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/) для HorizontalPodAutoscaler.
